// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: DatanodeProtocol.proto

package org.apache.hadoop.hdfs.protocol;

public final class DatanodeProtocolProtos {
  private DatanodeProtocolProtos() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistryLite registry) {
  }

  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
    registerAllExtensions(
        (com.google.protobuf.ExtensionRegistryLite) registry);
  }
  public interface DatanodeRegistrationProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.DatanodeRegistrationProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * Datanode information
     * </pre>
     *
     * <code>required .org.apache.hadoop.DatanodeIDProto datanodeID = 1;</code>
     * @return Whether the datanodeID field is set.
     */
    boolean hasDatanodeID();
    /**
     * <pre>
     * Datanode information
     * </pre>
     *
     * <code>required .org.apache.hadoop.DatanodeIDProto datanodeID = 1;</code>
     * @return The datanodeID.
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto getDatanodeID();
    /**
     * <pre>
     * Datanode information
     * </pre>
     *
     * <code>required .org.apache.hadoop.DatanodeIDProto datanodeID = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProtoOrBuilder getDatanodeIDOrBuilder();

    /**
     * <pre>
     * Node information
     * </pre>
     *
     * <code>required .org.apache.hadoop.StorageInfoProto storageInfo = 2;</code>
     * @return Whether the storageInfo field is set.
     */
    boolean hasStorageInfo();
    /**
     * <pre>
     * Node information
     * </pre>
     *
     * <code>required .org.apache.hadoop.StorageInfoProto storageInfo = 2;</code>
     * @return The storageInfo.
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto getStorageInfo();
    /**
     * <pre>
     * Node information
     * </pre>
     *
     * <code>required .org.apache.hadoop.StorageInfoProto storageInfo = 2;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProtoOrBuilder getStorageInfoOrBuilder();

    /**
     * <pre>
     * Block keys
     * </pre>
     *
     * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 3;</code>
     * @return Whether the keys field is set.
     */
    boolean hasKeys();
    /**
     * <pre>
     * Block keys
     * </pre>
     *
     * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 3;</code>
     * @return The keys.
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto getKeys();
    /**
     * <pre>
     * Block keys
     * </pre>
     *
     * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 3;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProtoOrBuilder getKeysOrBuilder();

    /**
     * <pre>
     * Software version of the DN, e.g. "2.0.0"
     * </pre>
     *
     * <code>required string softwareVersion = 4;</code>
     * @return Whether the softwareVersion field is set.
     */
    boolean hasSoftwareVersion();
    /**
     * <pre>
     * Software version of the DN, e.g. "2.0.0"
     * </pre>
     *
     * <code>required string softwareVersion = 4;</code>
     * @return The softwareVersion.
     */
    java.lang.String getSoftwareVersion();
    /**
     * <pre>
     * Software version of the DN, e.g. "2.0.0"
     * </pre>
     *
     * <code>required string softwareVersion = 4;</code>
     * @return The bytes for softwareVersion.
     */
    com.google.protobuf.ByteString
        getSoftwareVersionBytes();
  }
  /**
   * <pre>
   **
   * Information to identify a datanode to a namenode
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.DatanodeRegistrationProto}
   */
  public static final class DatanodeRegistrationProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.DatanodeRegistrationProto)
      DatanodeRegistrationProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use DatanodeRegistrationProto.newBuilder() to construct.
    private DatanodeRegistrationProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private DatanodeRegistrationProto() {
      softwareVersion_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new DatanodeRegistrationProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private DatanodeRegistrationProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = datanodeID_.toBuilder();
              }
              datanodeID_ = input.readMessage(org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(datanodeID_);
                datanodeID_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) != 0)) {
                subBuilder = storageInfo_.toBuilder();
              }
              storageInfo_ = input.readMessage(org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(storageInfo_);
                storageInfo_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            case 26: {
              org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000004) != 0)) {
                subBuilder = keys_.toBuilder();
              }
              keys_ = input.readMessage(org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(keys_);
                keys_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000004;
              break;
            }
            case 34: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000008;
              softwareVersion_ = bs;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeRegistrationProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeRegistrationProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder.class);
    }

    private int bitField0_;
    public static final int DATANODEID_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto datanodeID_;
    /**
     * <pre>
     * Datanode information
     * </pre>
     *
     * <code>required .org.apache.hadoop.DatanodeIDProto datanodeID = 1;</code>
     * @return Whether the datanodeID field is set.
     */
    @java.lang.Override
    public boolean hasDatanodeID() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <pre>
     * Datanode information
     * </pre>
     *
     * <code>required .org.apache.hadoop.DatanodeIDProto datanodeID = 1;</code>
     * @return The datanodeID.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto getDatanodeID() {
      return datanodeID_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.getDefaultInstance() : datanodeID_;
    }
    /**
     * <pre>
     * Datanode information
     * </pre>
     *
     * <code>required .org.apache.hadoop.DatanodeIDProto datanodeID = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProtoOrBuilder getDatanodeIDOrBuilder() {
      return datanodeID_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.getDefaultInstance() : datanodeID_;
    }

    public static final int STORAGEINFO_FIELD_NUMBER = 2;
    private org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto storageInfo_;
    /**
     * <pre>
     * Node information
     * </pre>
     *
     * <code>required .org.apache.hadoop.StorageInfoProto storageInfo = 2;</code>
     * @return Whether the storageInfo field is set.
     */
    @java.lang.Override
    public boolean hasStorageInfo() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <pre>
     * Node information
     * </pre>
     *
     * <code>required .org.apache.hadoop.StorageInfoProto storageInfo = 2;</code>
     * @return The storageInfo.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto getStorageInfo() {
      return storageInfo_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto.getDefaultInstance() : storageInfo_;
    }
    /**
     * <pre>
     * Node information
     * </pre>
     *
     * <code>required .org.apache.hadoop.StorageInfoProto storageInfo = 2;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProtoOrBuilder getStorageInfoOrBuilder() {
      return storageInfo_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto.getDefaultInstance() : storageInfo_;
    }

    public static final int KEYS_FIELD_NUMBER = 3;
    private org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto keys_;
    /**
     * <pre>
     * Block keys
     * </pre>
     *
     * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 3;</code>
     * @return Whether the keys field is set.
     */
    @java.lang.Override
    public boolean hasKeys() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <pre>
     * Block keys
     * </pre>
     *
     * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 3;</code>
     * @return The keys.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto getKeys() {
      return keys_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.getDefaultInstance() : keys_;
    }
    /**
     * <pre>
     * Block keys
     * </pre>
     *
     * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 3;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProtoOrBuilder getKeysOrBuilder() {
      return keys_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.getDefaultInstance() : keys_;
    }

    public static final int SOFTWAREVERSION_FIELD_NUMBER = 4;
    private volatile java.lang.Object softwareVersion_;
    /**
     * <pre>
     * Software version of the DN, e.g. "2.0.0"
     * </pre>
     *
     * <code>required string softwareVersion = 4;</code>
     * @return Whether the softwareVersion field is set.
     */
    @java.lang.Override
    public boolean hasSoftwareVersion() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <pre>
     * Software version of the DN, e.g. "2.0.0"
     * </pre>
     *
     * <code>required string softwareVersion = 4;</code>
     * @return The softwareVersion.
     */
    @java.lang.Override
    public java.lang.String getSoftwareVersion() {
      java.lang.Object ref = softwareVersion_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          softwareVersion_ = s;
        }
        return s;
      }
    }
    /**
     * <pre>
     * Software version of the DN, e.g. "2.0.0"
     * </pre>
     *
     * <code>required string softwareVersion = 4;</code>
     * @return The bytes for softwareVersion.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getSoftwareVersionBytes() {
      java.lang.Object ref = softwareVersion_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        softwareVersion_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasDatanodeID()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasStorageInfo()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasKeys()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasSoftwareVersion()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getDatanodeID().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getStorageInfo().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getKeys().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getDatanodeID());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeMessage(2, getStorageInfo());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeMessage(3, getKeys());
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 4, softwareVersion_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getDatanodeID());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, getStorageInfo());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, getKeys());
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(4, softwareVersion_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto) obj;

      if (hasDatanodeID() != other.hasDatanodeID()) return false;
      if (hasDatanodeID()) {
        if (!getDatanodeID()
            .equals(other.getDatanodeID())) return false;
      }
      if (hasStorageInfo() != other.hasStorageInfo()) return false;
      if (hasStorageInfo()) {
        if (!getStorageInfo()
            .equals(other.getStorageInfo())) return false;
      }
      if (hasKeys() != other.hasKeys()) return false;
      if (hasKeys()) {
        if (!getKeys()
            .equals(other.getKeys())) return false;
      }
      if (hasSoftwareVersion() != other.hasSoftwareVersion()) return false;
      if (hasSoftwareVersion()) {
        if (!getSoftwareVersion()
            .equals(other.getSoftwareVersion())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasDatanodeID()) {
        hash = (37 * hash) + DATANODEID_FIELD_NUMBER;
        hash = (53 * hash) + getDatanodeID().hashCode();
      }
      if (hasStorageInfo()) {
        hash = (37 * hash) + STORAGEINFO_FIELD_NUMBER;
        hash = (53 * hash) + getStorageInfo().hashCode();
      }
      if (hasKeys()) {
        hash = (37 * hash) + KEYS_FIELD_NUMBER;
        hash = (53 * hash) + getKeys().hashCode();
      }
      if (hasSoftwareVersion()) {
        hash = (37 * hash) + SOFTWAREVERSION_FIELD_NUMBER;
        hash = (53 * hash) + getSoftwareVersion().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * Information to identify a datanode to a namenode
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.DatanodeRegistrationProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.DatanodeRegistrationProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeRegistrationProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeRegistrationProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getDatanodeIDFieldBuilder();
          getStorageInfoFieldBuilder();
          getKeysFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (datanodeIDBuilder_ == null) {
          datanodeID_ = null;
        } else {
          datanodeIDBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (storageInfoBuilder_ == null) {
          storageInfo_ = null;
        } else {
          storageInfoBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        if (keysBuilder_ == null) {
          keys_ = null;
        } else {
          keysBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        softwareVersion_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeRegistrationProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (datanodeIDBuilder_ == null) {
            result.datanodeID_ = datanodeID_;
          } else {
            result.datanodeID_ = datanodeIDBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          if (storageInfoBuilder_ == null) {
            result.storageInfo_ = storageInfo_;
          } else {
            result.storageInfo_ = storageInfoBuilder_.build();
          }
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          if (keysBuilder_ == null) {
            result.keys_ = keys_;
          } else {
            result.keys_ = keysBuilder_.build();
          }
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          to_bitField0_ |= 0x00000008;
        }
        result.softwareVersion_ = softwareVersion_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance()) return this;
        if (other.hasDatanodeID()) {
          mergeDatanodeID(other.getDatanodeID());
        }
        if (other.hasStorageInfo()) {
          mergeStorageInfo(other.getStorageInfo());
        }
        if (other.hasKeys()) {
          mergeKeys(other.getKeys());
        }
        if (other.hasSoftwareVersion()) {
          bitField0_ |= 0x00000008;
          softwareVersion_ = other.softwareVersion_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasDatanodeID()) {
          return false;
        }
        if (!hasStorageInfo()) {
          return false;
        }
        if (!hasKeys()) {
          return false;
        }
        if (!hasSoftwareVersion()) {
          return false;
        }
        if (!getDatanodeID().isInitialized()) {
          return false;
        }
        if (!getStorageInfo().isInitialized()) {
          return false;
        }
        if (!getKeys().isInitialized()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto datanodeID_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProtoOrBuilder> datanodeIDBuilder_;
      /**
       * <pre>
       * Datanode information
       * </pre>
       *
       * <code>required .org.apache.hadoop.DatanodeIDProto datanodeID = 1;</code>
       * @return Whether the datanodeID field is set.
       */
      public boolean hasDatanodeID() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * Datanode information
       * </pre>
       *
       * <code>required .org.apache.hadoop.DatanodeIDProto datanodeID = 1;</code>
       * @return The datanodeID.
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto getDatanodeID() {
        if (datanodeIDBuilder_ == null) {
          return datanodeID_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.getDefaultInstance() : datanodeID_;
        } else {
          return datanodeIDBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * Datanode information
       * </pre>
       *
       * <code>required .org.apache.hadoop.DatanodeIDProto datanodeID = 1;</code>
       */
      public Builder setDatanodeID(org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto value) {
        if (datanodeIDBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          datanodeID_ = value;
          onChanged();
        } else {
          datanodeIDBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <pre>
       * Datanode information
       * </pre>
       *
       * <code>required .org.apache.hadoop.DatanodeIDProto datanodeID = 1;</code>
       */
      public Builder setDatanodeID(
          org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.Builder builderForValue) {
        if (datanodeIDBuilder_ == null) {
          datanodeID_ = builderForValue.build();
          onChanged();
        } else {
          datanodeIDBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <pre>
       * Datanode information
       * </pre>
       *
       * <code>required .org.apache.hadoop.DatanodeIDProto datanodeID = 1;</code>
       */
      public Builder mergeDatanodeID(org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto value) {
        if (datanodeIDBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              datanodeID_ != null &&
              datanodeID_ != org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.getDefaultInstance()) {
            datanodeID_ =
              org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.newBuilder(datanodeID_).mergeFrom(value).buildPartial();
          } else {
            datanodeID_ = value;
          }
          onChanged();
        } else {
          datanodeIDBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <pre>
       * Datanode information
       * </pre>
       *
       * <code>required .org.apache.hadoop.DatanodeIDProto datanodeID = 1;</code>
       */
      public Builder clearDatanodeID() {
        if (datanodeIDBuilder_ == null) {
          datanodeID_ = null;
          onChanged();
        } else {
          datanodeIDBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <pre>
       * Datanode information
       * </pre>
       *
       * <code>required .org.apache.hadoop.DatanodeIDProto datanodeID = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.Builder getDatanodeIDBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getDatanodeIDFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * Datanode information
       * </pre>
       *
       * <code>required .org.apache.hadoop.DatanodeIDProto datanodeID = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProtoOrBuilder getDatanodeIDOrBuilder() {
        if (datanodeIDBuilder_ != null) {
          return datanodeIDBuilder_.getMessageOrBuilder();
        } else {
          return datanodeID_ == null ?
              org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.getDefaultInstance() : datanodeID_;
        }
      }
      /**
       * <pre>
       * Datanode information
       * </pre>
       *
       * <code>required .org.apache.hadoop.DatanodeIDProto datanodeID = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProtoOrBuilder>
          getDatanodeIDFieldBuilder() {
        if (datanodeIDBuilder_ == null) {
          datanodeIDBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProtoOrBuilder>(
                  getDatanodeID(),
                  getParentForChildren(),
                  isClean());
          datanodeID_ = null;
        }
        return datanodeIDBuilder_;
      }

      private org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto storageInfo_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProtoOrBuilder> storageInfoBuilder_;
      /**
       * <pre>
       * Node information
       * </pre>
       *
       * <code>required .org.apache.hadoop.StorageInfoProto storageInfo = 2;</code>
       * @return Whether the storageInfo field is set.
       */
      public boolean hasStorageInfo() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <pre>
       * Node information
       * </pre>
       *
       * <code>required .org.apache.hadoop.StorageInfoProto storageInfo = 2;</code>
       * @return The storageInfo.
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto getStorageInfo() {
        if (storageInfoBuilder_ == null) {
          return storageInfo_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto.getDefaultInstance() : storageInfo_;
        } else {
          return storageInfoBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * Node information
       * </pre>
       *
       * <code>required .org.apache.hadoop.StorageInfoProto storageInfo = 2;</code>
       */
      public Builder setStorageInfo(org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto value) {
        if (storageInfoBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          storageInfo_ = value;
          onChanged();
        } else {
          storageInfoBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <pre>
       * Node information
       * </pre>
       *
       * <code>required .org.apache.hadoop.StorageInfoProto storageInfo = 2;</code>
       */
      public Builder setStorageInfo(
          org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto.Builder builderForValue) {
        if (storageInfoBuilder_ == null) {
          storageInfo_ = builderForValue.build();
          onChanged();
        } else {
          storageInfoBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <pre>
       * Node information
       * </pre>
       *
       * <code>required .org.apache.hadoop.StorageInfoProto storageInfo = 2;</code>
       */
      public Builder mergeStorageInfo(org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto value) {
        if (storageInfoBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0) &&
              storageInfo_ != null &&
              storageInfo_ != org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto.getDefaultInstance()) {
            storageInfo_ =
              org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto.newBuilder(storageInfo_).mergeFrom(value).buildPartial();
          } else {
            storageInfo_ = value;
          }
          onChanged();
        } else {
          storageInfoBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <pre>
       * Node information
       * </pre>
       *
       * <code>required .org.apache.hadoop.StorageInfoProto storageInfo = 2;</code>
       */
      public Builder clearStorageInfo() {
        if (storageInfoBuilder_ == null) {
          storageInfo_ = null;
          onChanged();
        } else {
          storageInfoBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <pre>
       * Node information
       * </pre>
       *
       * <code>required .org.apache.hadoop.StorageInfoProto storageInfo = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto.Builder getStorageInfoBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getStorageInfoFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * Node information
       * </pre>
       *
       * <code>required .org.apache.hadoop.StorageInfoProto storageInfo = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProtoOrBuilder getStorageInfoOrBuilder() {
        if (storageInfoBuilder_ != null) {
          return storageInfoBuilder_.getMessageOrBuilder();
        } else {
          return storageInfo_ == null ?
              org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto.getDefaultInstance() : storageInfo_;
        }
      }
      /**
       * <pre>
       * Node information
       * </pre>
       *
       * <code>required .org.apache.hadoop.StorageInfoProto storageInfo = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProtoOrBuilder>
          getStorageInfoFieldBuilder() {
        if (storageInfoBuilder_ == null) {
          storageInfoBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageInfoProtoOrBuilder>(
                  getStorageInfo(),
                  getParentForChildren(),
                  isClean());
          storageInfo_ = null;
        }
        return storageInfoBuilder_;
      }

      private org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto keys_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProtoOrBuilder> keysBuilder_;
      /**
       * <pre>
       * Block keys
       * </pre>
       *
       * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 3;</code>
       * @return Whether the keys field is set.
       */
      public boolean hasKeys() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <pre>
       * Block keys
       * </pre>
       *
       * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 3;</code>
       * @return The keys.
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto getKeys() {
        if (keysBuilder_ == null) {
          return keys_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.getDefaultInstance() : keys_;
        } else {
          return keysBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * Block keys
       * </pre>
       *
       * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 3;</code>
       */
      public Builder setKeys(org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto value) {
        if (keysBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          keys_ = value;
          onChanged();
        } else {
          keysBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <pre>
       * Block keys
       * </pre>
       *
       * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 3;</code>
       */
      public Builder setKeys(
          org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.Builder builderForValue) {
        if (keysBuilder_ == null) {
          keys_ = builderForValue.build();
          onChanged();
        } else {
          keysBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <pre>
       * Block keys
       * </pre>
       *
       * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 3;</code>
       */
      public Builder mergeKeys(org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto value) {
        if (keysBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0) &&
              keys_ != null &&
              keys_ != org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.getDefaultInstance()) {
            keys_ =
              org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.newBuilder(keys_).mergeFrom(value).buildPartial();
          } else {
            keys_ = value;
          }
          onChanged();
        } else {
          keysBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <pre>
       * Block keys
       * </pre>
       *
       * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 3;</code>
       */
      public Builder clearKeys() {
        if (keysBuilder_ == null) {
          keys_ = null;
          onChanged();
        } else {
          keysBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }
      /**
       * <pre>
       * Block keys
       * </pre>
       *
       * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.Builder getKeysBuilder() {
        bitField0_ |= 0x00000004;
        onChanged();
        return getKeysFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * Block keys
       * </pre>
       *
       * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProtoOrBuilder getKeysOrBuilder() {
        if (keysBuilder_ != null) {
          return keysBuilder_.getMessageOrBuilder();
        } else {
          return keys_ == null ?
              org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.getDefaultInstance() : keys_;
        }
      }
      /**
       * <pre>
       * Block keys
       * </pre>
       *
       * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProtoOrBuilder>
          getKeysFieldBuilder() {
        if (keysBuilder_ == null) {
          keysBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProtoOrBuilder>(
                  getKeys(),
                  getParentForChildren(),
                  isClean());
          keys_ = null;
        }
        return keysBuilder_;
      }

      private java.lang.Object softwareVersion_ = "";
      /**
       * <pre>
       * Software version of the DN, e.g. "2.0.0"
       * </pre>
       *
       * <code>required string softwareVersion = 4;</code>
       * @return Whether the softwareVersion field is set.
       */
      public boolean hasSoftwareVersion() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <pre>
       * Software version of the DN, e.g. "2.0.0"
       * </pre>
       *
       * <code>required string softwareVersion = 4;</code>
       * @return The softwareVersion.
       */
      public java.lang.String getSoftwareVersion() {
        java.lang.Object ref = softwareVersion_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            softwareVersion_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       * Software version of the DN, e.g. "2.0.0"
       * </pre>
       *
       * <code>required string softwareVersion = 4;</code>
       * @return The bytes for softwareVersion.
       */
      public com.google.protobuf.ByteString
          getSoftwareVersionBytes() {
        java.lang.Object ref = softwareVersion_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          softwareVersion_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       * Software version of the DN, e.g. "2.0.0"
       * </pre>
       *
       * <code>required string softwareVersion = 4;</code>
       * @param value The softwareVersion to set.
       * @return This builder for chaining.
       */
      public Builder setSoftwareVersion(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        softwareVersion_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Software version of the DN, e.g. "2.0.0"
       * </pre>
       *
       * <code>required string softwareVersion = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearSoftwareVersion() {
        bitField0_ = (bitField0_ & ~0x00000008);
        softwareVersion_ = getDefaultInstance().getSoftwareVersion();
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Software version of the DN, e.g. "2.0.0"
       * </pre>
       *
       * <code>required string softwareVersion = 4;</code>
       * @param value The bytes for softwareVersion to set.
       * @return This builder for chaining.
       */
      public Builder setSoftwareVersionBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        softwareVersion_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.DatanodeRegistrationProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.DatanodeRegistrationProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<DatanodeRegistrationProto>
        PARSER = new com.google.protobuf.AbstractParser<DatanodeRegistrationProto>() {
      @java.lang.Override
      public DatanodeRegistrationProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new DatanodeRegistrationProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<DatanodeRegistrationProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<DatanodeRegistrationProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface DatanodeCommandProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.DatanodeCommandProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * Type of the command
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeCommandProto.Type cmdType = 1;</code>
     * @return Whether the cmdType field is set.
     */
    boolean hasCmdType();
    /**
     * <pre>
     * Type of the command
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeCommandProto.Type cmdType = 1;</code>
     * @return The cmdType.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Type getCmdType();

    /**
     * <pre>
     * One of the following command is available when the corresponding
     * cmdType is set
     * </pre>
     *
     * <code>optional .org.apache.hadoop.datanode.BalancerBandwidthCommandProto balancerCmd = 2;</code>
     * @return Whether the balancerCmd field is set.
     */
    boolean hasBalancerCmd();
    /**
     * <pre>
     * One of the following command is available when the corresponding
     * cmdType is set
     * </pre>
     *
     * <code>optional .org.apache.hadoop.datanode.BalancerBandwidthCommandProto balancerCmd = 2;</code>
     * @return The balancerCmd.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto getBalancerCmd();
    /**
     * <pre>
     * One of the following command is available when the corresponding
     * cmdType is set
     * </pre>
     *
     * <code>optional .org.apache.hadoop.datanode.BalancerBandwidthCommandProto balancerCmd = 2;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProtoOrBuilder getBalancerCmdOrBuilder();

    /**
     * <code>optional .org.apache.hadoop.datanode.BlockCommandProto blkCmd = 3;</code>
     * @return Whether the blkCmd field is set.
     */
    boolean hasBlkCmd();
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockCommandProto blkCmd = 3;</code>
     * @return The blkCmd.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto getBlkCmd();
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockCommandProto blkCmd = 3;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProtoOrBuilder getBlkCmdOrBuilder();

    /**
     * <code>optional .org.apache.hadoop.datanode.BlockRecoveryCommandProto recoveryCmd = 4;</code>
     * @return Whether the recoveryCmd field is set.
     */
    boolean hasRecoveryCmd();
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockRecoveryCommandProto recoveryCmd = 4;</code>
     * @return The recoveryCmd.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto getRecoveryCmd();
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockRecoveryCommandProto recoveryCmd = 4;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProtoOrBuilder getRecoveryCmdOrBuilder();

    /**
     * <code>optional .org.apache.hadoop.datanode.FinalizeCommandProto finalizeCmd = 5;</code>
     * @return Whether the finalizeCmd field is set.
     */
    boolean hasFinalizeCmd();
    /**
     * <code>optional .org.apache.hadoop.datanode.FinalizeCommandProto finalizeCmd = 5;</code>
     * @return The finalizeCmd.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto getFinalizeCmd();
    /**
     * <code>optional .org.apache.hadoop.datanode.FinalizeCommandProto finalizeCmd = 5;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProtoOrBuilder getFinalizeCmdOrBuilder();

    /**
     * <code>optional .org.apache.hadoop.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;</code>
     * @return Whether the keyUpdateCmd field is set.
     */
    boolean hasKeyUpdateCmd();
    /**
     * <code>optional .org.apache.hadoop.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;</code>
     * @return The keyUpdateCmd.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto getKeyUpdateCmd();
    /**
     * <code>optional .org.apache.hadoop.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProtoOrBuilder getKeyUpdateCmdOrBuilder();

    /**
     * <code>optional .org.apache.hadoop.datanode.RegisterCommandProto registerCmd = 7;</code>
     * @return Whether the registerCmd field is set.
     */
    boolean hasRegisterCmd();
    /**
     * <code>optional .org.apache.hadoop.datanode.RegisterCommandProto registerCmd = 7;</code>
     * @return The registerCmd.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto getRegisterCmd();
    /**
     * <code>optional .org.apache.hadoop.datanode.RegisterCommandProto registerCmd = 7;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProtoOrBuilder getRegisterCmdOrBuilder();

    /**
     * <code>optional .org.apache.hadoop.datanode.BlockIdCommandProto blkIdCmd = 8;</code>
     * @return Whether the blkIdCmd field is set.
     */
    boolean hasBlkIdCmd();
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockIdCommandProto blkIdCmd = 8;</code>
     * @return The blkIdCmd.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto getBlkIdCmd();
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockIdCommandProto blkIdCmd = 8;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProtoOrBuilder getBlkIdCmdOrBuilder();

    /**
     * <code>optional .org.apache.hadoop.datanode.HashMismatchCommandProto mismatchHashesCmd = 11;</code>
     * @return Whether the mismatchHashesCmd field is set.
     */
    boolean hasMismatchHashesCmd();
    /**
     * <code>optional .org.apache.hadoop.datanode.HashMismatchCommandProto mismatchHashesCmd = 11;</code>
     * @return The mismatchHashesCmd.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto getMismatchHashesCmd();
    /**
     * <code>optional .org.apache.hadoop.datanode.HashMismatchCommandProto mismatchHashesCmd = 11;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProtoOrBuilder getMismatchHashesCmdOrBuilder();
  }
  /**
   * <pre>
   **
   * Commands sent from namenode to the datanodes
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.DatanodeCommandProto}
   */
  public static final class DatanodeCommandProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.DatanodeCommandProto)
      DatanodeCommandProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use DatanodeCommandProto.newBuilder() to construct.
    private DatanodeCommandProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private DatanodeCommandProto() {
      cmdType_ = 0;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new DatanodeCommandProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private DatanodeCommandProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Type value = org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Type.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(1, rawValue);
              } else {
                bitField0_ |= 0x00000001;
                cmdType_ = rawValue;
              }
              break;
            }
            case 18: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) != 0)) {
                subBuilder = balancerCmd_.toBuilder();
              }
              balancerCmd_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(balancerCmd_);
                balancerCmd_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            case 26: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000004) != 0)) {
                subBuilder = blkCmd_.toBuilder();
              }
              blkCmd_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(blkCmd_);
                blkCmd_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000004;
              break;
            }
            case 34: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000008) != 0)) {
                subBuilder = recoveryCmd_.toBuilder();
              }
              recoveryCmd_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(recoveryCmd_);
                recoveryCmd_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000008;
              break;
            }
            case 42: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000010) != 0)) {
                subBuilder = finalizeCmd_.toBuilder();
              }
              finalizeCmd_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(finalizeCmd_);
                finalizeCmd_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000010;
              break;
            }
            case 50: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000020) != 0)) {
                subBuilder = keyUpdateCmd_.toBuilder();
              }
              keyUpdateCmd_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(keyUpdateCmd_);
                keyUpdateCmd_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000020;
              break;
            }
            case 58: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000040) != 0)) {
                subBuilder = registerCmd_.toBuilder();
              }
              registerCmd_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(registerCmd_);
                registerCmd_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000040;
              break;
            }
            case 66: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000080) != 0)) {
                subBuilder = blkIdCmd_.toBuilder();
              }
              blkIdCmd_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(blkIdCmd_);
                blkIdCmd_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000080;
              break;
            }
            case 90: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000100) != 0)) {
                subBuilder = mismatchHashesCmd_.toBuilder();
              }
              mismatchHashesCmd_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(mismatchHashesCmd_);
                mismatchHashesCmd_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000100;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeCommandProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeCommandProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder.class);
    }

    /**
     * Protobuf enum {@code org.apache.hadoop.datanode.DatanodeCommandProto.Type}
     */
    public enum Type
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <code>BalancerBandwidthCommand = 0;</code>
       */
      BalancerBandwidthCommand(0),
      /**
       * <code>BlockCommand = 1;</code>
       */
      BlockCommand(1),
      /**
       * <code>BlockRecoveryCommand = 2;</code>
       */
      BlockRecoveryCommand(2),
      /**
       * <code>FinalizeCommand = 3;</code>
       */
      FinalizeCommand(3),
      /**
       * <code>KeyUpdateCommand = 4;</code>
       */
      KeyUpdateCommand(4),
      /**
       * <code>RegisterCommand = 5;</code>
       */
      RegisterCommand(5),
      /**
       * <code>UnusedUpgradeCommand = 6;</code>
       */
      UnusedUpgradeCommand(6),
      /**
       * <code>NullDatanodeCommand = 7;</code>
       */
      NullDatanodeCommand(7),
      /**
       * <code>BlockIdCommand = 8;</code>
       */
      BlockIdCommand(8),
      /**
       * <code>HashMismatchCommand = 11;</code>
       */
      HashMismatchCommand(11),
      ;

      /**
       * <code>BalancerBandwidthCommand = 0;</code>
       */
      public static final int BalancerBandwidthCommand_VALUE = 0;
      /**
       * <code>BlockCommand = 1;</code>
       */
      public static final int BlockCommand_VALUE = 1;
      /**
       * <code>BlockRecoveryCommand = 2;</code>
       */
      public static final int BlockRecoveryCommand_VALUE = 2;
      /**
       * <code>FinalizeCommand = 3;</code>
       */
      public static final int FinalizeCommand_VALUE = 3;
      /**
       * <code>KeyUpdateCommand = 4;</code>
       */
      public static final int KeyUpdateCommand_VALUE = 4;
      /**
       * <code>RegisterCommand = 5;</code>
       */
      public static final int RegisterCommand_VALUE = 5;
      /**
       * <code>UnusedUpgradeCommand = 6;</code>
       */
      public static final int UnusedUpgradeCommand_VALUE = 6;
      /**
       * <code>NullDatanodeCommand = 7;</code>
       */
      public static final int NullDatanodeCommand_VALUE = 7;
      /**
       * <code>BlockIdCommand = 8;</code>
       */
      public static final int BlockIdCommand_VALUE = 8;
      /**
       * <code>HashMismatchCommand = 11;</code>
       */
      public static final int HashMismatchCommand_VALUE = 11;


      public final int getNumber() {
        return value;
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static Type valueOf(int value) {
        return forNumber(value);
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       */
      public static Type forNumber(int value) {
        switch (value) {
          case 0: return BalancerBandwidthCommand;
          case 1: return BlockCommand;
          case 2: return BlockRecoveryCommand;
          case 3: return FinalizeCommand;
          case 4: return KeyUpdateCommand;
          case 5: return RegisterCommand;
          case 6: return UnusedUpgradeCommand;
          case 7: return NullDatanodeCommand;
          case 8: return BlockIdCommand;
          case 11: return HashMismatchCommand;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<Type>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          Type> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<Type>() {
              public Type findValueByNumber(int number) {
                return Type.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.getDescriptor().getEnumTypes().get(0);
      }

      private static final Type[] VALUES = values();

      public static Type valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private Type(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:org.apache.hadoop.datanode.DatanodeCommandProto.Type)
    }

    private int bitField0_;
    public static final int CMDTYPE_FIELD_NUMBER = 1;
    private int cmdType_;
    /**
     * <pre>
     * Type of the command
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeCommandProto.Type cmdType = 1;</code>
     * @return Whether the cmdType field is set.
     */
    @java.lang.Override public boolean hasCmdType() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <pre>
     * Type of the command
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeCommandProto.Type cmdType = 1;</code>
     * @return The cmdType.
     */
    @java.lang.Override public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Type getCmdType() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Type result = org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Type.valueOf(cmdType_);
      return result == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Type.BalancerBandwidthCommand : result;
    }

    public static final int BALANCERCMD_FIELD_NUMBER = 2;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto balancerCmd_;
    /**
     * <pre>
     * One of the following command is available when the corresponding
     * cmdType is set
     * </pre>
     *
     * <code>optional .org.apache.hadoop.datanode.BalancerBandwidthCommandProto balancerCmd = 2;</code>
     * @return Whether the balancerCmd field is set.
     */
    @java.lang.Override
    public boolean hasBalancerCmd() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <pre>
     * One of the following command is available when the corresponding
     * cmdType is set
     * </pre>
     *
     * <code>optional .org.apache.hadoop.datanode.BalancerBandwidthCommandProto balancerCmd = 2;</code>
     * @return The balancerCmd.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto getBalancerCmd() {
      return balancerCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.getDefaultInstance() : balancerCmd_;
    }
    /**
     * <pre>
     * One of the following command is available when the corresponding
     * cmdType is set
     * </pre>
     *
     * <code>optional .org.apache.hadoop.datanode.BalancerBandwidthCommandProto balancerCmd = 2;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProtoOrBuilder getBalancerCmdOrBuilder() {
      return balancerCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.getDefaultInstance() : balancerCmd_;
    }

    public static final int BLKCMD_FIELD_NUMBER = 3;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto blkCmd_;
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockCommandProto blkCmd = 3;</code>
     * @return Whether the blkCmd field is set.
     */
    @java.lang.Override
    public boolean hasBlkCmd() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockCommandProto blkCmd = 3;</code>
     * @return The blkCmd.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto getBlkCmd() {
      return blkCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.getDefaultInstance() : blkCmd_;
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockCommandProto blkCmd = 3;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProtoOrBuilder getBlkCmdOrBuilder() {
      return blkCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.getDefaultInstance() : blkCmd_;
    }

    public static final int RECOVERYCMD_FIELD_NUMBER = 4;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto recoveryCmd_;
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockRecoveryCommandProto recoveryCmd = 4;</code>
     * @return Whether the recoveryCmd field is set.
     */
    @java.lang.Override
    public boolean hasRecoveryCmd() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockRecoveryCommandProto recoveryCmd = 4;</code>
     * @return The recoveryCmd.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto getRecoveryCmd() {
      return recoveryCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.getDefaultInstance() : recoveryCmd_;
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockRecoveryCommandProto recoveryCmd = 4;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProtoOrBuilder getRecoveryCmdOrBuilder() {
      return recoveryCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.getDefaultInstance() : recoveryCmd_;
    }

    public static final int FINALIZECMD_FIELD_NUMBER = 5;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto finalizeCmd_;
    /**
     * <code>optional .org.apache.hadoop.datanode.FinalizeCommandProto finalizeCmd = 5;</code>
     * @return Whether the finalizeCmd field is set.
     */
    @java.lang.Override
    public boolean hasFinalizeCmd() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.FinalizeCommandProto finalizeCmd = 5;</code>
     * @return The finalizeCmd.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto getFinalizeCmd() {
      return finalizeCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.getDefaultInstance() : finalizeCmd_;
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.FinalizeCommandProto finalizeCmd = 5;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProtoOrBuilder getFinalizeCmdOrBuilder() {
      return finalizeCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.getDefaultInstance() : finalizeCmd_;
    }

    public static final int KEYUPDATECMD_FIELD_NUMBER = 6;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto keyUpdateCmd_;
    /**
     * <code>optional .org.apache.hadoop.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;</code>
     * @return Whether the keyUpdateCmd field is set.
     */
    @java.lang.Override
    public boolean hasKeyUpdateCmd() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;</code>
     * @return The keyUpdateCmd.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto getKeyUpdateCmd() {
      return keyUpdateCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.getDefaultInstance() : keyUpdateCmd_;
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProtoOrBuilder getKeyUpdateCmdOrBuilder() {
      return keyUpdateCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.getDefaultInstance() : keyUpdateCmd_;
    }

    public static final int REGISTERCMD_FIELD_NUMBER = 7;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto registerCmd_;
    /**
     * <code>optional .org.apache.hadoop.datanode.RegisterCommandProto registerCmd = 7;</code>
     * @return Whether the registerCmd field is set.
     */
    @java.lang.Override
    public boolean hasRegisterCmd() {
      return ((bitField0_ & 0x00000040) != 0);
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.RegisterCommandProto registerCmd = 7;</code>
     * @return The registerCmd.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto getRegisterCmd() {
      return registerCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.getDefaultInstance() : registerCmd_;
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.RegisterCommandProto registerCmd = 7;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProtoOrBuilder getRegisterCmdOrBuilder() {
      return registerCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.getDefaultInstance() : registerCmd_;
    }

    public static final int BLKIDCMD_FIELD_NUMBER = 8;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto blkIdCmd_;
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockIdCommandProto blkIdCmd = 8;</code>
     * @return Whether the blkIdCmd field is set.
     */
    @java.lang.Override
    public boolean hasBlkIdCmd() {
      return ((bitField0_ & 0x00000080) != 0);
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockIdCommandProto blkIdCmd = 8;</code>
     * @return The blkIdCmd.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto getBlkIdCmd() {
      return blkIdCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.getDefaultInstance() : blkIdCmd_;
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockIdCommandProto blkIdCmd = 8;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProtoOrBuilder getBlkIdCmdOrBuilder() {
      return blkIdCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.getDefaultInstance() : blkIdCmd_;
    }

    public static final int MISMATCHHASHESCMD_FIELD_NUMBER = 11;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto mismatchHashesCmd_;
    /**
     * <code>optional .org.apache.hadoop.datanode.HashMismatchCommandProto mismatchHashesCmd = 11;</code>
     * @return Whether the mismatchHashesCmd field is set.
     */
    @java.lang.Override
    public boolean hasMismatchHashesCmd() {
      return ((bitField0_ & 0x00000100) != 0);
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.HashMismatchCommandProto mismatchHashesCmd = 11;</code>
     * @return The mismatchHashesCmd.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto getMismatchHashesCmd() {
      return mismatchHashesCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.getDefaultInstance() : mismatchHashesCmd_;
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.HashMismatchCommandProto mismatchHashesCmd = 11;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProtoOrBuilder getMismatchHashesCmdOrBuilder() {
      return mismatchHashesCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.getDefaultInstance() : mismatchHashesCmd_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasCmdType()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (hasBalancerCmd()) {
        if (!getBalancerCmd().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasBlkCmd()) {
        if (!getBlkCmd().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasRecoveryCmd()) {
        if (!getRecoveryCmd().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasFinalizeCmd()) {
        if (!getFinalizeCmd().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasKeyUpdateCmd()) {
        if (!getKeyUpdateCmd().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasBlkIdCmd()) {
        if (!getBlkIdCmd().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasMismatchHashesCmd()) {
        if (!getMismatchHashesCmd().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeEnum(1, cmdType_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeMessage(2, getBalancerCmd());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeMessage(3, getBlkCmd());
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeMessage(4, getRecoveryCmd());
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        output.writeMessage(5, getFinalizeCmd());
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        output.writeMessage(6, getKeyUpdateCmd());
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        output.writeMessage(7, getRegisterCmd());
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        output.writeMessage(8, getBlkIdCmd());
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        output.writeMessage(11, getMismatchHashesCmd());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, cmdType_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, getBalancerCmd());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, getBlkCmd());
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, getRecoveryCmd());
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, getFinalizeCmd());
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, getKeyUpdateCmd());
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(7, getRegisterCmd());
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(8, getBlkIdCmd());
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(11, getMismatchHashesCmd());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto) obj;

      if (hasCmdType() != other.hasCmdType()) return false;
      if (hasCmdType()) {
        if (cmdType_ != other.cmdType_) return false;
      }
      if (hasBalancerCmd() != other.hasBalancerCmd()) return false;
      if (hasBalancerCmd()) {
        if (!getBalancerCmd()
            .equals(other.getBalancerCmd())) return false;
      }
      if (hasBlkCmd() != other.hasBlkCmd()) return false;
      if (hasBlkCmd()) {
        if (!getBlkCmd()
            .equals(other.getBlkCmd())) return false;
      }
      if (hasRecoveryCmd() != other.hasRecoveryCmd()) return false;
      if (hasRecoveryCmd()) {
        if (!getRecoveryCmd()
            .equals(other.getRecoveryCmd())) return false;
      }
      if (hasFinalizeCmd() != other.hasFinalizeCmd()) return false;
      if (hasFinalizeCmd()) {
        if (!getFinalizeCmd()
            .equals(other.getFinalizeCmd())) return false;
      }
      if (hasKeyUpdateCmd() != other.hasKeyUpdateCmd()) return false;
      if (hasKeyUpdateCmd()) {
        if (!getKeyUpdateCmd()
            .equals(other.getKeyUpdateCmd())) return false;
      }
      if (hasRegisterCmd() != other.hasRegisterCmd()) return false;
      if (hasRegisterCmd()) {
        if (!getRegisterCmd()
            .equals(other.getRegisterCmd())) return false;
      }
      if (hasBlkIdCmd() != other.hasBlkIdCmd()) return false;
      if (hasBlkIdCmd()) {
        if (!getBlkIdCmd()
            .equals(other.getBlkIdCmd())) return false;
      }
      if (hasMismatchHashesCmd() != other.hasMismatchHashesCmd()) return false;
      if (hasMismatchHashesCmd()) {
        if (!getMismatchHashesCmd()
            .equals(other.getMismatchHashesCmd())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasCmdType()) {
        hash = (37 * hash) + CMDTYPE_FIELD_NUMBER;
        hash = (53 * hash) + cmdType_;
      }
      if (hasBalancerCmd()) {
        hash = (37 * hash) + BALANCERCMD_FIELD_NUMBER;
        hash = (53 * hash) + getBalancerCmd().hashCode();
      }
      if (hasBlkCmd()) {
        hash = (37 * hash) + BLKCMD_FIELD_NUMBER;
        hash = (53 * hash) + getBlkCmd().hashCode();
      }
      if (hasRecoveryCmd()) {
        hash = (37 * hash) + RECOVERYCMD_FIELD_NUMBER;
        hash = (53 * hash) + getRecoveryCmd().hashCode();
      }
      if (hasFinalizeCmd()) {
        hash = (37 * hash) + FINALIZECMD_FIELD_NUMBER;
        hash = (53 * hash) + getFinalizeCmd().hashCode();
      }
      if (hasKeyUpdateCmd()) {
        hash = (37 * hash) + KEYUPDATECMD_FIELD_NUMBER;
        hash = (53 * hash) + getKeyUpdateCmd().hashCode();
      }
      if (hasRegisterCmd()) {
        hash = (37 * hash) + REGISTERCMD_FIELD_NUMBER;
        hash = (53 * hash) + getRegisterCmd().hashCode();
      }
      if (hasBlkIdCmd()) {
        hash = (37 * hash) + BLKIDCMD_FIELD_NUMBER;
        hash = (53 * hash) + getBlkIdCmd().hashCode();
      }
      if (hasMismatchHashesCmd()) {
        hash = (37 * hash) + MISMATCHHASHESCMD_FIELD_NUMBER;
        hash = (53 * hash) + getMismatchHashesCmd().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * Commands sent from namenode to the datanodes
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.DatanodeCommandProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.DatanodeCommandProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeCommandProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeCommandProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getBalancerCmdFieldBuilder();
          getBlkCmdFieldBuilder();
          getRecoveryCmdFieldBuilder();
          getFinalizeCmdFieldBuilder();
          getKeyUpdateCmdFieldBuilder();
          getRegisterCmdFieldBuilder();
          getBlkIdCmdFieldBuilder();
          getMismatchHashesCmdFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        cmdType_ = 0;
        bitField0_ = (bitField0_ & ~0x00000001);
        if (balancerCmdBuilder_ == null) {
          balancerCmd_ = null;
        } else {
          balancerCmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        if (blkCmdBuilder_ == null) {
          blkCmd_ = null;
        } else {
          blkCmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        if (recoveryCmdBuilder_ == null) {
          recoveryCmd_ = null;
        } else {
          recoveryCmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000008);
        if (finalizeCmdBuilder_ == null) {
          finalizeCmd_ = null;
        } else {
          finalizeCmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        if (keyUpdateCmdBuilder_ == null) {
          keyUpdateCmd_ = null;
        } else {
          keyUpdateCmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000020);
        if (registerCmdBuilder_ == null) {
          registerCmd_ = null;
        } else {
          registerCmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000040);
        if (blkIdCmdBuilder_ == null) {
          blkIdCmd_ = null;
        } else {
          blkIdCmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        if (mismatchHashesCmdBuilder_ == null) {
          mismatchHashesCmd_ = null;
        } else {
          mismatchHashesCmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000100);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeCommandProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.cmdType_ = cmdType_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          if (balancerCmdBuilder_ == null) {
            result.balancerCmd_ = balancerCmd_;
          } else {
            result.balancerCmd_ = balancerCmdBuilder_.build();
          }
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          if (blkCmdBuilder_ == null) {
            result.blkCmd_ = blkCmd_;
          } else {
            result.blkCmd_ = blkCmdBuilder_.build();
          }
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          if (recoveryCmdBuilder_ == null) {
            result.recoveryCmd_ = recoveryCmd_;
          } else {
            result.recoveryCmd_ = recoveryCmdBuilder_.build();
          }
          to_bitField0_ |= 0x00000008;
        }
        if (((from_bitField0_ & 0x00000010) != 0)) {
          if (finalizeCmdBuilder_ == null) {
            result.finalizeCmd_ = finalizeCmd_;
          } else {
            result.finalizeCmd_ = finalizeCmdBuilder_.build();
          }
          to_bitField0_ |= 0x00000010;
        }
        if (((from_bitField0_ & 0x00000020) != 0)) {
          if (keyUpdateCmdBuilder_ == null) {
            result.keyUpdateCmd_ = keyUpdateCmd_;
          } else {
            result.keyUpdateCmd_ = keyUpdateCmdBuilder_.build();
          }
          to_bitField0_ |= 0x00000020;
        }
        if (((from_bitField0_ & 0x00000040) != 0)) {
          if (registerCmdBuilder_ == null) {
            result.registerCmd_ = registerCmd_;
          } else {
            result.registerCmd_ = registerCmdBuilder_.build();
          }
          to_bitField0_ |= 0x00000040;
        }
        if (((from_bitField0_ & 0x00000080) != 0)) {
          if (blkIdCmdBuilder_ == null) {
            result.blkIdCmd_ = blkIdCmd_;
          } else {
            result.blkIdCmd_ = blkIdCmdBuilder_.build();
          }
          to_bitField0_ |= 0x00000080;
        }
        if (((from_bitField0_ & 0x00000100) != 0)) {
          if (mismatchHashesCmdBuilder_ == null) {
            result.mismatchHashesCmd_ = mismatchHashesCmd_;
          } else {
            result.mismatchHashesCmd_ = mismatchHashesCmdBuilder_.build();
          }
          to_bitField0_ |= 0x00000100;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.getDefaultInstance()) return this;
        if (other.hasCmdType()) {
          setCmdType(other.getCmdType());
        }
        if (other.hasBalancerCmd()) {
          mergeBalancerCmd(other.getBalancerCmd());
        }
        if (other.hasBlkCmd()) {
          mergeBlkCmd(other.getBlkCmd());
        }
        if (other.hasRecoveryCmd()) {
          mergeRecoveryCmd(other.getRecoveryCmd());
        }
        if (other.hasFinalizeCmd()) {
          mergeFinalizeCmd(other.getFinalizeCmd());
        }
        if (other.hasKeyUpdateCmd()) {
          mergeKeyUpdateCmd(other.getKeyUpdateCmd());
        }
        if (other.hasRegisterCmd()) {
          mergeRegisterCmd(other.getRegisterCmd());
        }
        if (other.hasBlkIdCmd()) {
          mergeBlkIdCmd(other.getBlkIdCmd());
        }
        if (other.hasMismatchHashesCmd()) {
          mergeMismatchHashesCmd(other.getMismatchHashesCmd());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasCmdType()) {
          return false;
        }
        if (hasBalancerCmd()) {
          if (!getBalancerCmd().isInitialized()) {
            return false;
          }
        }
        if (hasBlkCmd()) {
          if (!getBlkCmd().isInitialized()) {
            return false;
          }
        }
        if (hasRecoveryCmd()) {
          if (!getRecoveryCmd().isInitialized()) {
            return false;
          }
        }
        if (hasFinalizeCmd()) {
          if (!getFinalizeCmd().isInitialized()) {
            return false;
          }
        }
        if (hasKeyUpdateCmd()) {
          if (!getKeyUpdateCmd().isInitialized()) {
            return false;
          }
        }
        if (hasBlkIdCmd()) {
          if (!getBlkIdCmd().isInitialized()) {
            return false;
          }
        }
        if (hasMismatchHashesCmd()) {
          if (!getMismatchHashesCmd().isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int cmdType_ = 0;
      /**
       * <pre>
       * Type of the command
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeCommandProto.Type cmdType = 1;</code>
       * @return Whether the cmdType field is set.
       */
      @java.lang.Override public boolean hasCmdType() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * Type of the command
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeCommandProto.Type cmdType = 1;</code>
       * @return The cmdType.
       */
      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Type getCmdType() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Type result = org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Type.valueOf(cmdType_);
        return result == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Type.BalancerBandwidthCommand : result;
      }
      /**
       * <pre>
       * Type of the command
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeCommandProto.Type cmdType = 1;</code>
       * @param value The cmdType to set.
       * @return This builder for chaining.
       */
      public Builder setCmdType(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Type value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000001;
        cmdType_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Type of the command
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeCommandProto.Type cmdType = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearCmdType() {
        bitField0_ = (bitField0_ & ~0x00000001);
        cmdType_ = 0;
        onChanged();
        return this;
      }

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto balancerCmd_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProtoOrBuilder> balancerCmdBuilder_;
      /**
       * <pre>
       * One of the following command is available when the corresponding
       * cmdType is set
       * </pre>
       *
       * <code>optional .org.apache.hadoop.datanode.BalancerBandwidthCommandProto balancerCmd = 2;</code>
       * @return Whether the balancerCmd field is set.
       */
      public boolean hasBalancerCmd() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <pre>
       * One of the following command is available when the corresponding
       * cmdType is set
       * </pre>
       *
       * <code>optional .org.apache.hadoop.datanode.BalancerBandwidthCommandProto balancerCmd = 2;</code>
       * @return The balancerCmd.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto getBalancerCmd() {
        if (balancerCmdBuilder_ == null) {
          return balancerCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.getDefaultInstance() : balancerCmd_;
        } else {
          return balancerCmdBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * One of the following command is available when the corresponding
       * cmdType is set
       * </pre>
       *
       * <code>optional .org.apache.hadoop.datanode.BalancerBandwidthCommandProto balancerCmd = 2;</code>
       */
      public Builder setBalancerCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto value) {
        if (balancerCmdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          balancerCmd_ = value;
          onChanged();
        } else {
          balancerCmdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <pre>
       * One of the following command is available when the corresponding
       * cmdType is set
       * </pre>
       *
       * <code>optional .org.apache.hadoop.datanode.BalancerBandwidthCommandProto balancerCmd = 2;</code>
       */
      public Builder setBalancerCmd(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.Builder builderForValue) {
        if (balancerCmdBuilder_ == null) {
          balancerCmd_ = builderForValue.build();
          onChanged();
        } else {
          balancerCmdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <pre>
       * One of the following command is available when the corresponding
       * cmdType is set
       * </pre>
       *
       * <code>optional .org.apache.hadoop.datanode.BalancerBandwidthCommandProto balancerCmd = 2;</code>
       */
      public Builder mergeBalancerCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto value) {
        if (balancerCmdBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0) &&
              balancerCmd_ != null &&
              balancerCmd_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.getDefaultInstance()) {
            balancerCmd_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.newBuilder(balancerCmd_).mergeFrom(value).buildPartial();
          } else {
            balancerCmd_ = value;
          }
          onChanged();
        } else {
          balancerCmdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <pre>
       * One of the following command is available when the corresponding
       * cmdType is set
       * </pre>
       *
       * <code>optional .org.apache.hadoop.datanode.BalancerBandwidthCommandProto balancerCmd = 2;</code>
       */
      public Builder clearBalancerCmd() {
        if (balancerCmdBuilder_ == null) {
          balancerCmd_ = null;
          onChanged();
        } else {
          balancerCmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <pre>
       * One of the following command is available when the corresponding
       * cmdType is set
       * </pre>
       *
       * <code>optional .org.apache.hadoop.datanode.BalancerBandwidthCommandProto balancerCmd = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.Builder getBalancerCmdBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getBalancerCmdFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * One of the following command is available when the corresponding
       * cmdType is set
       * </pre>
       *
       * <code>optional .org.apache.hadoop.datanode.BalancerBandwidthCommandProto balancerCmd = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProtoOrBuilder getBalancerCmdOrBuilder() {
        if (balancerCmdBuilder_ != null) {
          return balancerCmdBuilder_.getMessageOrBuilder();
        } else {
          return balancerCmd_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.getDefaultInstance() : balancerCmd_;
        }
      }
      /**
       * <pre>
       * One of the following command is available when the corresponding
       * cmdType is set
       * </pre>
       *
       * <code>optional .org.apache.hadoop.datanode.BalancerBandwidthCommandProto balancerCmd = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProtoOrBuilder>
          getBalancerCmdFieldBuilder() {
        if (balancerCmdBuilder_ == null) {
          balancerCmdBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProtoOrBuilder>(
                  getBalancerCmd(),
                  getParentForChildren(),
                  isClean());
          balancerCmd_ = null;
        }
        return balancerCmdBuilder_;
      }

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto blkCmd_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProtoOrBuilder> blkCmdBuilder_;
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockCommandProto blkCmd = 3;</code>
       * @return Whether the blkCmd field is set.
       */
      public boolean hasBlkCmd() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockCommandProto blkCmd = 3;</code>
       * @return The blkCmd.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto getBlkCmd() {
        if (blkCmdBuilder_ == null) {
          return blkCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.getDefaultInstance() : blkCmd_;
        } else {
          return blkCmdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockCommandProto blkCmd = 3;</code>
       */
      public Builder setBlkCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto value) {
        if (blkCmdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          blkCmd_ = value;
          onChanged();
        } else {
          blkCmdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockCommandProto blkCmd = 3;</code>
       */
      public Builder setBlkCmd(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Builder builderForValue) {
        if (blkCmdBuilder_ == null) {
          blkCmd_ = builderForValue.build();
          onChanged();
        } else {
          blkCmdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockCommandProto blkCmd = 3;</code>
       */
      public Builder mergeBlkCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto value) {
        if (blkCmdBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0) &&
              blkCmd_ != null &&
              blkCmd_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.getDefaultInstance()) {
            blkCmd_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.newBuilder(blkCmd_).mergeFrom(value).buildPartial();
          } else {
            blkCmd_ = value;
          }
          onChanged();
        } else {
          blkCmdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockCommandProto blkCmd = 3;</code>
       */
      public Builder clearBlkCmd() {
        if (blkCmdBuilder_ == null) {
          blkCmd_ = null;
          onChanged();
        } else {
          blkCmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockCommandProto blkCmd = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Builder getBlkCmdBuilder() {
        bitField0_ |= 0x00000004;
        onChanged();
        return getBlkCmdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockCommandProto blkCmd = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProtoOrBuilder getBlkCmdOrBuilder() {
        if (blkCmdBuilder_ != null) {
          return blkCmdBuilder_.getMessageOrBuilder();
        } else {
          return blkCmd_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.getDefaultInstance() : blkCmd_;
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockCommandProto blkCmd = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProtoOrBuilder>
          getBlkCmdFieldBuilder() {
        if (blkCmdBuilder_ == null) {
          blkCmdBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProtoOrBuilder>(
                  getBlkCmd(),
                  getParentForChildren(),
                  isClean());
          blkCmd_ = null;
        }
        return blkCmdBuilder_;
      }

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto recoveryCmd_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProtoOrBuilder> recoveryCmdBuilder_;
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockRecoveryCommandProto recoveryCmd = 4;</code>
       * @return Whether the recoveryCmd field is set.
       */
      public boolean hasRecoveryCmd() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockRecoveryCommandProto recoveryCmd = 4;</code>
       * @return The recoveryCmd.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto getRecoveryCmd() {
        if (recoveryCmdBuilder_ == null) {
          return recoveryCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.getDefaultInstance() : recoveryCmd_;
        } else {
          return recoveryCmdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockRecoveryCommandProto recoveryCmd = 4;</code>
       */
      public Builder setRecoveryCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto value) {
        if (recoveryCmdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          recoveryCmd_ = value;
          onChanged();
        } else {
          recoveryCmdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockRecoveryCommandProto recoveryCmd = 4;</code>
       */
      public Builder setRecoveryCmd(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.Builder builderForValue) {
        if (recoveryCmdBuilder_ == null) {
          recoveryCmd_ = builderForValue.build();
          onChanged();
        } else {
          recoveryCmdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockRecoveryCommandProto recoveryCmd = 4;</code>
       */
      public Builder mergeRecoveryCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto value) {
        if (recoveryCmdBuilder_ == null) {
          if (((bitField0_ & 0x00000008) != 0) &&
              recoveryCmd_ != null &&
              recoveryCmd_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.getDefaultInstance()) {
            recoveryCmd_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.newBuilder(recoveryCmd_).mergeFrom(value).buildPartial();
          } else {
            recoveryCmd_ = value;
          }
          onChanged();
        } else {
          recoveryCmdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockRecoveryCommandProto recoveryCmd = 4;</code>
       */
      public Builder clearRecoveryCmd() {
        if (recoveryCmdBuilder_ == null) {
          recoveryCmd_ = null;
          onChanged();
        } else {
          recoveryCmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockRecoveryCommandProto recoveryCmd = 4;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.Builder getRecoveryCmdBuilder() {
        bitField0_ |= 0x00000008;
        onChanged();
        return getRecoveryCmdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockRecoveryCommandProto recoveryCmd = 4;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProtoOrBuilder getRecoveryCmdOrBuilder() {
        if (recoveryCmdBuilder_ != null) {
          return recoveryCmdBuilder_.getMessageOrBuilder();
        } else {
          return recoveryCmd_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.getDefaultInstance() : recoveryCmd_;
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockRecoveryCommandProto recoveryCmd = 4;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProtoOrBuilder>
          getRecoveryCmdFieldBuilder() {
        if (recoveryCmdBuilder_ == null) {
          recoveryCmdBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProtoOrBuilder>(
                  getRecoveryCmd(),
                  getParentForChildren(),
                  isClean());
          recoveryCmd_ = null;
        }
        return recoveryCmdBuilder_;
      }

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto finalizeCmd_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProtoOrBuilder> finalizeCmdBuilder_;
      /**
       * <code>optional .org.apache.hadoop.datanode.FinalizeCommandProto finalizeCmd = 5;</code>
       * @return Whether the finalizeCmd field is set.
       */
      public boolean hasFinalizeCmd() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.FinalizeCommandProto finalizeCmd = 5;</code>
       * @return The finalizeCmd.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto getFinalizeCmd() {
        if (finalizeCmdBuilder_ == null) {
          return finalizeCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.getDefaultInstance() : finalizeCmd_;
        } else {
          return finalizeCmdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.FinalizeCommandProto finalizeCmd = 5;</code>
       */
      public Builder setFinalizeCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto value) {
        if (finalizeCmdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          finalizeCmd_ = value;
          onChanged();
        } else {
          finalizeCmdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.FinalizeCommandProto finalizeCmd = 5;</code>
       */
      public Builder setFinalizeCmd(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.Builder builderForValue) {
        if (finalizeCmdBuilder_ == null) {
          finalizeCmd_ = builderForValue.build();
          onChanged();
        } else {
          finalizeCmdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.FinalizeCommandProto finalizeCmd = 5;</code>
       */
      public Builder mergeFinalizeCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto value) {
        if (finalizeCmdBuilder_ == null) {
          if (((bitField0_ & 0x00000010) != 0) &&
              finalizeCmd_ != null &&
              finalizeCmd_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.getDefaultInstance()) {
            finalizeCmd_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.newBuilder(finalizeCmd_).mergeFrom(value).buildPartial();
          } else {
            finalizeCmd_ = value;
          }
          onChanged();
        } else {
          finalizeCmdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.FinalizeCommandProto finalizeCmd = 5;</code>
       */
      public Builder clearFinalizeCmd() {
        if (finalizeCmdBuilder_ == null) {
          finalizeCmd_ = null;
          onChanged();
        } else {
          finalizeCmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.FinalizeCommandProto finalizeCmd = 5;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.Builder getFinalizeCmdBuilder() {
        bitField0_ |= 0x00000010;
        onChanged();
        return getFinalizeCmdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.FinalizeCommandProto finalizeCmd = 5;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProtoOrBuilder getFinalizeCmdOrBuilder() {
        if (finalizeCmdBuilder_ != null) {
          return finalizeCmdBuilder_.getMessageOrBuilder();
        } else {
          return finalizeCmd_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.getDefaultInstance() : finalizeCmd_;
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.FinalizeCommandProto finalizeCmd = 5;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProtoOrBuilder>
          getFinalizeCmdFieldBuilder() {
        if (finalizeCmdBuilder_ == null) {
          finalizeCmdBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProtoOrBuilder>(
                  getFinalizeCmd(),
                  getParentForChildren(),
                  isClean());
          finalizeCmd_ = null;
        }
        return finalizeCmdBuilder_;
      }

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto keyUpdateCmd_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProtoOrBuilder> keyUpdateCmdBuilder_;
      /**
       * <code>optional .org.apache.hadoop.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;</code>
       * @return Whether the keyUpdateCmd field is set.
       */
      public boolean hasKeyUpdateCmd() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;</code>
       * @return The keyUpdateCmd.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto getKeyUpdateCmd() {
        if (keyUpdateCmdBuilder_ == null) {
          return keyUpdateCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.getDefaultInstance() : keyUpdateCmd_;
        } else {
          return keyUpdateCmdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;</code>
       */
      public Builder setKeyUpdateCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto value) {
        if (keyUpdateCmdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          keyUpdateCmd_ = value;
          onChanged();
        } else {
          keyUpdateCmdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000020;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;</code>
       */
      public Builder setKeyUpdateCmd(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.Builder builderForValue) {
        if (keyUpdateCmdBuilder_ == null) {
          keyUpdateCmd_ = builderForValue.build();
          onChanged();
        } else {
          keyUpdateCmdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000020;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;</code>
       */
      public Builder mergeKeyUpdateCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto value) {
        if (keyUpdateCmdBuilder_ == null) {
          if (((bitField0_ & 0x00000020) != 0) &&
              keyUpdateCmd_ != null &&
              keyUpdateCmd_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.getDefaultInstance()) {
            keyUpdateCmd_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.newBuilder(keyUpdateCmd_).mergeFrom(value).buildPartial();
          } else {
            keyUpdateCmd_ = value;
          }
          onChanged();
        } else {
          keyUpdateCmdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000020;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;</code>
       */
      public Builder clearKeyUpdateCmd() {
        if (keyUpdateCmdBuilder_ == null) {
          keyUpdateCmd_ = null;
          onChanged();
        } else {
          keyUpdateCmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000020);
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.Builder getKeyUpdateCmdBuilder() {
        bitField0_ |= 0x00000020;
        onChanged();
        return getKeyUpdateCmdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProtoOrBuilder getKeyUpdateCmdOrBuilder() {
        if (keyUpdateCmdBuilder_ != null) {
          return keyUpdateCmdBuilder_.getMessageOrBuilder();
        } else {
          return keyUpdateCmd_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.getDefaultInstance() : keyUpdateCmd_;
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProtoOrBuilder>
          getKeyUpdateCmdFieldBuilder() {
        if (keyUpdateCmdBuilder_ == null) {
          keyUpdateCmdBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProtoOrBuilder>(
                  getKeyUpdateCmd(),
                  getParentForChildren(),
                  isClean());
          keyUpdateCmd_ = null;
        }
        return keyUpdateCmdBuilder_;
      }

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto registerCmd_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProtoOrBuilder> registerCmdBuilder_;
      /**
       * <code>optional .org.apache.hadoop.datanode.RegisterCommandProto registerCmd = 7;</code>
       * @return Whether the registerCmd field is set.
       */
      public boolean hasRegisterCmd() {
        return ((bitField0_ & 0x00000040) != 0);
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.RegisterCommandProto registerCmd = 7;</code>
       * @return The registerCmd.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto getRegisterCmd() {
        if (registerCmdBuilder_ == null) {
          return registerCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.getDefaultInstance() : registerCmd_;
        } else {
          return registerCmdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.RegisterCommandProto registerCmd = 7;</code>
       */
      public Builder setRegisterCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto value) {
        if (registerCmdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          registerCmd_ = value;
          onChanged();
        } else {
          registerCmdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000040;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.RegisterCommandProto registerCmd = 7;</code>
       */
      public Builder setRegisterCmd(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.Builder builderForValue) {
        if (registerCmdBuilder_ == null) {
          registerCmd_ = builderForValue.build();
          onChanged();
        } else {
          registerCmdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000040;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.RegisterCommandProto registerCmd = 7;</code>
       */
      public Builder mergeRegisterCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto value) {
        if (registerCmdBuilder_ == null) {
          if (((bitField0_ & 0x00000040) != 0) &&
              registerCmd_ != null &&
              registerCmd_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.getDefaultInstance()) {
            registerCmd_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.newBuilder(registerCmd_).mergeFrom(value).buildPartial();
          } else {
            registerCmd_ = value;
          }
          onChanged();
        } else {
          registerCmdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000040;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.RegisterCommandProto registerCmd = 7;</code>
       */
      public Builder clearRegisterCmd() {
        if (registerCmdBuilder_ == null) {
          registerCmd_ = null;
          onChanged();
        } else {
          registerCmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000040);
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.RegisterCommandProto registerCmd = 7;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.Builder getRegisterCmdBuilder() {
        bitField0_ |= 0x00000040;
        onChanged();
        return getRegisterCmdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.RegisterCommandProto registerCmd = 7;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProtoOrBuilder getRegisterCmdOrBuilder() {
        if (registerCmdBuilder_ != null) {
          return registerCmdBuilder_.getMessageOrBuilder();
        } else {
          return registerCmd_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.getDefaultInstance() : registerCmd_;
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.RegisterCommandProto registerCmd = 7;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProtoOrBuilder>
          getRegisterCmdFieldBuilder() {
        if (registerCmdBuilder_ == null) {
          registerCmdBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProtoOrBuilder>(
                  getRegisterCmd(),
                  getParentForChildren(),
                  isClean());
          registerCmd_ = null;
        }
        return registerCmdBuilder_;
      }

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto blkIdCmd_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProtoOrBuilder> blkIdCmdBuilder_;
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockIdCommandProto blkIdCmd = 8;</code>
       * @return Whether the blkIdCmd field is set.
       */
      public boolean hasBlkIdCmd() {
        return ((bitField0_ & 0x00000080) != 0);
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockIdCommandProto blkIdCmd = 8;</code>
       * @return The blkIdCmd.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto getBlkIdCmd() {
        if (blkIdCmdBuilder_ == null) {
          return blkIdCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.getDefaultInstance() : blkIdCmd_;
        } else {
          return blkIdCmdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockIdCommandProto blkIdCmd = 8;</code>
       */
      public Builder setBlkIdCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto value) {
        if (blkIdCmdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          blkIdCmd_ = value;
          onChanged();
        } else {
          blkIdCmdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockIdCommandProto blkIdCmd = 8;</code>
       */
      public Builder setBlkIdCmd(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Builder builderForValue) {
        if (blkIdCmdBuilder_ == null) {
          blkIdCmd_ = builderForValue.build();
          onChanged();
        } else {
          blkIdCmdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockIdCommandProto blkIdCmd = 8;</code>
       */
      public Builder mergeBlkIdCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto value) {
        if (blkIdCmdBuilder_ == null) {
          if (((bitField0_ & 0x00000080) != 0) &&
              blkIdCmd_ != null &&
              blkIdCmd_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.getDefaultInstance()) {
            blkIdCmd_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.newBuilder(blkIdCmd_).mergeFrom(value).buildPartial();
          } else {
            blkIdCmd_ = value;
          }
          onChanged();
        } else {
          blkIdCmdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockIdCommandProto blkIdCmd = 8;</code>
       */
      public Builder clearBlkIdCmd() {
        if (blkIdCmdBuilder_ == null) {
          blkIdCmd_ = null;
          onChanged();
        } else {
          blkIdCmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockIdCommandProto blkIdCmd = 8;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Builder getBlkIdCmdBuilder() {
        bitField0_ |= 0x00000080;
        onChanged();
        return getBlkIdCmdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockIdCommandProto blkIdCmd = 8;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProtoOrBuilder getBlkIdCmdOrBuilder() {
        if (blkIdCmdBuilder_ != null) {
          return blkIdCmdBuilder_.getMessageOrBuilder();
        } else {
          return blkIdCmd_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.getDefaultInstance() : blkIdCmd_;
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockIdCommandProto blkIdCmd = 8;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProtoOrBuilder>
          getBlkIdCmdFieldBuilder() {
        if (blkIdCmdBuilder_ == null) {
          blkIdCmdBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProtoOrBuilder>(
                  getBlkIdCmd(),
                  getParentForChildren(),
                  isClean());
          blkIdCmd_ = null;
        }
        return blkIdCmdBuilder_;
      }

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto mismatchHashesCmd_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProtoOrBuilder> mismatchHashesCmdBuilder_;
      /**
       * <code>optional .org.apache.hadoop.datanode.HashMismatchCommandProto mismatchHashesCmd = 11;</code>
       * @return Whether the mismatchHashesCmd field is set.
       */
      public boolean hasMismatchHashesCmd() {
        return ((bitField0_ & 0x00000100) != 0);
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.HashMismatchCommandProto mismatchHashesCmd = 11;</code>
       * @return The mismatchHashesCmd.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto getMismatchHashesCmd() {
        if (mismatchHashesCmdBuilder_ == null) {
          return mismatchHashesCmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.getDefaultInstance() : mismatchHashesCmd_;
        } else {
          return mismatchHashesCmdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.HashMismatchCommandProto mismatchHashesCmd = 11;</code>
       */
      public Builder setMismatchHashesCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto value) {
        if (mismatchHashesCmdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          mismatchHashesCmd_ = value;
          onChanged();
        } else {
          mismatchHashesCmdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000100;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.HashMismatchCommandProto mismatchHashesCmd = 11;</code>
       */
      public Builder setMismatchHashesCmd(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.Builder builderForValue) {
        if (mismatchHashesCmdBuilder_ == null) {
          mismatchHashesCmd_ = builderForValue.build();
          onChanged();
        } else {
          mismatchHashesCmdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000100;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.HashMismatchCommandProto mismatchHashesCmd = 11;</code>
       */
      public Builder mergeMismatchHashesCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto value) {
        if (mismatchHashesCmdBuilder_ == null) {
          if (((bitField0_ & 0x00000100) != 0) &&
              mismatchHashesCmd_ != null &&
              mismatchHashesCmd_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.getDefaultInstance()) {
            mismatchHashesCmd_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.newBuilder(mismatchHashesCmd_).mergeFrom(value).buildPartial();
          } else {
            mismatchHashesCmd_ = value;
          }
          onChanged();
        } else {
          mismatchHashesCmdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000100;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.HashMismatchCommandProto mismatchHashesCmd = 11;</code>
       */
      public Builder clearMismatchHashesCmd() {
        if (mismatchHashesCmdBuilder_ == null) {
          mismatchHashesCmd_ = null;
          onChanged();
        } else {
          mismatchHashesCmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000100);
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.HashMismatchCommandProto mismatchHashesCmd = 11;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.Builder getMismatchHashesCmdBuilder() {
        bitField0_ |= 0x00000100;
        onChanged();
        return getMismatchHashesCmdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.HashMismatchCommandProto mismatchHashesCmd = 11;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProtoOrBuilder getMismatchHashesCmdOrBuilder() {
        if (mismatchHashesCmdBuilder_ != null) {
          return mismatchHashesCmdBuilder_.getMessageOrBuilder();
        } else {
          return mismatchHashesCmd_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.getDefaultInstance() : mismatchHashesCmd_;
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.HashMismatchCommandProto mismatchHashesCmd = 11;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProtoOrBuilder>
          getMismatchHashesCmdFieldBuilder() {
        if (mismatchHashesCmdBuilder_ == null) {
          mismatchHashesCmdBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProtoOrBuilder>(
                  getMismatchHashesCmd(),
                  getParentForChildren(),
                  isClean());
          mismatchHashesCmd_ = null;
        }
        return mismatchHashesCmdBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.DatanodeCommandProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.DatanodeCommandProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<DatanodeCommandProto>
        PARSER = new com.google.protobuf.AbstractParser<DatanodeCommandProto>() {
      @java.lang.Override
      public DatanodeCommandProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new DatanodeCommandProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<DatanodeCommandProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<DatanodeCommandProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface BalancerBandwidthCommandProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.BalancerBandwidthCommandProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * Maximum bandwidth to be used by datanode for balancing
     * </pre>
     *
     * <code>required uint64 bandwidth = 1;</code>
     * @return Whether the bandwidth field is set.
     */
    boolean hasBandwidth();
    /**
     * <pre>
     * Maximum bandwidth to be used by datanode for balancing
     * </pre>
     *
     * <code>required uint64 bandwidth = 1;</code>
     * @return The bandwidth.
     */
    long getBandwidth();
  }
  /**
   * <pre>
   **
   * Command sent from namenode to datanode to set the
   * maximum bandwidth to be used for balancing.
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.BalancerBandwidthCommandProto}
   */
  public static final class BalancerBandwidthCommandProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.BalancerBandwidthCommandProto)
      BalancerBandwidthCommandProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use BalancerBandwidthCommandProto.newBuilder() to construct.
    private BalancerBandwidthCommandProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private BalancerBandwidthCommandProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new BalancerBandwidthCommandProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private BalancerBandwidthCommandProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              bitField0_ |= 0x00000001;
              bandwidth_ = input.readUInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BalancerBandwidthCommandProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BalancerBandwidthCommandProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.Builder.class);
    }

    private int bitField0_;
    public static final int BANDWIDTH_FIELD_NUMBER = 1;
    private long bandwidth_;
    /**
     * <pre>
     * Maximum bandwidth to be used by datanode for balancing
     * </pre>
     *
     * <code>required uint64 bandwidth = 1;</code>
     * @return Whether the bandwidth field is set.
     */
    @java.lang.Override
    public boolean hasBandwidth() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <pre>
     * Maximum bandwidth to be used by datanode for balancing
     * </pre>
     *
     * <code>required uint64 bandwidth = 1;</code>
     * @return The bandwidth.
     */
    @java.lang.Override
    public long getBandwidth() {
      return bandwidth_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasBandwidth()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeUInt64(1, bandwidth_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(1, bandwidth_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto) obj;

      if (hasBandwidth() != other.hasBandwidth()) return false;
      if (hasBandwidth()) {
        if (getBandwidth()
            != other.getBandwidth()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasBandwidth()) {
        hash = (37 * hash) + BANDWIDTH_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getBandwidth());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * Command sent from namenode to datanode to set the
     * maximum bandwidth to be used for balancing.
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.BalancerBandwidthCommandProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.BalancerBandwidthCommandProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BalancerBandwidthCommandProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BalancerBandwidthCommandProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        bandwidth_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BalancerBandwidthCommandProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.bandwidth_ = bandwidth_;
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto.getDefaultInstance()) return this;
        if (other.hasBandwidth()) {
          setBandwidth(other.getBandwidth());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasBandwidth()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long bandwidth_ ;
      /**
       * <pre>
       * Maximum bandwidth to be used by datanode for balancing
       * </pre>
       *
       * <code>required uint64 bandwidth = 1;</code>
       * @return Whether the bandwidth field is set.
       */
      @java.lang.Override
      public boolean hasBandwidth() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * Maximum bandwidth to be used by datanode for balancing
       * </pre>
       *
       * <code>required uint64 bandwidth = 1;</code>
       * @return The bandwidth.
       */
      @java.lang.Override
      public long getBandwidth() {
        return bandwidth_;
      }
      /**
       * <pre>
       * Maximum bandwidth to be used by datanode for balancing
       * </pre>
       *
       * <code>required uint64 bandwidth = 1;</code>
       * @param value The bandwidth to set.
       * @return This builder for chaining.
       */
      public Builder setBandwidth(long value) {
        bitField0_ |= 0x00000001;
        bandwidth_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Maximum bandwidth to be used by datanode for balancing
       * </pre>
       *
       * <code>required uint64 bandwidth = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearBandwidth() {
        bitField0_ = (bitField0_ & ~0x00000001);
        bandwidth_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.BalancerBandwidthCommandProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.BalancerBandwidthCommandProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<BalancerBandwidthCommandProto>
        PARSER = new com.google.protobuf.AbstractParser<BalancerBandwidthCommandProto>() {
      @java.lang.Override
      public BalancerBandwidthCommandProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new BalancerBandwidthCommandProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<BalancerBandwidthCommandProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<BalancerBandwidthCommandProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BalancerBandwidthCommandProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface BlockCommandProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.BlockCommandProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required .org.apache.hadoop.datanode.BlockCommandProto.Action action = 1;</code>
     * @return Whether the action field is set.
     */
    boolean hasAction();
    /**
     * <code>required .org.apache.hadoop.datanode.BlockCommandProto.Action action = 1;</code>
     * @return The action.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Action getAction();

    /**
     * <code>required string blockPoolId = 2;</code>
     * @return Whether the blockPoolId field is set.
     */
    boolean hasBlockPoolId();
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The blockPoolId.
     */
    java.lang.String getBlockPoolId();
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The bytes for blockPoolId.
     */
    com.google.protobuf.ByteString
        getBlockPoolIdBytes();

    /**
     * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto>
        getBlocksList();
    /**
     * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto getBlocks(int index);
    /**
     * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
     */
    int getBlocksCount();
    /**
     * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProtoOrBuilder>
        getBlocksOrBuilderList();
    /**
     * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProtoOrBuilder getBlocksOrBuilder(
        int index);

    /**
     * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto>
        getTargetsList();
    /**
     * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto getTargets(int index);
    /**
     * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
     */
    int getTargetsCount();
    /**
     * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProtoOrBuilder>
        getTargetsOrBuilderList();
    /**
     * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProtoOrBuilder getTargetsOrBuilder(
        int index);

    /**
     * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto>
        getTargetStorageUuidsList();
    /**
     * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto getTargetStorageUuids(int index);
    /**
     * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
     */
    int getTargetStorageUuidsCount();
    /**
     * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProtoOrBuilder>
        getTargetStorageUuidsOrBuilderList();
    /**
     * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProtoOrBuilder getTargetStorageUuidsOrBuilder(
        int index);

    /**
     * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto>
        getTargetStorageTypesList();
    /**
     * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto getTargetStorageTypes(int index);
    /**
     * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
     */
    int getTargetStorageTypesCount();
    /**
     * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProtoOrBuilder>
        getTargetStorageTypesOrBuilderList();
    /**
     * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProtoOrBuilder getTargetStorageTypesOrBuilder(
        int index);
  }
  /**
   * <pre>
   **
   * Command to instruct datanodes to perform certain action
   * on the given set of blocks.
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.BlockCommandProto}
   */
  public static final class BlockCommandProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.BlockCommandProto)
      BlockCommandProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use BlockCommandProto.newBuilder() to construct.
    private BlockCommandProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private BlockCommandProto() {
      action_ = 1;
      blockPoolId_ = "";
      blocks_ = java.util.Collections.emptyList();
      targets_ = java.util.Collections.emptyList();
      targetStorageUuids_ = java.util.Collections.emptyList();
      targetStorageTypes_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new BlockCommandProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private BlockCommandProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Action value = org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Action.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(1, rawValue);
              } else {
                bitField0_ |= 0x00000001;
                action_ = rawValue;
              }
              break;
            }
            case 18: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000002;
              blockPoolId_ = bs;
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                blocks_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto>();
                mutable_bitField0_ |= 0x00000004;
              }
              blocks_.add(
                  input.readMessage(org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.PARSER, extensionRegistry));
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000008) != 0)) {
                targets_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto>();
                mutable_bitField0_ |= 0x00000008;
              }
              targets_.add(
                  input.readMessage(org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto.PARSER, extensionRegistry));
              break;
            }
            case 42: {
              if (!((mutable_bitField0_ & 0x00000010) != 0)) {
                targetStorageUuids_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto>();
                mutable_bitField0_ |= 0x00000010;
              }
              targetStorageUuids_.add(
                  input.readMessage(org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto.PARSER, extensionRegistry));
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000020) != 0)) {
                targetStorageTypes_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto>();
                mutable_bitField0_ |= 0x00000020;
              }
              targetStorageTypes_.add(
                  input.readMessage(org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto.PARSER, extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          blocks_ = java.util.Collections.unmodifiableList(blocks_);
        }
        if (((mutable_bitField0_ & 0x00000008) != 0)) {
          targets_ = java.util.Collections.unmodifiableList(targets_);
        }
        if (((mutable_bitField0_ & 0x00000010) != 0)) {
          targetStorageUuids_ = java.util.Collections.unmodifiableList(targetStorageUuids_);
        }
        if (((mutable_bitField0_ & 0x00000020) != 0)) {
          targetStorageTypes_ = java.util.Collections.unmodifiableList(targetStorageTypes_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockCommandProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockCommandProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Builder.class);
    }

    /**
     * Protobuf enum {@code org.apache.hadoop.datanode.BlockCommandProto.Action}
     */
    public enum Action
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <pre>
       * Transfer blocks to another datanode
       * </pre>
       *
       * <code>TRANSFER = 1;</code>
       */
      TRANSFER(1),
      /**
       * <pre>
       * Invalidate blocks
       * </pre>
       *
       * <code>INVALIDATE = 2;</code>
       */
      INVALIDATE(2),
      /**
       * <pre>
       * Shutdown the datanode
       * </pre>
       *
       * <code>SHUTDOWN = 3;</code>
       */
      SHUTDOWN(3),
      ;

      /**
       * <pre>
       * Transfer blocks to another datanode
       * </pre>
       *
       * <code>TRANSFER = 1;</code>
       */
      public static final int TRANSFER_VALUE = 1;
      /**
       * <pre>
       * Invalidate blocks
       * </pre>
       *
       * <code>INVALIDATE = 2;</code>
       */
      public static final int INVALIDATE_VALUE = 2;
      /**
       * <pre>
       * Shutdown the datanode
       * </pre>
       *
       * <code>SHUTDOWN = 3;</code>
       */
      public static final int SHUTDOWN_VALUE = 3;


      public final int getNumber() {
        return value;
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static Action valueOf(int value) {
        return forNumber(value);
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       */
      public static Action forNumber(int value) {
        switch (value) {
          case 1: return TRANSFER;
          case 2: return INVALIDATE;
          case 3: return SHUTDOWN;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<Action>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          Action> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<Action>() {
              public Action findValueByNumber(int number) {
                return Action.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.getDescriptor().getEnumTypes().get(0);
      }

      private static final Action[] VALUES = values();

      public static Action valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private Action(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:org.apache.hadoop.datanode.BlockCommandProto.Action)
    }

    private int bitField0_;
    public static final int ACTION_FIELD_NUMBER = 1;
    private int action_;
    /**
     * <code>required .org.apache.hadoop.datanode.BlockCommandProto.Action action = 1;</code>
     * @return Whether the action field is set.
     */
    @java.lang.Override public boolean hasAction() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required .org.apache.hadoop.datanode.BlockCommandProto.Action action = 1;</code>
     * @return The action.
     */
    @java.lang.Override public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Action getAction() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Action result = org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Action.valueOf(action_);
      return result == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Action.TRANSFER : result;
    }

    public static final int BLOCKPOOLID_FIELD_NUMBER = 2;
    private volatile java.lang.Object blockPoolId_;
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return Whether the blockPoolId field is set.
     */
    @java.lang.Override
    public boolean hasBlockPoolId() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The blockPoolId.
     */
    @java.lang.Override
    public java.lang.String getBlockPoolId() {
      java.lang.Object ref = blockPoolId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          blockPoolId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The bytes for blockPoolId.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getBlockPoolIdBytes() {
      java.lang.Object ref = blockPoolId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        blockPoolId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int BLOCKS_FIELD_NUMBER = 3;
    private java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto> blocks_;
    /**
     * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto> getBlocksList() {
      return blocks_;
    }
    /**
     * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProtoOrBuilder>
        getBlocksOrBuilderList() {
      return blocks_;
    }
    /**
     * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
     */
    @java.lang.Override
    public int getBlocksCount() {
      return blocks_.size();
    }
    /**
     * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto getBlocks(int index) {
      return blocks_.get(index);
    }
    /**
     * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProtoOrBuilder getBlocksOrBuilder(
        int index) {
      return blocks_.get(index);
    }

    public static final int TARGETS_FIELD_NUMBER = 4;
    private java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto> targets_;
    /**
     * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto> getTargetsList() {
      return targets_;
    }
    /**
     * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProtoOrBuilder>
        getTargetsOrBuilderList() {
      return targets_;
    }
    /**
     * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
     */
    @java.lang.Override
    public int getTargetsCount() {
      return targets_.size();
    }
    /**
     * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto getTargets(int index) {
      return targets_.get(index);
    }
    /**
     * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProtoOrBuilder getTargetsOrBuilder(
        int index) {
      return targets_.get(index);
    }

    public static final int TARGETSTORAGEUUIDS_FIELD_NUMBER = 5;
    private java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto> targetStorageUuids_;
    /**
     * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto> getTargetStorageUuidsList() {
      return targetStorageUuids_;
    }
    /**
     * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProtoOrBuilder>
        getTargetStorageUuidsOrBuilderList() {
      return targetStorageUuids_;
    }
    /**
     * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
     */
    @java.lang.Override
    public int getTargetStorageUuidsCount() {
      return targetStorageUuids_.size();
    }
    /**
     * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto getTargetStorageUuids(int index) {
      return targetStorageUuids_.get(index);
    }
    /**
     * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProtoOrBuilder getTargetStorageUuidsOrBuilder(
        int index) {
      return targetStorageUuids_.get(index);
    }

    public static final int TARGETSTORAGETYPES_FIELD_NUMBER = 6;
    private java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto> targetStorageTypes_;
    /**
     * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto> getTargetStorageTypesList() {
      return targetStorageTypes_;
    }
    /**
     * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProtoOrBuilder>
        getTargetStorageTypesOrBuilderList() {
      return targetStorageTypes_;
    }
    /**
     * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
     */
    @java.lang.Override
    public int getTargetStorageTypesCount() {
      return targetStorageTypes_.size();
    }
    /**
     * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto getTargetStorageTypes(int index) {
      return targetStorageTypes_.get(index);
    }
    /**
     * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProtoOrBuilder getTargetStorageTypesOrBuilder(
        int index) {
      return targetStorageTypes_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasAction()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasBlockPoolId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      for (int i = 0; i < getBlocksCount(); i++) {
        if (!getBlocks(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getTargetsCount(); i++) {
        if (!getTargets(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeEnum(1, action_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, blockPoolId_);
      }
      for (int i = 0; i < blocks_.size(); i++) {
        output.writeMessage(3, blocks_.get(i));
      }
      for (int i = 0; i < targets_.size(); i++) {
        output.writeMessage(4, targets_.get(i));
      }
      for (int i = 0; i < targetStorageUuids_.size(); i++) {
        output.writeMessage(5, targetStorageUuids_.get(i));
      }
      for (int i = 0; i < targetStorageTypes_.size(); i++) {
        output.writeMessage(6, targetStorageTypes_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, action_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, blockPoolId_);
      }
      for (int i = 0; i < blocks_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, blocks_.get(i));
      }
      for (int i = 0; i < targets_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, targets_.get(i));
      }
      for (int i = 0; i < targetStorageUuids_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, targetStorageUuids_.get(i));
      }
      for (int i = 0; i < targetStorageTypes_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, targetStorageTypes_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto) obj;

      if (hasAction() != other.hasAction()) return false;
      if (hasAction()) {
        if (action_ != other.action_) return false;
      }
      if (hasBlockPoolId() != other.hasBlockPoolId()) return false;
      if (hasBlockPoolId()) {
        if (!getBlockPoolId()
            .equals(other.getBlockPoolId())) return false;
      }
      if (!getBlocksList()
          .equals(other.getBlocksList())) return false;
      if (!getTargetsList()
          .equals(other.getTargetsList())) return false;
      if (!getTargetStorageUuidsList()
          .equals(other.getTargetStorageUuidsList())) return false;
      if (!getTargetStorageTypesList()
          .equals(other.getTargetStorageTypesList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasAction()) {
        hash = (37 * hash) + ACTION_FIELD_NUMBER;
        hash = (53 * hash) + action_;
      }
      if (hasBlockPoolId()) {
        hash = (37 * hash) + BLOCKPOOLID_FIELD_NUMBER;
        hash = (53 * hash) + getBlockPoolId().hashCode();
      }
      if (getBlocksCount() > 0) {
        hash = (37 * hash) + BLOCKS_FIELD_NUMBER;
        hash = (53 * hash) + getBlocksList().hashCode();
      }
      if (getTargetsCount() > 0) {
        hash = (37 * hash) + TARGETS_FIELD_NUMBER;
        hash = (53 * hash) + getTargetsList().hashCode();
      }
      if (getTargetStorageUuidsCount() > 0) {
        hash = (37 * hash) + TARGETSTORAGEUUIDS_FIELD_NUMBER;
        hash = (53 * hash) + getTargetStorageUuidsList().hashCode();
      }
      if (getTargetStorageTypesCount() > 0) {
        hash = (37 * hash) + TARGETSTORAGETYPES_FIELD_NUMBER;
        hash = (53 * hash) + getTargetStorageTypesList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * Command to instruct datanodes to perform certain action
     * on the given set of blocks.
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.BlockCommandProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.BlockCommandProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockCommandProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockCommandProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getBlocksFieldBuilder();
          getTargetsFieldBuilder();
          getTargetStorageUuidsFieldBuilder();
          getTargetStorageTypesFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        action_ = 1;
        bitField0_ = (bitField0_ & ~0x00000001);
        blockPoolId_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        if (blocksBuilder_ == null) {
          blocks_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          blocksBuilder_.clear();
        }
        if (targetsBuilder_ == null) {
          targets_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
        } else {
          targetsBuilder_.clear();
        }
        if (targetStorageUuidsBuilder_ == null) {
          targetStorageUuids_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
        } else {
          targetStorageUuidsBuilder_.clear();
        }
        if (targetStorageTypesBuilder_ == null) {
          targetStorageTypes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
        } else {
          targetStorageTypesBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockCommandProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.action_ = action_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.blockPoolId_ = blockPoolId_;
        if (blocksBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0)) {
            blocks_ = java.util.Collections.unmodifiableList(blocks_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.blocks_ = blocks_;
        } else {
          result.blocks_ = blocksBuilder_.build();
        }
        if (targetsBuilder_ == null) {
          if (((bitField0_ & 0x00000008) != 0)) {
            targets_ = java.util.Collections.unmodifiableList(targets_);
            bitField0_ = (bitField0_ & ~0x00000008);
          }
          result.targets_ = targets_;
        } else {
          result.targets_ = targetsBuilder_.build();
        }
        if (targetStorageUuidsBuilder_ == null) {
          if (((bitField0_ & 0x00000010) != 0)) {
            targetStorageUuids_ = java.util.Collections.unmodifiableList(targetStorageUuids_);
            bitField0_ = (bitField0_ & ~0x00000010);
          }
          result.targetStorageUuids_ = targetStorageUuids_;
        } else {
          result.targetStorageUuids_ = targetStorageUuidsBuilder_.build();
        }
        if (targetStorageTypesBuilder_ == null) {
          if (((bitField0_ & 0x00000020) != 0)) {
            targetStorageTypes_ = java.util.Collections.unmodifiableList(targetStorageTypes_);
            bitField0_ = (bitField0_ & ~0x00000020);
          }
          result.targetStorageTypes_ = targetStorageTypes_;
        } else {
          result.targetStorageTypes_ = targetStorageTypesBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.getDefaultInstance()) return this;
        if (other.hasAction()) {
          setAction(other.getAction());
        }
        if (other.hasBlockPoolId()) {
          bitField0_ |= 0x00000002;
          blockPoolId_ = other.blockPoolId_;
          onChanged();
        }
        if (blocksBuilder_ == null) {
          if (!other.blocks_.isEmpty()) {
            if (blocks_.isEmpty()) {
              blocks_ = other.blocks_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureBlocksIsMutable();
              blocks_.addAll(other.blocks_);
            }
            onChanged();
          }
        } else {
          if (!other.blocks_.isEmpty()) {
            if (blocksBuilder_.isEmpty()) {
              blocksBuilder_.dispose();
              blocksBuilder_ = null;
              blocks_ = other.blocks_;
              bitField0_ = (bitField0_ & ~0x00000004);
              blocksBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getBlocksFieldBuilder() : null;
            } else {
              blocksBuilder_.addAllMessages(other.blocks_);
            }
          }
        }
        if (targetsBuilder_ == null) {
          if (!other.targets_.isEmpty()) {
            if (targets_.isEmpty()) {
              targets_ = other.targets_;
              bitField0_ = (bitField0_ & ~0x00000008);
            } else {
              ensureTargetsIsMutable();
              targets_.addAll(other.targets_);
            }
            onChanged();
          }
        } else {
          if (!other.targets_.isEmpty()) {
            if (targetsBuilder_.isEmpty()) {
              targetsBuilder_.dispose();
              targetsBuilder_ = null;
              targets_ = other.targets_;
              bitField0_ = (bitField0_ & ~0x00000008);
              targetsBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getTargetsFieldBuilder() : null;
            } else {
              targetsBuilder_.addAllMessages(other.targets_);
            }
          }
        }
        if (targetStorageUuidsBuilder_ == null) {
          if (!other.targetStorageUuids_.isEmpty()) {
            if (targetStorageUuids_.isEmpty()) {
              targetStorageUuids_ = other.targetStorageUuids_;
              bitField0_ = (bitField0_ & ~0x00000010);
            } else {
              ensureTargetStorageUuidsIsMutable();
              targetStorageUuids_.addAll(other.targetStorageUuids_);
            }
            onChanged();
          }
        } else {
          if (!other.targetStorageUuids_.isEmpty()) {
            if (targetStorageUuidsBuilder_.isEmpty()) {
              targetStorageUuidsBuilder_.dispose();
              targetStorageUuidsBuilder_ = null;
              targetStorageUuids_ = other.targetStorageUuids_;
              bitField0_ = (bitField0_ & ~0x00000010);
              targetStorageUuidsBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getTargetStorageUuidsFieldBuilder() : null;
            } else {
              targetStorageUuidsBuilder_.addAllMessages(other.targetStorageUuids_);
            }
          }
        }
        if (targetStorageTypesBuilder_ == null) {
          if (!other.targetStorageTypes_.isEmpty()) {
            if (targetStorageTypes_.isEmpty()) {
              targetStorageTypes_ = other.targetStorageTypes_;
              bitField0_ = (bitField0_ & ~0x00000020);
            } else {
              ensureTargetStorageTypesIsMutable();
              targetStorageTypes_.addAll(other.targetStorageTypes_);
            }
            onChanged();
          }
        } else {
          if (!other.targetStorageTypes_.isEmpty()) {
            if (targetStorageTypesBuilder_.isEmpty()) {
              targetStorageTypesBuilder_.dispose();
              targetStorageTypesBuilder_ = null;
              targetStorageTypes_ = other.targetStorageTypes_;
              bitField0_ = (bitField0_ & ~0x00000020);
              targetStorageTypesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getTargetStorageTypesFieldBuilder() : null;
            } else {
              targetStorageTypesBuilder_.addAllMessages(other.targetStorageTypes_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasAction()) {
          return false;
        }
        if (!hasBlockPoolId()) {
          return false;
        }
        for (int i = 0; i < getBlocksCount(); i++) {
          if (!getBlocks(i).isInitialized()) {
            return false;
          }
        }
        for (int i = 0; i < getTargetsCount(); i++) {
          if (!getTargets(i).isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int action_ = 1;
      /**
       * <code>required .org.apache.hadoop.datanode.BlockCommandProto.Action action = 1;</code>
       * @return Whether the action field is set.
       */
      @java.lang.Override public boolean hasAction() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required .org.apache.hadoop.datanode.BlockCommandProto.Action action = 1;</code>
       * @return The action.
       */
      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Action getAction() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Action result = org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Action.valueOf(action_);
        return result == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Action.TRANSFER : result;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.BlockCommandProto.Action action = 1;</code>
       * @param value The action to set.
       * @return This builder for chaining.
       */
      public Builder setAction(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto.Action value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000001;
        action_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.BlockCommandProto.Action action = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearAction() {
        bitField0_ = (bitField0_ & ~0x00000001);
        action_ = 1;
        onChanged();
        return this;
      }

      private java.lang.Object blockPoolId_ = "";
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return Whether the blockPoolId field is set.
       */
      public boolean hasBlockPoolId() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return The blockPoolId.
       */
      public java.lang.String getBlockPoolId() {
        java.lang.Object ref = blockPoolId_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            blockPoolId_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return The bytes for blockPoolId.
       */
      public com.google.protobuf.ByteString
          getBlockPoolIdBytes() {
        java.lang.Object ref = blockPoolId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          blockPoolId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @param value The blockPoolId to set.
       * @return This builder for chaining.
       */
      public Builder setBlockPoolId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        blockPoolId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearBlockPoolId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        blockPoolId_ = getDefaultInstance().getBlockPoolId();
        onChanged();
        return this;
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @param value The bytes for blockPoolId to set.
       * @return This builder for chaining.
       */
      public Builder setBlockPoolIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        blockPoolId_ = value;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto> blocks_ =
        java.util.Collections.emptyList();
      private void ensureBlocksIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          blocks_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto>(blocks_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProtoOrBuilder> blocksBuilder_;

      /**
       * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto> getBlocksList() {
        if (blocksBuilder_ == null) {
          return java.util.Collections.unmodifiableList(blocks_);
        } else {
          return blocksBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
       */
      public int getBlocksCount() {
        if (blocksBuilder_ == null) {
          return blocks_.size();
        } else {
          return blocksBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto getBlocks(int index) {
        if (blocksBuilder_ == null) {
          return blocks_.get(index);
        } else {
          return blocksBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
       */
      public Builder setBlocks(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto value) {
        if (blocksBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBlocksIsMutable();
          blocks_.set(index, value);
          onChanged();
        } else {
          blocksBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
       */
      public Builder setBlocks(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.Builder builderForValue) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.set(index, builderForValue.build());
          onChanged();
        } else {
          blocksBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
       */
      public Builder addBlocks(org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto value) {
        if (blocksBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBlocksIsMutable();
          blocks_.add(value);
          onChanged();
        } else {
          blocksBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
       */
      public Builder addBlocks(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto value) {
        if (blocksBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBlocksIsMutable();
          blocks_.add(index, value);
          onChanged();
        } else {
          blocksBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
       */
      public Builder addBlocks(
          org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.Builder builderForValue) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.add(builderForValue.build());
          onChanged();
        } else {
          blocksBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
       */
      public Builder addBlocks(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.Builder builderForValue) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.add(index, builderForValue.build());
          onChanged();
        } else {
          blocksBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
       */
      public Builder addAllBlocks(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto> values) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, blocks_);
          onChanged();
        } else {
          blocksBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
       */
      public Builder clearBlocks() {
        if (blocksBuilder_ == null) {
          blocks_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          blocksBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
       */
      public Builder removeBlocks(int index) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.remove(index);
          onChanged();
        } else {
          blocksBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.Builder getBlocksBuilder(
          int index) {
        return getBlocksFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProtoOrBuilder getBlocksOrBuilder(
          int index) {
        if (blocksBuilder_ == null) {
          return blocks_.get(index);  } else {
          return blocksBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProtoOrBuilder>
           getBlocksOrBuilderList() {
        if (blocksBuilder_ != null) {
          return blocksBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(blocks_);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.Builder addBlocksBuilder() {
        return getBlocksFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.Builder addBlocksBuilder(
          int index) {
        return getBlocksFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.BlockProto blocks = 3;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.Builder>
           getBlocksBuilderList() {
        return getBlocksFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProtoOrBuilder>
          getBlocksFieldBuilder() {
        if (blocksBuilder_ == null) {
          blocksBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProtoOrBuilder>(
                  blocks_,
                  ((bitField0_ & 0x00000004) != 0),
                  getParentForChildren(),
                  isClean());
          blocks_ = null;
        }
        return blocksBuilder_;
      }

      private java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto> targets_ =
        java.util.Collections.emptyList();
      private void ensureTargetsIsMutable() {
        if (!((bitField0_ & 0x00000008) != 0)) {
          targets_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto>(targets_);
          bitField0_ |= 0x00000008;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProtoOrBuilder> targetsBuilder_;

      /**
       * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto> getTargetsList() {
        if (targetsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(targets_);
        } else {
          return targetsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
       */
      public int getTargetsCount() {
        if (targetsBuilder_ == null) {
          return targets_.size();
        } else {
          return targetsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto getTargets(int index) {
        if (targetsBuilder_ == null) {
          return targets_.get(index);
        } else {
          return targetsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
       */
      public Builder setTargets(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto value) {
        if (targetsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureTargetsIsMutable();
          targets_.set(index, value);
          onChanged();
        } else {
          targetsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
       */
      public Builder setTargets(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto.Builder builderForValue) {
        if (targetsBuilder_ == null) {
          ensureTargetsIsMutable();
          targets_.set(index, builderForValue.build());
          onChanged();
        } else {
          targetsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
       */
      public Builder addTargets(org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto value) {
        if (targetsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureTargetsIsMutable();
          targets_.add(value);
          onChanged();
        } else {
          targetsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
       */
      public Builder addTargets(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto value) {
        if (targetsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureTargetsIsMutable();
          targets_.add(index, value);
          onChanged();
        } else {
          targetsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
       */
      public Builder addTargets(
          org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto.Builder builderForValue) {
        if (targetsBuilder_ == null) {
          ensureTargetsIsMutable();
          targets_.add(builderForValue.build());
          onChanged();
        } else {
          targetsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
       */
      public Builder addTargets(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto.Builder builderForValue) {
        if (targetsBuilder_ == null) {
          ensureTargetsIsMutable();
          targets_.add(index, builderForValue.build());
          onChanged();
        } else {
          targetsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
       */
      public Builder addAllTargets(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto> values) {
        if (targetsBuilder_ == null) {
          ensureTargetsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, targets_);
          onChanged();
        } else {
          targetsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
       */
      public Builder clearTargets() {
        if (targetsBuilder_ == null) {
          targets_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
          onChanged();
        } else {
          targetsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
       */
      public Builder removeTargets(int index) {
        if (targetsBuilder_ == null) {
          ensureTargetsIsMutable();
          targets_.remove(index);
          onChanged();
        } else {
          targetsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto.Builder getTargetsBuilder(
          int index) {
        return getTargetsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProtoOrBuilder getTargetsOrBuilder(
          int index) {
        if (targetsBuilder_ == null) {
          return targets_.get(index);  } else {
          return targetsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProtoOrBuilder>
           getTargetsOrBuilderList() {
        if (targetsBuilder_ != null) {
          return targetsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(targets_);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto.Builder addTargetsBuilder() {
        return getTargetsFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto.Builder addTargetsBuilder(
          int index) {
        return getTargetsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeInfosProto targets = 4;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto.Builder>
           getTargetsBuilderList() {
        return getTargetsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProtoOrBuilder>
          getTargetsFieldBuilder() {
        if (targetsBuilder_ == null) {
          targetsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeInfosProtoOrBuilder>(
                  targets_,
                  ((bitField0_ & 0x00000008) != 0),
                  getParentForChildren(),
                  isClean());
          targets_ = null;
        }
        return targetsBuilder_;
      }

      private java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto> targetStorageUuids_ =
        java.util.Collections.emptyList();
      private void ensureTargetStorageUuidsIsMutable() {
        if (!((bitField0_ & 0x00000010) != 0)) {
          targetStorageUuids_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto>(targetStorageUuids_);
          bitField0_ |= 0x00000010;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProtoOrBuilder> targetStorageUuidsBuilder_;

      /**
       * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto> getTargetStorageUuidsList() {
        if (targetStorageUuidsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(targetStorageUuids_);
        } else {
          return targetStorageUuidsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
       */
      public int getTargetStorageUuidsCount() {
        if (targetStorageUuidsBuilder_ == null) {
          return targetStorageUuids_.size();
        } else {
          return targetStorageUuidsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto getTargetStorageUuids(int index) {
        if (targetStorageUuidsBuilder_ == null) {
          return targetStorageUuids_.get(index);
        } else {
          return targetStorageUuidsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
       */
      public Builder setTargetStorageUuids(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto value) {
        if (targetStorageUuidsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureTargetStorageUuidsIsMutable();
          targetStorageUuids_.set(index, value);
          onChanged();
        } else {
          targetStorageUuidsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
       */
      public Builder setTargetStorageUuids(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto.Builder builderForValue) {
        if (targetStorageUuidsBuilder_ == null) {
          ensureTargetStorageUuidsIsMutable();
          targetStorageUuids_.set(index, builderForValue.build());
          onChanged();
        } else {
          targetStorageUuidsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
       */
      public Builder addTargetStorageUuids(org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto value) {
        if (targetStorageUuidsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureTargetStorageUuidsIsMutable();
          targetStorageUuids_.add(value);
          onChanged();
        } else {
          targetStorageUuidsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
       */
      public Builder addTargetStorageUuids(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto value) {
        if (targetStorageUuidsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureTargetStorageUuidsIsMutable();
          targetStorageUuids_.add(index, value);
          onChanged();
        } else {
          targetStorageUuidsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
       */
      public Builder addTargetStorageUuids(
          org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto.Builder builderForValue) {
        if (targetStorageUuidsBuilder_ == null) {
          ensureTargetStorageUuidsIsMutable();
          targetStorageUuids_.add(builderForValue.build());
          onChanged();
        } else {
          targetStorageUuidsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
       */
      public Builder addTargetStorageUuids(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto.Builder builderForValue) {
        if (targetStorageUuidsBuilder_ == null) {
          ensureTargetStorageUuidsIsMutable();
          targetStorageUuids_.add(index, builderForValue.build());
          onChanged();
        } else {
          targetStorageUuidsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
       */
      public Builder addAllTargetStorageUuids(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto> values) {
        if (targetStorageUuidsBuilder_ == null) {
          ensureTargetStorageUuidsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, targetStorageUuids_);
          onChanged();
        } else {
          targetStorageUuidsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
       */
      public Builder clearTargetStorageUuids() {
        if (targetStorageUuidsBuilder_ == null) {
          targetStorageUuids_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
          onChanged();
        } else {
          targetStorageUuidsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
       */
      public Builder removeTargetStorageUuids(int index) {
        if (targetStorageUuidsBuilder_ == null) {
          ensureTargetStorageUuidsIsMutable();
          targetStorageUuids_.remove(index);
          onChanged();
        } else {
          targetStorageUuidsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto.Builder getTargetStorageUuidsBuilder(
          int index) {
        return getTargetStorageUuidsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProtoOrBuilder getTargetStorageUuidsOrBuilder(
          int index) {
        if (targetStorageUuidsBuilder_ == null) {
          return targetStorageUuids_.get(index);  } else {
          return targetStorageUuidsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProtoOrBuilder>
           getTargetStorageUuidsOrBuilderList() {
        if (targetStorageUuidsBuilder_ != null) {
          return targetStorageUuidsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(targetStorageUuids_);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto.Builder addTargetStorageUuidsBuilder() {
        return getTargetStorageUuidsFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto.Builder addTargetStorageUuidsBuilder(
          int index) {
        return getTargetStorageUuidsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageUuidsProto targetStorageUuids = 5;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto.Builder>
           getTargetStorageUuidsBuilderList() {
        return getTargetStorageUuidsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProtoOrBuilder>
          getTargetStorageUuidsFieldBuilder() {
        if (targetStorageUuidsBuilder_ == null) {
          targetStorageUuidsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageUuidsProtoOrBuilder>(
                  targetStorageUuids_,
                  ((bitField0_ & 0x00000010) != 0),
                  getParentForChildren(),
                  isClean());
          targetStorageUuids_ = null;
        }
        return targetStorageUuidsBuilder_;
      }

      private java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto> targetStorageTypes_ =
        java.util.Collections.emptyList();
      private void ensureTargetStorageTypesIsMutable() {
        if (!((bitField0_ & 0x00000020) != 0)) {
          targetStorageTypes_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto>(targetStorageTypes_);
          bitField0_ |= 0x00000020;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProtoOrBuilder> targetStorageTypesBuilder_;

      /**
       * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto> getTargetStorageTypesList() {
        if (targetStorageTypesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(targetStorageTypes_);
        } else {
          return targetStorageTypesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
       */
      public int getTargetStorageTypesCount() {
        if (targetStorageTypesBuilder_ == null) {
          return targetStorageTypes_.size();
        } else {
          return targetStorageTypesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto getTargetStorageTypes(int index) {
        if (targetStorageTypesBuilder_ == null) {
          return targetStorageTypes_.get(index);
        } else {
          return targetStorageTypesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
       */
      public Builder setTargetStorageTypes(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto value) {
        if (targetStorageTypesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureTargetStorageTypesIsMutable();
          targetStorageTypes_.set(index, value);
          onChanged();
        } else {
          targetStorageTypesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
       */
      public Builder setTargetStorageTypes(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto.Builder builderForValue) {
        if (targetStorageTypesBuilder_ == null) {
          ensureTargetStorageTypesIsMutable();
          targetStorageTypes_.set(index, builderForValue.build());
          onChanged();
        } else {
          targetStorageTypesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
       */
      public Builder addTargetStorageTypes(org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto value) {
        if (targetStorageTypesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureTargetStorageTypesIsMutable();
          targetStorageTypes_.add(value);
          onChanged();
        } else {
          targetStorageTypesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
       */
      public Builder addTargetStorageTypes(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto value) {
        if (targetStorageTypesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureTargetStorageTypesIsMutable();
          targetStorageTypes_.add(index, value);
          onChanged();
        } else {
          targetStorageTypesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
       */
      public Builder addTargetStorageTypes(
          org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto.Builder builderForValue) {
        if (targetStorageTypesBuilder_ == null) {
          ensureTargetStorageTypesIsMutable();
          targetStorageTypes_.add(builderForValue.build());
          onChanged();
        } else {
          targetStorageTypesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
       */
      public Builder addTargetStorageTypes(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto.Builder builderForValue) {
        if (targetStorageTypesBuilder_ == null) {
          ensureTargetStorageTypesIsMutable();
          targetStorageTypes_.add(index, builderForValue.build());
          onChanged();
        } else {
          targetStorageTypesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
       */
      public Builder addAllTargetStorageTypes(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto> values) {
        if (targetStorageTypesBuilder_ == null) {
          ensureTargetStorageTypesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, targetStorageTypes_);
          onChanged();
        } else {
          targetStorageTypesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
       */
      public Builder clearTargetStorageTypes() {
        if (targetStorageTypesBuilder_ == null) {
          targetStorageTypes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
          onChanged();
        } else {
          targetStorageTypesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
       */
      public Builder removeTargetStorageTypes(int index) {
        if (targetStorageTypesBuilder_ == null) {
          ensureTargetStorageTypesIsMutable();
          targetStorageTypes_.remove(index);
          onChanged();
        } else {
          targetStorageTypesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto.Builder getTargetStorageTypesBuilder(
          int index) {
        return getTargetStorageTypesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProtoOrBuilder getTargetStorageTypesOrBuilder(
          int index) {
        if (targetStorageTypesBuilder_ == null) {
          return targetStorageTypes_.get(index);  } else {
          return targetStorageTypesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProtoOrBuilder>
           getTargetStorageTypesOrBuilderList() {
        if (targetStorageTypesBuilder_ != null) {
          return targetStorageTypesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(targetStorageTypes_);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto.Builder addTargetStorageTypesBuilder() {
        return getTargetStorageTypesFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto.Builder addTargetStorageTypesBuilder(
          int index) {
        return getTargetStorageTypesFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageTypesProto targetStorageTypes = 6;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto.Builder>
           getTargetStorageTypesBuilderList() {
        return getTargetStorageTypesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProtoOrBuilder>
          getTargetStorageTypesFieldBuilder() {
        if (targetStorageTypesBuilder_ == null) {
          targetStorageTypesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageTypesProtoOrBuilder>(
                  targetStorageTypes_,
                  ((bitField0_ & 0x00000020) != 0),
                  getParentForChildren(),
                  isClean());
          targetStorageTypes_ = null;
        }
        return targetStorageTypesBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.BlockCommandProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.BlockCommandProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<BlockCommandProto>
        PARSER = new com.google.protobuf.AbstractParser<BlockCommandProto>() {
      @java.lang.Override
      public BlockCommandProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new BlockCommandProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<BlockCommandProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<BlockCommandProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockCommandProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface BlockIdCommandProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.BlockIdCommandProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required .org.apache.hadoop.datanode.BlockIdCommandProto.Action action = 1;</code>
     * @return Whether the action field is set.
     */
    boolean hasAction();
    /**
     * <code>required .org.apache.hadoop.datanode.BlockIdCommandProto.Action action = 1;</code>
     * @return The action.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Action getAction();

    /**
     * <code>required string blockPoolId = 2;</code>
     * @return Whether the blockPoolId field is set.
     */
    boolean hasBlockPoolId();
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The blockPoolId.
     */
    java.lang.String getBlockPoolId();
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The bytes for blockPoolId.
     */
    com.google.protobuf.ByteString
        getBlockPoolIdBytes();

    /**
     * <code>repeated uint64 blockIds = 3 [packed = true];</code>
     * @return A list containing the blockIds.
     */
    java.util.List<java.lang.Long> getBlockIdsList();
    /**
     * <code>repeated uint64 blockIds = 3 [packed = true];</code>
     * @return The count of blockIds.
     */
    int getBlockIdsCount();
    /**
     * <code>repeated uint64 blockIds = 3 [packed = true];</code>
     * @param index The index of the element to return.
     * @return The blockIds at the given index.
     */
    long getBlockIds(int index);
  }
  /**
   * <pre>
   **
   * Command to instruct datanodes to perform certain action
   * on the given set of block IDs.
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.BlockIdCommandProto}
   */
  public static final class BlockIdCommandProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.BlockIdCommandProto)
      BlockIdCommandProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use BlockIdCommandProto.newBuilder() to construct.
    private BlockIdCommandProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private BlockIdCommandProto() {
      action_ = 1;
      blockPoolId_ = "";
      blockIds_ = emptyLongList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new BlockIdCommandProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private BlockIdCommandProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Action value = org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Action.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(1, rawValue);
              } else {
                bitField0_ |= 0x00000001;
                action_ = rawValue;
              }
              break;
            }
            case 18: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000002;
              blockPoolId_ = bs;
              break;
            }
            case 24: {
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                blockIds_ = newLongList();
                mutable_bitField0_ |= 0x00000004;
              }
              blockIds_.addLong(input.readUInt64());
              break;
            }
            case 26: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000004) != 0) && input.getBytesUntilLimit() > 0) {
                blockIds_ = newLongList();
                mutable_bitField0_ |= 0x00000004;
              }
              while (input.getBytesUntilLimit() > 0) {
                blockIds_.addLong(input.readUInt64());
              }
              input.popLimit(limit);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          blockIds_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockIdCommandProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockIdCommandProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Builder.class);
    }

    /**
     * Protobuf enum {@code org.apache.hadoop.datanode.BlockIdCommandProto.Action}
     */
    public enum Action
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <code>CACHE = 1;</code>
       */
      CACHE(1),
      /**
       * <code>UNCACHE = 2;</code>
       */
      UNCACHE(2),
      ;

      /**
       * <code>CACHE = 1;</code>
       */
      public static final int CACHE_VALUE = 1;
      /**
       * <code>UNCACHE = 2;</code>
       */
      public static final int UNCACHE_VALUE = 2;


      public final int getNumber() {
        return value;
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static Action valueOf(int value) {
        return forNumber(value);
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       */
      public static Action forNumber(int value) {
        switch (value) {
          case 1: return CACHE;
          case 2: return UNCACHE;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<Action>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          Action> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<Action>() {
              public Action findValueByNumber(int number) {
                return Action.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.getDescriptor().getEnumTypes().get(0);
      }

      private static final Action[] VALUES = values();

      public static Action valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private Action(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:org.apache.hadoop.datanode.BlockIdCommandProto.Action)
    }

    private int bitField0_;
    public static final int ACTION_FIELD_NUMBER = 1;
    private int action_;
    /**
     * <code>required .org.apache.hadoop.datanode.BlockIdCommandProto.Action action = 1;</code>
     * @return Whether the action field is set.
     */
    @java.lang.Override public boolean hasAction() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required .org.apache.hadoop.datanode.BlockIdCommandProto.Action action = 1;</code>
     * @return The action.
     */
    @java.lang.Override public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Action getAction() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Action result = org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Action.valueOf(action_);
      return result == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Action.CACHE : result;
    }

    public static final int BLOCKPOOLID_FIELD_NUMBER = 2;
    private volatile java.lang.Object blockPoolId_;
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return Whether the blockPoolId field is set.
     */
    @java.lang.Override
    public boolean hasBlockPoolId() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The blockPoolId.
     */
    @java.lang.Override
    public java.lang.String getBlockPoolId() {
      java.lang.Object ref = blockPoolId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          blockPoolId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The bytes for blockPoolId.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getBlockPoolIdBytes() {
      java.lang.Object ref = blockPoolId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        blockPoolId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int BLOCKIDS_FIELD_NUMBER = 3;
    private com.google.protobuf.Internal.LongList blockIds_;
    /**
     * <code>repeated uint64 blockIds = 3 [packed = true];</code>
     * @return A list containing the blockIds.
     */
    @java.lang.Override
    public java.util.List<java.lang.Long>
        getBlockIdsList() {
      return blockIds_;
    }
    /**
     * <code>repeated uint64 blockIds = 3 [packed = true];</code>
     * @return The count of blockIds.
     */
    public int getBlockIdsCount() {
      return blockIds_.size();
    }
    /**
     * <code>repeated uint64 blockIds = 3 [packed = true];</code>
     * @param index The index of the element to return.
     * @return The blockIds at the given index.
     */
    public long getBlockIds(int index) {
      return blockIds_.getLong(index);
    }
    private int blockIdsMemoizedSerializedSize = -1;

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasAction()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasBlockPoolId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeEnum(1, action_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, blockPoolId_);
      }
      if (getBlockIdsList().size() > 0) {
        output.writeUInt32NoTag(26);
        output.writeUInt32NoTag(blockIdsMemoizedSerializedSize);
      }
      for (int i = 0; i < blockIds_.size(); i++) {
        output.writeUInt64NoTag(blockIds_.getLong(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, action_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, blockPoolId_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < blockIds_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeUInt64SizeNoTag(blockIds_.getLong(i));
        }
        size += dataSize;
        if (!getBlockIdsList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        blockIdsMemoizedSerializedSize = dataSize;
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto) obj;

      if (hasAction() != other.hasAction()) return false;
      if (hasAction()) {
        if (action_ != other.action_) return false;
      }
      if (hasBlockPoolId() != other.hasBlockPoolId()) return false;
      if (hasBlockPoolId()) {
        if (!getBlockPoolId()
            .equals(other.getBlockPoolId())) return false;
      }
      if (!getBlockIdsList()
          .equals(other.getBlockIdsList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasAction()) {
        hash = (37 * hash) + ACTION_FIELD_NUMBER;
        hash = (53 * hash) + action_;
      }
      if (hasBlockPoolId()) {
        hash = (37 * hash) + BLOCKPOOLID_FIELD_NUMBER;
        hash = (53 * hash) + getBlockPoolId().hashCode();
      }
      if (getBlockIdsCount() > 0) {
        hash = (37 * hash) + BLOCKIDS_FIELD_NUMBER;
        hash = (53 * hash) + getBlockIdsList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * Command to instruct datanodes to perform certain action
     * on the given set of block IDs.
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.BlockIdCommandProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.BlockIdCommandProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockIdCommandProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockIdCommandProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        action_ = 1;
        bitField0_ = (bitField0_ & ~0x00000001);
        blockPoolId_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        blockIds_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockIdCommandProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.action_ = action_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.blockPoolId_ = blockPoolId_;
        if (((bitField0_ & 0x00000004) != 0)) {
          blockIds_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000004);
        }
        result.blockIds_ = blockIds_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.getDefaultInstance()) return this;
        if (other.hasAction()) {
          setAction(other.getAction());
        }
        if (other.hasBlockPoolId()) {
          bitField0_ |= 0x00000002;
          blockPoolId_ = other.blockPoolId_;
          onChanged();
        }
        if (!other.blockIds_.isEmpty()) {
          if (blockIds_.isEmpty()) {
            blockIds_ = other.blockIds_;
            bitField0_ = (bitField0_ & ~0x00000004);
          } else {
            ensureBlockIdsIsMutable();
            blockIds_.addAll(other.blockIds_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasAction()) {
          return false;
        }
        if (!hasBlockPoolId()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int action_ = 1;
      /**
       * <code>required .org.apache.hadoop.datanode.BlockIdCommandProto.Action action = 1;</code>
       * @return Whether the action field is set.
       */
      @java.lang.Override public boolean hasAction() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required .org.apache.hadoop.datanode.BlockIdCommandProto.Action action = 1;</code>
       * @return The action.
       */
      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Action getAction() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Action result = org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Action.valueOf(action_);
        return result == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Action.CACHE : result;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.BlockIdCommandProto.Action action = 1;</code>
       * @param value The action to set.
       * @return This builder for chaining.
       */
      public Builder setAction(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto.Action value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000001;
        action_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.BlockIdCommandProto.Action action = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearAction() {
        bitField0_ = (bitField0_ & ~0x00000001);
        action_ = 1;
        onChanged();
        return this;
      }

      private java.lang.Object blockPoolId_ = "";
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return Whether the blockPoolId field is set.
       */
      public boolean hasBlockPoolId() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return The blockPoolId.
       */
      public java.lang.String getBlockPoolId() {
        java.lang.Object ref = blockPoolId_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            blockPoolId_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return The bytes for blockPoolId.
       */
      public com.google.protobuf.ByteString
          getBlockPoolIdBytes() {
        java.lang.Object ref = blockPoolId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          blockPoolId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @param value The blockPoolId to set.
       * @return This builder for chaining.
       */
      public Builder setBlockPoolId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        blockPoolId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearBlockPoolId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        blockPoolId_ = getDefaultInstance().getBlockPoolId();
        onChanged();
        return this;
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @param value The bytes for blockPoolId to set.
       * @return This builder for chaining.
       */
      public Builder setBlockPoolIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        blockPoolId_ = value;
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.LongList blockIds_ = emptyLongList();
      private void ensureBlockIdsIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          blockIds_ = mutableCopy(blockIds_);
          bitField0_ |= 0x00000004;
         }
      }
      /**
       * <code>repeated uint64 blockIds = 3 [packed = true];</code>
       * @return A list containing the blockIds.
       */
      public java.util.List<java.lang.Long>
          getBlockIdsList() {
        return ((bitField0_ & 0x00000004) != 0) ?
                 java.util.Collections.unmodifiableList(blockIds_) : blockIds_;
      }
      /**
       * <code>repeated uint64 blockIds = 3 [packed = true];</code>
       * @return The count of blockIds.
       */
      public int getBlockIdsCount() {
        return blockIds_.size();
      }
      /**
       * <code>repeated uint64 blockIds = 3 [packed = true];</code>
       * @param index The index of the element to return.
       * @return The blockIds at the given index.
       */
      public long getBlockIds(int index) {
        return blockIds_.getLong(index);
      }
      /**
       * <code>repeated uint64 blockIds = 3 [packed = true];</code>
       * @param index The index to set the value at.
       * @param value The blockIds to set.
       * @return This builder for chaining.
       */
      public Builder setBlockIds(
          int index, long value) {
        ensureBlockIdsIsMutable();
        blockIds_.setLong(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint64 blockIds = 3 [packed = true];</code>
       * @param value The blockIds to add.
       * @return This builder for chaining.
       */
      public Builder addBlockIds(long value) {
        ensureBlockIdsIsMutable();
        blockIds_.addLong(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint64 blockIds = 3 [packed = true];</code>
       * @param values The blockIds to add.
       * @return This builder for chaining.
       */
      public Builder addAllBlockIds(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureBlockIdsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, blockIds_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint64 blockIds = 3 [packed = true];</code>
       * @return This builder for chaining.
       */
      public Builder clearBlockIds() {
        blockIds_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000004);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.BlockIdCommandProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.BlockIdCommandProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<BlockIdCommandProto>
        PARSER = new com.google.protobuf.AbstractParser<BlockIdCommandProto>() {
      @java.lang.Override
      public BlockIdCommandProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new BlockIdCommandProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<BlockIdCommandProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<BlockIdCommandProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockIdCommandProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface BlockRecoveryCommandProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.BlockRecoveryCommandProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto>
        getBlocksList();
    /**
     * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto getBlocks(int index);
    /**
     * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
     */
    int getBlocksCount();
    /**
     * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProtoOrBuilder>
        getBlocksOrBuilderList();
    /**
     * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProtoOrBuilder getBlocksOrBuilder(
        int index);
  }
  /**
   * <pre>
   **
   * List of blocks to be recovered by the datanode
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.BlockRecoveryCommandProto}
   */
  public static final class BlockRecoveryCommandProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.BlockRecoveryCommandProto)
      BlockRecoveryCommandProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use BlockRecoveryCommandProto.newBuilder() to construct.
    private BlockRecoveryCommandProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private BlockRecoveryCommandProto() {
      blocks_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new BlockRecoveryCommandProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private BlockRecoveryCommandProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                blocks_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              blocks_.add(
                  input.readMessage(org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto.PARSER, extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          blocks_ = java.util.Collections.unmodifiableList(blocks_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockRecoveryCommandProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockRecoveryCommandProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.Builder.class);
    }

    public static final int BLOCKS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto> blocks_;
    /**
     * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto> getBlocksList() {
      return blocks_;
    }
    /**
     * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProtoOrBuilder>
        getBlocksOrBuilderList() {
      return blocks_;
    }
    /**
     * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
     */
    @java.lang.Override
    public int getBlocksCount() {
      return blocks_.size();
    }
    /**
     * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto getBlocks(int index) {
      return blocks_.get(index);
    }
    /**
     * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProtoOrBuilder getBlocksOrBuilder(
        int index) {
      return blocks_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      for (int i = 0; i < getBlocksCount(); i++) {
        if (!getBlocks(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < blocks_.size(); i++) {
        output.writeMessage(1, blocks_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < blocks_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, blocks_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto) obj;

      if (!getBlocksList()
          .equals(other.getBlocksList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getBlocksCount() > 0) {
        hash = (37 * hash) + BLOCKS_FIELD_NUMBER;
        hash = (53 * hash) + getBlocksList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * List of blocks to be recovered by the datanode
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.BlockRecoveryCommandProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.BlockRecoveryCommandProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockRecoveryCommandProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockRecoveryCommandProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getBlocksFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (blocksBuilder_ == null) {
          blocks_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          blocksBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockRecoveryCommandProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto(this);
        int from_bitField0_ = bitField0_;
        if (blocksBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            blocks_ = java.util.Collections.unmodifiableList(blocks_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.blocks_ = blocks_;
        } else {
          result.blocks_ = blocksBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto.getDefaultInstance()) return this;
        if (blocksBuilder_ == null) {
          if (!other.blocks_.isEmpty()) {
            if (blocks_.isEmpty()) {
              blocks_ = other.blocks_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureBlocksIsMutable();
              blocks_.addAll(other.blocks_);
            }
            onChanged();
          }
        } else {
          if (!other.blocks_.isEmpty()) {
            if (blocksBuilder_.isEmpty()) {
              blocksBuilder_.dispose();
              blocksBuilder_ = null;
              blocks_ = other.blocks_;
              bitField0_ = (bitField0_ & ~0x00000001);
              blocksBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getBlocksFieldBuilder() : null;
            } else {
              blocksBuilder_.addAllMessages(other.blocks_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        for (int i = 0; i < getBlocksCount(); i++) {
          if (!getBlocks(i).isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto> blocks_ =
        java.util.Collections.emptyList();
      private void ensureBlocksIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          blocks_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto>(blocks_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProtoOrBuilder> blocksBuilder_;

      /**
       * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto> getBlocksList() {
        if (blocksBuilder_ == null) {
          return java.util.Collections.unmodifiableList(blocks_);
        } else {
          return blocksBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
       */
      public int getBlocksCount() {
        if (blocksBuilder_ == null) {
          return blocks_.size();
        } else {
          return blocksBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto getBlocks(int index) {
        if (blocksBuilder_ == null) {
          return blocks_.get(index);
        } else {
          return blocksBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
       */
      public Builder setBlocks(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto value) {
        if (blocksBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBlocksIsMutable();
          blocks_.set(index, value);
          onChanged();
        } else {
          blocksBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
       */
      public Builder setBlocks(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto.Builder builderForValue) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.set(index, builderForValue.build());
          onChanged();
        } else {
          blocksBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
       */
      public Builder addBlocks(org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto value) {
        if (blocksBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBlocksIsMutable();
          blocks_.add(value);
          onChanged();
        } else {
          blocksBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
       */
      public Builder addBlocks(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto value) {
        if (blocksBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBlocksIsMutable();
          blocks_.add(index, value);
          onChanged();
        } else {
          blocksBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
       */
      public Builder addBlocks(
          org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto.Builder builderForValue) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.add(builderForValue.build());
          onChanged();
        } else {
          blocksBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
       */
      public Builder addBlocks(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto.Builder builderForValue) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.add(index, builderForValue.build());
          onChanged();
        } else {
          blocksBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
       */
      public Builder addAllBlocks(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto> values) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, blocks_);
          onChanged();
        } else {
          blocksBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
       */
      public Builder clearBlocks() {
        if (blocksBuilder_ == null) {
          blocks_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          blocksBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
       */
      public Builder removeBlocks(int index) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.remove(index);
          onChanged();
        } else {
          blocksBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto.Builder getBlocksBuilder(
          int index) {
        return getBlocksFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProtoOrBuilder getBlocksOrBuilder(
          int index) {
        if (blocksBuilder_ == null) {
          return blocks_.get(index);  } else {
          return blocksBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProtoOrBuilder>
           getBlocksOrBuilderList() {
        if (blocksBuilder_ != null) {
          return blocksBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(blocks_);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto.Builder addBlocksBuilder() {
        return getBlocksFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto.Builder addBlocksBuilder(
          int index) {
        return getBlocksFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.RecoveringBlockProto blocks = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto.Builder>
           getBlocksBuilderList() {
        return getBlocksFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProtoOrBuilder>
          getBlocksFieldBuilder() {
        if (blocksBuilder_ == null) {
          blocksBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.RecoveringBlockProtoOrBuilder>(
                  blocks_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          blocks_ = null;
        }
        return blocksBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.BlockRecoveryCommandProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.BlockRecoveryCommandProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<BlockRecoveryCommandProto>
        PARSER = new com.google.protobuf.AbstractParser<BlockRecoveryCommandProto>() {
      @java.lang.Override
      public BlockRecoveryCommandProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new BlockRecoveryCommandProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<BlockRecoveryCommandProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<BlockRecoveryCommandProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockRecoveryCommandProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface FinalizeCommandProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.FinalizeCommandProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * Block pool to be finalized
     * </pre>
     *
     * <code>required string blockPoolId = 1;</code>
     * @return Whether the blockPoolId field is set.
     */
    boolean hasBlockPoolId();
    /**
     * <pre>
     * Block pool to be finalized
     * </pre>
     *
     * <code>required string blockPoolId = 1;</code>
     * @return The blockPoolId.
     */
    java.lang.String getBlockPoolId();
    /**
     * <pre>
     * Block pool to be finalized
     * </pre>
     *
     * <code>required string blockPoolId = 1;</code>
     * @return The bytes for blockPoolId.
     */
    com.google.protobuf.ByteString
        getBlockPoolIdBytes();
  }
  /**
   * <pre>
   **
   * Finalize the upgrade at the datanode
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.FinalizeCommandProto}
   */
  public static final class FinalizeCommandProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.FinalizeCommandProto)
      FinalizeCommandProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use FinalizeCommandProto.newBuilder() to construct.
    private FinalizeCommandProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private FinalizeCommandProto() {
      blockPoolId_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new FinalizeCommandProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private FinalizeCommandProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              blockPoolId_ = bs;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_FinalizeCommandProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_FinalizeCommandProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.Builder.class);
    }

    private int bitField0_;
    public static final int BLOCKPOOLID_FIELD_NUMBER = 1;
    private volatile java.lang.Object blockPoolId_;
    /**
     * <pre>
     * Block pool to be finalized
     * </pre>
     *
     * <code>required string blockPoolId = 1;</code>
     * @return Whether the blockPoolId field is set.
     */
    @java.lang.Override
    public boolean hasBlockPoolId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <pre>
     * Block pool to be finalized
     * </pre>
     *
     * <code>required string blockPoolId = 1;</code>
     * @return The blockPoolId.
     */
    @java.lang.Override
    public java.lang.String getBlockPoolId() {
      java.lang.Object ref = blockPoolId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          blockPoolId_ = s;
        }
        return s;
      }
    }
    /**
     * <pre>
     * Block pool to be finalized
     * </pre>
     *
     * <code>required string blockPoolId = 1;</code>
     * @return The bytes for blockPoolId.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getBlockPoolIdBytes() {
      java.lang.Object ref = blockPoolId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        blockPoolId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasBlockPoolId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, blockPoolId_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, blockPoolId_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto) obj;

      if (hasBlockPoolId() != other.hasBlockPoolId()) return false;
      if (hasBlockPoolId()) {
        if (!getBlockPoolId()
            .equals(other.getBlockPoolId())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasBlockPoolId()) {
        hash = (37 * hash) + BLOCKPOOLID_FIELD_NUMBER;
        hash = (53 * hash) + getBlockPoolId().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * Finalize the upgrade at the datanode
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.FinalizeCommandProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.FinalizeCommandProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_FinalizeCommandProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_FinalizeCommandProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        blockPoolId_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_FinalizeCommandProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.blockPoolId_ = blockPoolId_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto.getDefaultInstance()) return this;
        if (other.hasBlockPoolId()) {
          bitField0_ |= 0x00000001;
          blockPoolId_ = other.blockPoolId_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasBlockPoolId()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object blockPoolId_ = "";
      /**
       * <pre>
       * Block pool to be finalized
       * </pre>
       *
       * <code>required string blockPoolId = 1;</code>
       * @return Whether the blockPoolId field is set.
       */
      public boolean hasBlockPoolId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * Block pool to be finalized
       * </pre>
       *
       * <code>required string blockPoolId = 1;</code>
       * @return The blockPoolId.
       */
      public java.lang.String getBlockPoolId() {
        java.lang.Object ref = blockPoolId_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            blockPoolId_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       * Block pool to be finalized
       * </pre>
       *
       * <code>required string blockPoolId = 1;</code>
       * @return The bytes for blockPoolId.
       */
      public com.google.protobuf.ByteString
          getBlockPoolIdBytes() {
        java.lang.Object ref = blockPoolId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          blockPoolId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       * Block pool to be finalized
       * </pre>
       *
       * <code>required string blockPoolId = 1;</code>
       * @param value The blockPoolId to set.
       * @return This builder for chaining.
       */
      public Builder setBlockPoolId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        blockPoolId_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Block pool to be finalized
       * </pre>
       *
       * <code>required string blockPoolId = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearBlockPoolId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        blockPoolId_ = getDefaultInstance().getBlockPoolId();
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Block pool to be finalized
       * </pre>
       *
       * <code>required string blockPoolId = 1;</code>
       * @param value The bytes for blockPoolId to set.
       * @return This builder for chaining.
       */
      public Builder setBlockPoolIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        blockPoolId_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.FinalizeCommandProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.FinalizeCommandProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<FinalizeCommandProto>
        PARSER = new com.google.protobuf.AbstractParser<FinalizeCommandProto>() {
      @java.lang.Override
      public FinalizeCommandProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new FinalizeCommandProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<FinalizeCommandProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<FinalizeCommandProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.FinalizeCommandProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface StorageMismatchingHashesOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.StorageMismatchingHashes)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required string storageID = 1;</code>
     * @return Whether the storageID field is set.
     */
    boolean hasStorageID();
    /**
     * <code>required string storageID = 1;</code>
     * @return The storageID.
     */
    java.lang.String getStorageID();
    /**
     * <code>required string storageID = 1;</code>
     * @return The bytes for storageID.
     */
    com.google.protobuf.ByteString
        getStorageIDBytes();

    /**
     * <code>repeated uint32 bucketIDs = 2;</code>
     * @return A list containing the bucketIDs.
     */
    java.util.List<java.lang.Integer> getBucketIDsList();
    /**
     * <code>repeated uint32 bucketIDs = 2;</code>
     * @return The count of bucketIDs.
     */
    int getBucketIDsCount();
    /**
     * <code>repeated uint32 bucketIDs = 2;</code>
     * @param index The index of the element to return.
     * @return The bucketIDs at the given index.
     */
    int getBucketIDs(int index);
  }
  /**
   * Protobuf type {@code org.apache.hadoop.datanode.StorageMismatchingHashes}
   */
  public static final class StorageMismatchingHashes extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.StorageMismatchingHashes)
      StorageMismatchingHashesOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use StorageMismatchingHashes.newBuilder() to construct.
    private StorageMismatchingHashes(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private StorageMismatchingHashes() {
      storageID_ = "";
      bucketIDs_ = emptyIntList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new StorageMismatchingHashes();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private StorageMismatchingHashes(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              storageID_ = bs;
              break;
            }
            case 16: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                bucketIDs_ = newIntList();
                mutable_bitField0_ |= 0x00000002;
              }
              bucketIDs_.addInt(input.readUInt32());
              break;
            }
            case 18: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000002) != 0) && input.getBytesUntilLimit() > 0) {
                bucketIDs_ = newIntList();
                mutable_bitField0_ |= 0x00000002;
              }
              while (input.getBytesUntilLimit() > 0) {
                bucketIDs_.addInt(input.readUInt32());
              }
              input.popLimit(limit);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          bucketIDs_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageMismatchingHashes_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageMismatchingHashes_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.Builder.class);
    }

    private int bitField0_;
    public static final int STORAGEID_FIELD_NUMBER = 1;
    private volatile java.lang.Object storageID_;
    /**
     * <code>required string storageID = 1;</code>
     * @return Whether the storageID field is set.
     */
    @java.lang.Override
    public boolean hasStorageID() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required string storageID = 1;</code>
     * @return The storageID.
     */
    @java.lang.Override
    public java.lang.String getStorageID() {
      java.lang.Object ref = storageID_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          storageID_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string storageID = 1;</code>
     * @return The bytes for storageID.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getStorageIDBytes() {
      java.lang.Object ref = storageID_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        storageID_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int BUCKETIDS_FIELD_NUMBER = 2;
    private com.google.protobuf.Internal.IntList bucketIDs_;
    /**
     * <code>repeated uint32 bucketIDs = 2;</code>
     * @return A list containing the bucketIDs.
     */
    @java.lang.Override
    public java.util.List<java.lang.Integer>
        getBucketIDsList() {
      return bucketIDs_;
    }
    /**
     * <code>repeated uint32 bucketIDs = 2;</code>
     * @return The count of bucketIDs.
     */
    public int getBucketIDsCount() {
      return bucketIDs_.size();
    }
    /**
     * <code>repeated uint32 bucketIDs = 2;</code>
     * @param index The index of the element to return.
     * @return The bucketIDs at the given index.
     */
    public int getBucketIDs(int index) {
      return bucketIDs_.getInt(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasStorageID()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, storageID_);
      }
      for (int i = 0; i < bucketIDs_.size(); i++) {
        output.writeUInt32(2, bucketIDs_.getInt(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, storageID_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < bucketIDs_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeUInt32SizeNoTag(bucketIDs_.getInt(i));
        }
        size += dataSize;
        size += 1 * getBucketIDsList().size();
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes) obj;

      if (hasStorageID() != other.hasStorageID()) return false;
      if (hasStorageID()) {
        if (!getStorageID()
            .equals(other.getStorageID())) return false;
      }
      if (!getBucketIDsList()
          .equals(other.getBucketIDsList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasStorageID()) {
        hash = (37 * hash) + STORAGEID_FIELD_NUMBER;
        hash = (53 * hash) + getStorageID().hashCode();
      }
      if (getBucketIDsCount() > 0) {
        hash = (37 * hash) + BUCKETIDS_FIELD_NUMBER;
        hash = (53 * hash) + getBucketIDsList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.hadoop.datanode.StorageMismatchingHashes}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.StorageMismatchingHashes)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashesOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageMismatchingHashes_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageMismatchingHashes_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        storageID_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        bucketIDs_ = emptyIntList();
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageMismatchingHashes_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.storageID_ = storageID_;
        if (((bitField0_ & 0x00000002) != 0)) {
          bucketIDs_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.bucketIDs_ = bucketIDs_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.getDefaultInstance()) return this;
        if (other.hasStorageID()) {
          bitField0_ |= 0x00000001;
          storageID_ = other.storageID_;
          onChanged();
        }
        if (!other.bucketIDs_.isEmpty()) {
          if (bucketIDs_.isEmpty()) {
            bucketIDs_ = other.bucketIDs_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensureBucketIDsIsMutable();
            bucketIDs_.addAll(other.bucketIDs_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasStorageID()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object storageID_ = "";
      /**
       * <code>required string storageID = 1;</code>
       * @return Whether the storageID field is set.
       */
      public boolean hasStorageID() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required string storageID = 1;</code>
       * @return The storageID.
       */
      public java.lang.String getStorageID() {
        java.lang.Object ref = storageID_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            storageID_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string storageID = 1;</code>
       * @return The bytes for storageID.
       */
      public com.google.protobuf.ByteString
          getStorageIDBytes() {
        java.lang.Object ref = storageID_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          storageID_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string storageID = 1;</code>
       * @param value The storageID to set.
       * @return This builder for chaining.
       */
      public Builder setStorageID(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        storageID_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string storageID = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearStorageID() {
        bitField0_ = (bitField0_ & ~0x00000001);
        storageID_ = getDefaultInstance().getStorageID();
        onChanged();
        return this;
      }
      /**
       * <code>required string storageID = 1;</code>
       * @param value The bytes for storageID to set.
       * @return This builder for chaining.
       */
      public Builder setStorageIDBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        storageID_ = value;
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.IntList bucketIDs_ = emptyIntList();
      private void ensureBucketIDsIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          bucketIDs_ = mutableCopy(bucketIDs_);
          bitField0_ |= 0x00000002;
         }
      }
      /**
       * <code>repeated uint32 bucketIDs = 2;</code>
       * @return A list containing the bucketIDs.
       */
      public java.util.List<java.lang.Integer>
          getBucketIDsList() {
        return ((bitField0_ & 0x00000002) != 0) ?
                 java.util.Collections.unmodifiableList(bucketIDs_) : bucketIDs_;
      }
      /**
       * <code>repeated uint32 bucketIDs = 2;</code>
       * @return The count of bucketIDs.
       */
      public int getBucketIDsCount() {
        return bucketIDs_.size();
      }
      /**
       * <code>repeated uint32 bucketIDs = 2;</code>
       * @param index The index of the element to return.
       * @return The bucketIDs at the given index.
       */
      public int getBucketIDs(int index) {
        return bucketIDs_.getInt(index);
      }
      /**
       * <code>repeated uint32 bucketIDs = 2;</code>
       * @param index The index to set the value at.
       * @param value The bucketIDs to set.
       * @return This builder for chaining.
       */
      public Builder setBucketIDs(
          int index, int value) {
        ensureBucketIDsIsMutable();
        bucketIDs_.setInt(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint32 bucketIDs = 2;</code>
       * @param value The bucketIDs to add.
       * @return This builder for chaining.
       */
      public Builder addBucketIDs(int value) {
        ensureBucketIDsIsMutable();
        bucketIDs_.addInt(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint32 bucketIDs = 2;</code>
       * @param values The bucketIDs to add.
       * @return This builder for chaining.
       */
      public Builder addAllBucketIDs(
          java.lang.Iterable<? extends java.lang.Integer> values) {
        ensureBucketIDsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, bucketIDs_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint32 bucketIDs = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearBucketIDs() {
        bucketIDs_ = emptyIntList();
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.StorageMismatchingHashes)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.StorageMismatchingHashes)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<StorageMismatchingHashes>
        PARSER = new com.google.protobuf.AbstractParser<StorageMismatchingHashes>() {
      @java.lang.Override
      public StorageMismatchingHashes parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StorageMismatchingHashes(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<StorageMismatchingHashes> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<StorageMismatchingHashes> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface AllStorageMismatchingHashesOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.AllStorageMismatchingHashes)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes>
        getStoragesList();
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes getStorages(int index);
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
     */
    int getStoragesCount();
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashesOrBuilder>
        getStoragesOrBuilderList();
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashesOrBuilder getStoragesOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code org.apache.hadoop.datanode.AllStorageMismatchingHashes}
   */
  public static final class AllStorageMismatchingHashes extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.AllStorageMismatchingHashes)
      AllStorageMismatchingHashesOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use AllStorageMismatchingHashes.newBuilder() to construct.
    private AllStorageMismatchingHashes(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private AllStorageMismatchingHashes() {
      storages_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new AllStorageMismatchingHashes();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private AllStorageMismatchingHashes(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                storages_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes>();
                mutable_bitField0_ |= 0x00000001;
              }
              storages_.add(
                  input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.PARSER, extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          storages_ = java.util.Collections.unmodifiableList(storages_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_AllStorageMismatchingHashes_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_AllStorageMismatchingHashes_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.Builder.class);
    }

    public static final int STORAGES_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes> storages_;
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes> getStoragesList() {
      return storages_;
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashesOrBuilder>
        getStoragesOrBuilderList() {
      return storages_;
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
     */
    @java.lang.Override
    public int getStoragesCount() {
      return storages_.size();
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes getStorages(int index) {
      return storages_.get(index);
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashesOrBuilder getStoragesOrBuilder(
        int index) {
      return storages_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      for (int i = 0; i < getStoragesCount(); i++) {
        if (!getStorages(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < storages_.size(); i++) {
        output.writeMessage(1, storages_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < storages_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, storages_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes) obj;

      if (!getStoragesList()
          .equals(other.getStoragesList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getStoragesCount() > 0) {
        hash = (37 * hash) + STORAGES_FIELD_NUMBER;
        hash = (53 * hash) + getStoragesList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.hadoop.datanode.AllStorageMismatchingHashes}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.AllStorageMismatchingHashes)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashesOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_AllStorageMismatchingHashes_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_AllStorageMismatchingHashes_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getStoragesFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (storagesBuilder_ == null) {
          storages_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          storagesBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_AllStorageMismatchingHashes_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes(this);
        int from_bitField0_ = bitField0_;
        if (storagesBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            storages_ = java.util.Collections.unmodifiableList(storages_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.storages_ = storages_;
        } else {
          result.storages_ = storagesBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.getDefaultInstance()) return this;
        if (storagesBuilder_ == null) {
          if (!other.storages_.isEmpty()) {
            if (storages_.isEmpty()) {
              storages_ = other.storages_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureStoragesIsMutable();
              storages_.addAll(other.storages_);
            }
            onChanged();
          }
        } else {
          if (!other.storages_.isEmpty()) {
            if (storagesBuilder_.isEmpty()) {
              storagesBuilder_.dispose();
              storagesBuilder_ = null;
              storages_ = other.storages_;
              bitField0_ = (bitField0_ & ~0x00000001);
              storagesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getStoragesFieldBuilder() : null;
            } else {
              storagesBuilder_.addAllMessages(other.storages_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        for (int i = 0; i < getStoragesCount(); i++) {
          if (!getStorages(i).isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes> storages_ =
        java.util.Collections.emptyList();
      private void ensureStoragesIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          storages_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes>(storages_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashesOrBuilder> storagesBuilder_;

      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes> getStoragesList() {
        if (storagesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(storages_);
        } else {
          return storagesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
       */
      public int getStoragesCount() {
        if (storagesBuilder_ == null) {
          return storages_.size();
        } else {
          return storagesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes getStorages(int index) {
        if (storagesBuilder_ == null) {
          return storages_.get(index);
        } else {
          return storagesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
       */
      public Builder setStorages(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes value) {
        if (storagesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStoragesIsMutable();
          storages_.set(index, value);
          onChanged();
        } else {
          storagesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
       */
      public Builder setStorages(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.Builder builderForValue) {
        if (storagesBuilder_ == null) {
          ensureStoragesIsMutable();
          storages_.set(index, builderForValue.build());
          onChanged();
        } else {
          storagesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
       */
      public Builder addStorages(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes value) {
        if (storagesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStoragesIsMutable();
          storages_.add(value);
          onChanged();
        } else {
          storagesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
       */
      public Builder addStorages(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes value) {
        if (storagesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStoragesIsMutable();
          storages_.add(index, value);
          onChanged();
        } else {
          storagesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
       */
      public Builder addStorages(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.Builder builderForValue) {
        if (storagesBuilder_ == null) {
          ensureStoragesIsMutable();
          storages_.add(builderForValue.build());
          onChanged();
        } else {
          storagesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
       */
      public Builder addStorages(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.Builder builderForValue) {
        if (storagesBuilder_ == null) {
          ensureStoragesIsMutable();
          storages_.add(index, builderForValue.build());
          onChanged();
        } else {
          storagesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
       */
      public Builder addAllStorages(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes> values) {
        if (storagesBuilder_ == null) {
          ensureStoragesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, storages_);
          onChanged();
        } else {
          storagesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
       */
      public Builder clearStorages() {
        if (storagesBuilder_ == null) {
          storages_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          storagesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
       */
      public Builder removeStorages(int index) {
        if (storagesBuilder_ == null) {
          ensureStoragesIsMutable();
          storages_.remove(index);
          onChanged();
        } else {
          storagesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.Builder getStoragesBuilder(
          int index) {
        return getStoragesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashesOrBuilder getStoragesOrBuilder(
          int index) {
        if (storagesBuilder_ == null) {
          return storages_.get(index);  } else {
          return storagesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashesOrBuilder>
           getStoragesOrBuilderList() {
        if (storagesBuilder_ != null) {
          return storagesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(storages_);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.Builder addStoragesBuilder() {
        return getStoragesFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.Builder addStoragesBuilder(
          int index) {
        return getStoragesFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageMismatchingHashes storages = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.Builder>
           getStoragesBuilderList() {
        return getStoragesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashesOrBuilder>
          getStoragesFieldBuilder() {
        if (storagesBuilder_ == null) {
          storagesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashes.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageMismatchingHashesOrBuilder>(
                  storages_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          storages_ = null;
        }
        return storagesBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.AllStorageMismatchingHashes)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.AllStorageMismatchingHashes)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<AllStorageMismatchingHashes>
        PARSER = new com.google.protobuf.AbstractParser<AllStorageMismatchingHashes>() {
      @java.lang.Override
      public AllStorageMismatchingHashes parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new AllStorageMismatchingHashes(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<AllStorageMismatchingHashes> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<AllStorageMismatchingHashes> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface HashMismatchCommandProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.HashMismatchCommandProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required .org.apache.hadoop.datanode.AllStorageMismatchingHashes storages = 1;</code>
     * @return Whether the storages field is set.
     */
    boolean hasStorages();
    /**
     * <code>required .org.apache.hadoop.datanode.AllStorageMismatchingHashes storages = 1;</code>
     * @return The storages.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes getStorages();
    /**
     * <code>required .org.apache.hadoop.datanode.AllStorageMismatchingHashes storages = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashesOrBuilder getStoragesOrBuilder();
  }
  /**
   * Protobuf type {@code org.apache.hadoop.datanode.HashMismatchCommandProto}
   */
  public static final class HashMismatchCommandProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.HashMismatchCommandProto)
      HashMismatchCommandProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use HashMismatchCommandProto.newBuilder() to construct.
    private HashMismatchCommandProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private HashMismatchCommandProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new HashMismatchCommandProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private HashMismatchCommandProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = storages_.toBuilder();
              }
              storages_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(storages_);
                storages_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HashMismatchCommandProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HashMismatchCommandProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.Builder.class);
    }

    private int bitField0_;
    public static final int STORAGES_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes storages_;
    /**
     * <code>required .org.apache.hadoop.datanode.AllStorageMismatchingHashes storages = 1;</code>
     * @return Whether the storages field is set.
     */
    @java.lang.Override
    public boolean hasStorages() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required .org.apache.hadoop.datanode.AllStorageMismatchingHashes storages = 1;</code>
     * @return The storages.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes getStorages() {
      return storages_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.getDefaultInstance() : storages_;
    }
    /**
     * <code>required .org.apache.hadoop.datanode.AllStorageMismatchingHashes storages = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashesOrBuilder getStoragesOrBuilder() {
      return storages_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.getDefaultInstance() : storages_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasStorages()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getStorages().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getStorages());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getStorages());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto) obj;

      if (hasStorages() != other.hasStorages()) return false;
      if (hasStorages()) {
        if (!getStorages()
            .equals(other.getStorages())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasStorages()) {
        hash = (37 * hash) + STORAGES_FIELD_NUMBER;
        hash = (53 * hash) + getStorages().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.hadoop.datanode.HashMismatchCommandProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.HashMismatchCommandProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HashMismatchCommandProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HashMismatchCommandProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getStoragesFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (storagesBuilder_ == null) {
          storages_ = null;
        } else {
          storagesBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HashMismatchCommandProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (storagesBuilder_ == null) {
            result.storages_ = storages_;
          } else {
            result.storages_ = storagesBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto.getDefaultInstance()) return this;
        if (other.hasStorages()) {
          mergeStorages(other.getStorages());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasStorages()) {
          return false;
        }
        if (!getStorages().isInitialized()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes storages_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashesOrBuilder> storagesBuilder_;
      /**
       * <code>required .org.apache.hadoop.datanode.AllStorageMismatchingHashes storages = 1;</code>
       * @return Whether the storages field is set.
       */
      public boolean hasStorages() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required .org.apache.hadoop.datanode.AllStorageMismatchingHashes storages = 1;</code>
       * @return The storages.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes getStorages() {
        if (storagesBuilder_ == null) {
          return storages_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.getDefaultInstance() : storages_;
        } else {
          return storagesBuilder_.getMessage();
        }
      }
      /**
       * <code>required .org.apache.hadoop.datanode.AllStorageMismatchingHashes storages = 1;</code>
       */
      public Builder setStorages(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes value) {
        if (storagesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          storages_ = value;
          onChanged();
        } else {
          storagesBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.AllStorageMismatchingHashes storages = 1;</code>
       */
      public Builder setStorages(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.Builder builderForValue) {
        if (storagesBuilder_ == null) {
          storages_ = builderForValue.build();
          onChanged();
        } else {
          storagesBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.AllStorageMismatchingHashes storages = 1;</code>
       */
      public Builder mergeStorages(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes value) {
        if (storagesBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              storages_ != null &&
              storages_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.getDefaultInstance()) {
            storages_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.newBuilder(storages_).mergeFrom(value).buildPartial();
          } else {
            storages_ = value;
          }
          onChanged();
        } else {
          storagesBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.AllStorageMismatchingHashes storages = 1;</code>
       */
      public Builder clearStorages() {
        if (storagesBuilder_ == null) {
          storages_ = null;
          onChanged();
        } else {
          storagesBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.AllStorageMismatchingHashes storages = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.Builder getStoragesBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getStoragesFieldBuilder().getBuilder();
      }
      /**
       * <code>required .org.apache.hadoop.datanode.AllStorageMismatchingHashes storages = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashesOrBuilder getStoragesOrBuilder() {
        if (storagesBuilder_ != null) {
          return storagesBuilder_.getMessageOrBuilder();
        } else {
          return storages_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.getDefaultInstance() : storages_;
        }
      }
      /**
       * <code>required .org.apache.hadoop.datanode.AllStorageMismatchingHashes storages = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashesOrBuilder>
          getStoragesFieldBuilder() {
        if (storagesBuilder_ == null) {
          storagesBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashes.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.AllStorageMismatchingHashesOrBuilder>(
                  getStorages(),
                  getParentForChildren(),
                  isClean());
          storages_ = null;
        }
        return storagesBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.HashMismatchCommandProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.HashMismatchCommandProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<HashMismatchCommandProto>
        PARSER = new com.google.protobuf.AbstractParser<HashMismatchCommandProto>() {
      @java.lang.Override
      public HashMismatchCommandProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new HashMismatchCommandProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<HashMismatchCommandProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<HashMismatchCommandProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HashMismatchCommandProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface KeyUpdateCommandProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.KeyUpdateCommandProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 1;</code>
     * @return Whether the keys field is set.
     */
    boolean hasKeys();
    /**
     * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 1;</code>
     * @return The keys.
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto getKeys();
    /**
     * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProtoOrBuilder getKeysOrBuilder();
  }
  /**
   * <pre>
   **
   * Update the block keys at the datanode
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.KeyUpdateCommandProto}
   */
  public static final class KeyUpdateCommandProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.KeyUpdateCommandProto)
      KeyUpdateCommandProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use KeyUpdateCommandProto.newBuilder() to construct.
    private KeyUpdateCommandProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private KeyUpdateCommandProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new KeyUpdateCommandProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private KeyUpdateCommandProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = keys_.toBuilder();
              }
              keys_ = input.readMessage(org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(keys_);
                keys_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_KeyUpdateCommandProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_KeyUpdateCommandProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.Builder.class);
    }

    private int bitField0_;
    public static final int KEYS_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto keys_;
    /**
     * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 1;</code>
     * @return Whether the keys field is set.
     */
    @java.lang.Override
    public boolean hasKeys() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 1;</code>
     * @return The keys.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto getKeys() {
      return keys_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.getDefaultInstance() : keys_;
    }
    /**
     * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProtoOrBuilder getKeysOrBuilder() {
      return keys_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.getDefaultInstance() : keys_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasKeys()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getKeys().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getKeys());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getKeys());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto) obj;

      if (hasKeys() != other.hasKeys()) return false;
      if (hasKeys()) {
        if (!getKeys()
            .equals(other.getKeys())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasKeys()) {
        hash = (37 * hash) + KEYS_FIELD_NUMBER;
        hash = (53 * hash) + getKeys().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * Update the block keys at the datanode
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.KeyUpdateCommandProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.KeyUpdateCommandProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_KeyUpdateCommandProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_KeyUpdateCommandProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getKeysFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (keysBuilder_ == null) {
          keys_ = null;
        } else {
          keysBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_KeyUpdateCommandProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (keysBuilder_ == null) {
            result.keys_ = keys_;
          } else {
            result.keys_ = keysBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto.getDefaultInstance()) return this;
        if (other.hasKeys()) {
          mergeKeys(other.getKeys());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasKeys()) {
          return false;
        }
        if (!getKeys().isInitialized()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto keys_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProtoOrBuilder> keysBuilder_;
      /**
       * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 1;</code>
       * @return Whether the keys field is set.
       */
      public boolean hasKeys() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 1;</code>
       * @return The keys.
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto getKeys() {
        if (keysBuilder_ == null) {
          return keys_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.getDefaultInstance() : keys_;
        } else {
          return keysBuilder_.getMessage();
        }
      }
      /**
       * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 1;</code>
       */
      public Builder setKeys(org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto value) {
        if (keysBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          keys_ = value;
          onChanged();
        } else {
          keysBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 1;</code>
       */
      public Builder setKeys(
          org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.Builder builderForValue) {
        if (keysBuilder_ == null) {
          keys_ = builderForValue.build();
          onChanged();
        } else {
          keysBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 1;</code>
       */
      public Builder mergeKeys(org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto value) {
        if (keysBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              keys_ != null &&
              keys_ != org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.getDefaultInstance()) {
            keys_ =
              org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.newBuilder(keys_).mergeFrom(value).buildPartial();
          } else {
            keys_ = value;
          }
          onChanged();
        } else {
          keysBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 1;</code>
       */
      public Builder clearKeys() {
        if (keysBuilder_ == null) {
          keys_ = null;
          onChanged();
        } else {
          keysBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.Builder getKeysBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getKeysFieldBuilder().getBuilder();
      }
      /**
       * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProtoOrBuilder getKeysOrBuilder() {
        if (keysBuilder_ != null) {
          return keysBuilder_.getMessageOrBuilder();
        } else {
          return keys_ == null ?
              org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.getDefaultInstance() : keys_;
        }
      }
      /**
       * <code>required .org.apache.hadoop.ExportedBlockKeysProto keys = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProtoOrBuilder>
          getKeysFieldBuilder() {
        if (keysBuilder_ == null) {
          keysBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.ExportedBlockKeysProtoOrBuilder>(
                  getKeys(),
                  getParentForChildren(),
                  isClean());
          keys_ = null;
        }
        return keysBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.KeyUpdateCommandProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.KeyUpdateCommandProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<KeyUpdateCommandProto>
        PARSER = new com.google.protobuf.AbstractParser<KeyUpdateCommandProto>() {
      @java.lang.Override
      public KeyUpdateCommandProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new KeyUpdateCommandProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<KeyUpdateCommandProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<KeyUpdateCommandProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.KeyUpdateCommandProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface RegisterCommandProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.RegisterCommandProto)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * <pre>
   **
   * Instruct datanode to register with the namenode
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.RegisterCommandProto}
   */
  public static final class RegisterCommandProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.RegisterCommandProto)
      RegisterCommandProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use RegisterCommandProto.newBuilder() to construct.
    private RegisterCommandProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private RegisterCommandProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new RegisterCommandProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private RegisterCommandProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterCommandProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterCommandProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto) obj;

      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * Instruct datanode to register with the namenode
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.RegisterCommandProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.RegisterCommandProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterCommandProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterCommandProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterCommandProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.RegisterCommandProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.RegisterCommandProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<RegisterCommandProto>
        PARSER = new com.google.protobuf.AbstractParser<RegisterCommandProto>() {
      @java.lang.Override
      public RegisterCommandProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RegisterCommandProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<RegisterCommandProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<RegisterCommandProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterCommandProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface RegisterDatanodeRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.RegisterDatanodeRequestProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * Datanode info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return Whether the registration field is set.
     */
    boolean hasRegistration();
    /**
     * <pre>
     * Datanode info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return The registration.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration();
    /**
     * <pre>
     * Datanode info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder();
  }
  /**
   * <pre>
   **
   * registration - Information of the datanode registering with the namenode
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.RegisterDatanodeRequestProto}
   */
  public static final class RegisterDatanodeRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.RegisterDatanodeRequestProto)
      RegisterDatanodeRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use RegisterDatanodeRequestProto.newBuilder() to construct.
    private RegisterDatanodeRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private RegisterDatanodeRequestProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new RegisterDatanodeRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private RegisterDatanodeRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = registration_.toBuilder();
              }
              registration_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(registration_);
                registration_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int REGISTRATION_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto registration_;
    /**
     * <pre>
     * Datanode info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return Whether the registration field is set.
     */
    @java.lang.Override
    public boolean hasRegistration() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <pre>
     * Datanode info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return The registration.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration() {
      return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
    }
    /**
     * <pre>
     * Datanode info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder() {
      return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasRegistration()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getRegistration().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getRegistration());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getRegistration());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto) obj;

      if (hasRegistration() != other.hasRegistration()) return false;
      if (hasRegistration()) {
        if (!getRegistration()
            .equals(other.getRegistration())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasRegistration()) {
        hash = (37 * hash) + REGISTRATION_FIELD_NUMBER;
        hash = (53 * hash) + getRegistration().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * registration - Information of the datanode registering with the namenode
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.RegisterDatanodeRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.RegisterDatanodeRequestProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getRegistrationFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (registrationBuilder_ == null) {
          registration_ = null;
        } else {
          registrationBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (registrationBuilder_ == null) {
            result.registration_ = registration_;
          } else {
            result.registration_ = registrationBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto.getDefaultInstance()) return this;
        if (other.hasRegistration()) {
          mergeRegistration(other.getRegistration());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasRegistration()) {
          return false;
        }
        if (!getRegistration().isInitialized()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto registration_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder> registrationBuilder_;
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       * @return Whether the registration field is set.
       */
      public boolean hasRegistration() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       * @return The registration.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration() {
        if (registrationBuilder_ == null) {
          return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
        } else {
          return registrationBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder setRegistration(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto value) {
        if (registrationBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          registration_ = value;
          onChanged();
        } else {
          registrationBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder setRegistration(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder builderForValue) {
        if (registrationBuilder_ == null) {
          registration_ = builderForValue.build();
          onChanged();
        } else {
          registrationBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder mergeRegistration(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto value) {
        if (registrationBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              registration_ != null &&
              registration_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance()) {
            registration_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.newBuilder(registration_).mergeFrom(value).buildPartial();
          } else {
            registration_ = value;
          }
          onChanged();
        } else {
          registrationBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder clearRegistration() {
        if (registrationBuilder_ == null) {
          registration_ = null;
          onChanged();
        } else {
          registrationBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder getRegistrationBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getRegistrationFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder() {
        if (registrationBuilder_ != null) {
          return registrationBuilder_.getMessageOrBuilder();
        } else {
          return registration_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
        }
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder>
          getRegistrationFieldBuilder() {
        if (registrationBuilder_ == null) {
          registrationBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder>(
                  getRegistration(),
                  getParentForChildren(),
                  isClean());
          registration_ = null;
        }
        return registrationBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.RegisterDatanodeRequestProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.RegisterDatanodeRequestProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<RegisterDatanodeRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<RegisterDatanodeRequestProto>() {
      @java.lang.Override
      public RegisterDatanodeRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RegisterDatanodeRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<RegisterDatanodeRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<RegisterDatanodeRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface RegisterDatanodeResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.RegisterDatanodeResponseProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * Datanode info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return Whether the registration field is set.
     */
    boolean hasRegistration();
    /**
     * <pre>
     * Datanode info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return The registration.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration();
    /**
     * <pre>
     * Datanode info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder();
  }
  /**
   * <pre>
   **
   * registration - Update registration of the datanode that successfully 
   *                registered. StorageInfo will be updated to include new 
   *                storage ID if the datanode did not have one in the request.
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.RegisterDatanodeResponseProto}
   */
  public static final class RegisterDatanodeResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.RegisterDatanodeResponseProto)
      RegisterDatanodeResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use RegisterDatanodeResponseProto.newBuilder() to construct.
    private RegisterDatanodeResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private RegisterDatanodeResponseProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new RegisterDatanodeResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private RegisterDatanodeResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = registration_.toBuilder();
              }
              registration_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(registration_);
                registration_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto.Builder.class);
    }

    private int bitField0_;
    public static final int REGISTRATION_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto registration_;
    /**
     * <pre>
     * Datanode info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return Whether the registration field is set.
     */
    @java.lang.Override
    public boolean hasRegistration() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <pre>
     * Datanode info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return The registration.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration() {
      return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
    }
    /**
     * <pre>
     * Datanode info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder() {
      return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasRegistration()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getRegistration().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getRegistration());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getRegistration());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto) obj;

      if (hasRegistration() != other.hasRegistration()) return false;
      if (hasRegistration()) {
        if (!getRegistration()
            .equals(other.getRegistration())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasRegistration()) {
        hash = (37 * hash) + REGISTRATION_FIELD_NUMBER;
        hash = (53 * hash) + getRegistration().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * registration - Update registration of the datanode that successfully 
     *                registered. StorageInfo will be updated to include new 
     *                storage ID if the datanode did not have one in the request.
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.RegisterDatanodeResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.RegisterDatanodeResponseProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getRegistrationFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (registrationBuilder_ == null) {
          registration_ = null;
        } else {
          registrationBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (registrationBuilder_ == null) {
            result.registration_ = registration_;
          } else {
            result.registration_ = registrationBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto.getDefaultInstance()) return this;
        if (other.hasRegistration()) {
          mergeRegistration(other.getRegistration());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasRegistration()) {
          return false;
        }
        if (!getRegistration().isInitialized()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto registration_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder> registrationBuilder_;
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       * @return Whether the registration field is set.
       */
      public boolean hasRegistration() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       * @return The registration.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration() {
        if (registrationBuilder_ == null) {
          return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
        } else {
          return registrationBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder setRegistration(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto value) {
        if (registrationBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          registration_ = value;
          onChanged();
        } else {
          registrationBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder setRegistration(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder builderForValue) {
        if (registrationBuilder_ == null) {
          registration_ = builderForValue.build();
          onChanged();
        } else {
          registrationBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder mergeRegistration(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto value) {
        if (registrationBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              registration_ != null &&
              registration_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance()) {
            registration_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.newBuilder(registration_).mergeFrom(value).buildPartial();
          } else {
            registration_ = value;
          }
          onChanged();
        } else {
          registrationBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder clearRegistration() {
        if (registrationBuilder_ == null) {
          registration_ = null;
          onChanged();
        } else {
          registrationBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder getRegistrationBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getRegistrationFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder() {
        if (registrationBuilder_ != null) {
          return registrationBuilder_.getMessageOrBuilder();
        } else {
          return registration_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
        }
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder>
          getRegistrationFieldBuilder() {
        if (registrationBuilder_ == null) {
          registrationBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder>(
                  getRegistration(),
                  getParentForChildren(),
                  isClean());
          registration_ = null;
        }
        return registrationBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.RegisterDatanodeResponseProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.RegisterDatanodeResponseProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<RegisterDatanodeResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<RegisterDatanodeResponseProto>() {
      @java.lang.Override
      public RegisterDatanodeResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RegisterDatanodeResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<RegisterDatanodeResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<RegisterDatanodeResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface VolumeFailureSummaryProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.VolumeFailureSummaryProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated string failedStorageLocations = 1;</code>
     * @return A list containing the failedStorageLocations.
     */
    java.util.List<java.lang.String>
        getFailedStorageLocationsList();
    /**
     * <code>repeated string failedStorageLocations = 1;</code>
     * @return The count of failedStorageLocations.
     */
    int getFailedStorageLocationsCount();
    /**
     * <code>repeated string failedStorageLocations = 1;</code>
     * @param index The index of the element to return.
     * @return The failedStorageLocations at the given index.
     */
    java.lang.String getFailedStorageLocations(int index);
    /**
     * <code>repeated string failedStorageLocations = 1;</code>
     * @param index The index of the value to return.
     * @return The bytes of the failedStorageLocations at the given index.
     */
    com.google.protobuf.ByteString
        getFailedStorageLocationsBytes(int index);

    /**
     * <code>required uint64 lastVolumeFailureDate = 2;</code>
     * @return Whether the lastVolumeFailureDate field is set.
     */
    boolean hasLastVolumeFailureDate();
    /**
     * <code>required uint64 lastVolumeFailureDate = 2;</code>
     * @return The lastVolumeFailureDate.
     */
    long getLastVolumeFailureDate();

    /**
     * <code>required uint64 estimatedCapacityLostTotal = 3;</code>
     * @return Whether the estimatedCapacityLostTotal field is set.
     */
    boolean hasEstimatedCapacityLostTotal();
    /**
     * <code>required uint64 estimatedCapacityLostTotal = 3;</code>
     * @return The estimatedCapacityLostTotal.
     */
    long getEstimatedCapacityLostTotal();
  }
  /**
   * <pre>
   **
   * failedStorageLocations - storage locations that have failed
   * lastVolumeFailureDate - date/time of last volume failure
   * estimatedCapacityLost - estimate of total capacity lost due to volume failures
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.VolumeFailureSummaryProto}
   */
  public static final class VolumeFailureSummaryProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.VolumeFailureSummaryProto)
      VolumeFailureSummaryProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use VolumeFailureSummaryProto.newBuilder() to construct.
    private VolumeFailureSummaryProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private VolumeFailureSummaryProto() {
      failedStorageLocations_ = com.google.protobuf.LazyStringArrayList.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new VolumeFailureSummaryProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private VolumeFailureSummaryProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              com.google.protobuf.ByteString bs = input.readBytes();
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                failedStorageLocations_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000001;
              }
              failedStorageLocations_.add(bs);
              break;
            }
            case 16: {
              bitField0_ |= 0x00000001;
              lastVolumeFailureDate_ = input.readUInt64();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000002;
              estimatedCapacityLostTotal_ = input.readUInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          failedStorageLocations_ = failedStorageLocations_.getUnmodifiableView();
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_VolumeFailureSummaryProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_VolumeFailureSummaryProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.Builder.class);
    }

    private int bitField0_;
    public static final int FAILEDSTORAGELOCATIONS_FIELD_NUMBER = 1;
    private com.google.protobuf.LazyStringList failedStorageLocations_;
    /**
     * <code>repeated string failedStorageLocations = 1;</code>
     * @return A list containing the failedStorageLocations.
     */
    public com.google.protobuf.ProtocolStringList
        getFailedStorageLocationsList() {
      return failedStorageLocations_;
    }
    /**
     * <code>repeated string failedStorageLocations = 1;</code>
     * @return The count of failedStorageLocations.
     */
    public int getFailedStorageLocationsCount() {
      return failedStorageLocations_.size();
    }
    /**
     * <code>repeated string failedStorageLocations = 1;</code>
     * @param index The index of the element to return.
     * @return The failedStorageLocations at the given index.
     */
    public java.lang.String getFailedStorageLocations(int index) {
      return failedStorageLocations_.get(index);
    }
    /**
     * <code>repeated string failedStorageLocations = 1;</code>
     * @param index The index of the value to return.
     * @return The bytes of the failedStorageLocations at the given index.
     */
    public com.google.protobuf.ByteString
        getFailedStorageLocationsBytes(int index) {
      return failedStorageLocations_.getByteString(index);
    }

    public static final int LASTVOLUMEFAILUREDATE_FIELD_NUMBER = 2;
    private long lastVolumeFailureDate_;
    /**
     * <code>required uint64 lastVolumeFailureDate = 2;</code>
     * @return Whether the lastVolumeFailureDate field is set.
     */
    @java.lang.Override
    public boolean hasLastVolumeFailureDate() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required uint64 lastVolumeFailureDate = 2;</code>
     * @return The lastVolumeFailureDate.
     */
    @java.lang.Override
    public long getLastVolumeFailureDate() {
      return lastVolumeFailureDate_;
    }

    public static final int ESTIMATEDCAPACITYLOSTTOTAL_FIELD_NUMBER = 3;
    private long estimatedCapacityLostTotal_;
    /**
     * <code>required uint64 estimatedCapacityLostTotal = 3;</code>
     * @return Whether the estimatedCapacityLostTotal field is set.
     */
    @java.lang.Override
    public boolean hasEstimatedCapacityLostTotal() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>required uint64 estimatedCapacityLostTotal = 3;</code>
     * @return The estimatedCapacityLostTotal.
     */
    @java.lang.Override
    public long getEstimatedCapacityLostTotal() {
      return estimatedCapacityLostTotal_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasLastVolumeFailureDate()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasEstimatedCapacityLostTotal()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < failedStorageLocations_.size(); i++) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, failedStorageLocations_.getRaw(i));
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeUInt64(2, lastVolumeFailureDate_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeUInt64(3, estimatedCapacityLostTotal_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        for (int i = 0; i < failedStorageLocations_.size(); i++) {
          dataSize += computeStringSizeNoTag(failedStorageLocations_.getRaw(i));
        }
        size += dataSize;
        size += 1 * getFailedStorageLocationsList().size();
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, lastVolumeFailureDate_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, estimatedCapacityLostTotal_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto) obj;

      if (!getFailedStorageLocationsList()
          .equals(other.getFailedStorageLocationsList())) return false;
      if (hasLastVolumeFailureDate() != other.hasLastVolumeFailureDate()) return false;
      if (hasLastVolumeFailureDate()) {
        if (getLastVolumeFailureDate()
            != other.getLastVolumeFailureDate()) return false;
      }
      if (hasEstimatedCapacityLostTotal() != other.hasEstimatedCapacityLostTotal()) return false;
      if (hasEstimatedCapacityLostTotal()) {
        if (getEstimatedCapacityLostTotal()
            != other.getEstimatedCapacityLostTotal()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getFailedStorageLocationsCount() > 0) {
        hash = (37 * hash) + FAILEDSTORAGELOCATIONS_FIELD_NUMBER;
        hash = (53 * hash) + getFailedStorageLocationsList().hashCode();
      }
      if (hasLastVolumeFailureDate()) {
        hash = (37 * hash) + LASTVOLUMEFAILUREDATE_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getLastVolumeFailureDate());
      }
      if (hasEstimatedCapacityLostTotal()) {
        hash = (37 * hash) + ESTIMATEDCAPACITYLOSTTOTAL_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getEstimatedCapacityLostTotal());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * failedStorageLocations - storage locations that have failed
     * lastVolumeFailureDate - date/time of last volume failure
     * estimatedCapacityLost - estimate of total capacity lost due to volume failures
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.VolumeFailureSummaryProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.VolumeFailureSummaryProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_VolumeFailureSummaryProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_VolumeFailureSummaryProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        failedStorageLocations_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        lastVolumeFailureDate_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        estimatedCapacityLostTotal_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_VolumeFailureSummaryProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((bitField0_ & 0x00000001) != 0)) {
          failedStorageLocations_ = failedStorageLocations_.getUnmodifiableView();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.failedStorageLocations_ = failedStorageLocations_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.lastVolumeFailureDate_ = lastVolumeFailureDate_;
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.estimatedCapacityLostTotal_ = estimatedCapacityLostTotal_;
          to_bitField0_ |= 0x00000002;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.getDefaultInstance()) return this;
        if (!other.failedStorageLocations_.isEmpty()) {
          if (failedStorageLocations_.isEmpty()) {
            failedStorageLocations_ = other.failedStorageLocations_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureFailedStorageLocationsIsMutable();
            failedStorageLocations_.addAll(other.failedStorageLocations_);
          }
          onChanged();
        }
        if (other.hasLastVolumeFailureDate()) {
          setLastVolumeFailureDate(other.getLastVolumeFailureDate());
        }
        if (other.hasEstimatedCapacityLostTotal()) {
          setEstimatedCapacityLostTotal(other.getEstimatedCapacityLostTotal());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasLastVolumeFailureDate()) {
          return false;
        }
        if (!hasEstimatedCapacityLostTotal()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.LazyStringList failedStorageLocations_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureFailedStorageLocationsIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          failedStorageLocations_ = new com.google.protobuf.LazyStringArrayList(failedStorageLocations_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <code>repeated string failedStorageLocations = 1;</code>
       * @return A list containing the failedStorageLocations.
       */
      public com.google.protobuf.ProtocolStringList
          getFailedStorageLocationsList() {
        return failedStorageLocations_.getUnmodifiableView();
      }
      /**
       * <code>repeated string failedStorageLocations = 1;</code>
       * @return The count of failedStorageLocations.
       */
      public int getFailedStorageLocationsCount() {
        return failedStorageLocations_.size();
      }
      /**
       * <code>repeated string failedStorageLocations = 1;</code>
       * @param index The index of the element to return.
       * @return The failedStorageLocations at the given index.
       */
      public java.lang.String getFailedStorageLocations(int index) {
        return failedStorageLocations_.get(index);
      }
      /**
       * <code>repeated string failedStorageLocations = 1;</code>
       * @param index The index of the value to return.
       * @return The bytes of the failedStorageLocations at the given index.
       */
      public com.google.protobuf.ByteString
          getFailedStorageLocationsBytes(int index) {
        return failedStorageLocations_.getByteString(index);
      }
      /**
       * <code>repeated string failedStorageLocations = 1;</code>
       * @param index The index to set the value at.
       * @param value The failedStorageLocations to set.
       * @return This builder for chaining.
       */
      public Builder setFailedStorageLocations(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureFailedStorageLocationsIsMutable();
        failedStorageLocations_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string failedStorageLocations = 1;</code>
       * @param value The failedStorageLocations to add.
       * @return This builder for chaining.
       */
      public Builder addFailedStorageLocations(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureFailedStorageLocationsIsMutable();
        failedStorageLocations_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string failedStorageLocations = 1;</code>
       * @param values The failedStorageLocations to add.
       * @return This builder for chaining.
       */
      public Builder addAllFailedStorageLocations(
          java.lang.Iterable<java.lang.String> values) {
        ensureFailedStorageLocationsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, failedStorageLocations_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string failedStorageLocations = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearFailedStorageLocations() {
        failedStorageLocations_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string failedStorageLocations = 1;</code>
       * @param value The bytes of the failedStorageLocations to add.
       * @return This builder for chaining.
       */
      public Builder addFailedStorageLocationsBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureFailedStorageLocationsIsMutable();
        failedStorageLocations_.add(value);
        onChanged();
        return this;
      }

      private long lastVolumeFailureDate_ ;
      /**
       * <code>required uint64 lastVolumeFailureDate = 2;</code>
       * @return Whether the lastVolumeFailureDate field is set.
       */
      @java.lang.Override
      public boolean hasLastVolumeFailureDate() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>required uint64 lastVolumeFailureDate = 2;</code>
       * @return The lastVolumeFailureDate.
       */
      @java.lang.Override
      public long getLastVolumeFailureDate() {
        return lastVolumeFailureDate_;
      }
      /**
       * <code>required uint64 lastVolumeFailureDate = 2;</code>
       * @param value The lastVolumeFailureDate to set.
       * @return This builder for chaining.
       */
      public Builder setLastVolumeFailureDate(long value) {
        bitField0_ |= 0x00000002;
        lastVolumeFailureDate_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 lastVolumeFailureDate = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearLastVolumeFailureDate() {
        bitField0_ = (bitField0_ & ~0x00000002);
        lastVolumeFailureDate_ = 0L;
        onChanged();
        return this;
      }

      private long estimatedCapacityLostTotal_ ;
      /**
       * <code>required uint64 estimatedCapacityLostTotal = 3;</code>
       * @return Whether the estimatedCapacityLostTotal field is set.
       */
      @java.lang.Override
      public boolean hasEstimatedCapacityLostTotal() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>required uint64 estimatedCapacityLostTotal = 3;</code>
       * @return The estimatedCapacityLostTotal.
       */
      @java.lang.Override
      public long getEstimatedCapacityLostTotal() {
        return estimatedCapacityLostTotal_;
      }
      /**
       * <code>required uint64 estimatedCapacityLostTotal = 3;</code>
       * @param value The estimatedCapacityLostTotal to set.
       * @return This builder for chaining.
       */
      public Builder setEstimatedCapacityLostTotal(long value) {
        bitField0_ |= 0x00000004;
        estimatedCapacityLostTotal_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 estimatedCapacityLostTotal = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearEstimatedCapacityLostTotal() {
        bitField0_ = (bitField0_ & ~0x00000004);
        estimatedCapacityLostTotal_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.VolumeFailureSummaryProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.VolumeFailureSummaryProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<VolumeFailureSummaryProto>
        PARSER = new com.google.protobuf.AbstractParser<VolumeFailureSummaryProto>() {
      @java.lang.Override
      public VolumeFailureSummaryProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new VolumeFailureSummaryProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<VolumeFailureSummaryProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<VolumeFailureSummaryProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface HeartbeatRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.HeartbeatRequestProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * Datanode info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return Whether the registration field is set.
     */
    boolean hasRegistration();
    /**
     * <pre>
     * Datanode info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return The registration.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration();
    /**
     * <pre>
     * Datanode info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder();

    /**
     * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto>
        getReportsList();
    /**
     * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto getReports(int index);
    /**
     * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
     */
    int getReportsCount();
    /**
     * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProtoOrBuilder>
        getReportsOrBuilderList();
    /**
     * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProtoOrBuilder getReportsOrBuilder(
        int index);

    /**
     * <code>optional uint32 xmitsInProgress = 3 [default = 0];</code>
     * @return Whether the xmitsInProgress field is set.
     */
    boolean hasXmitsInProgress();
    /**
     * <code>optional uint32 xmitsInProgress = 3 [default = 0];</code>
     * @return The xmitsInProgress.
     */
    int getXmitsInProgress();

    /**
     * <code>optional uint32 xceiverCount = 4 [default = 0];</code>
     * @return Whether the xceiverCount field is set.
     */
    boolean hasXceiverCount();
    /**
     * <code>optional uint32 xceiverCount = 4 [default = 0];</code>
     * @return The xceiverCount.
     */
    int getXceiverCount();

    /**
     * <code>optional uint32 failedVolumes = 5 [default = 0];</code>
     * @return Whether the failedVolumes field is set.
     */
    boolean hasFailedVolumes();
    /**
     * <code>optional uint32 failedVolumes = 5 [default = 0];</code>
     * @return The failedVolumes.
     */
    int getFailedVolumes();

    /**
     * <code>optional uint64 cacheCapacity = 6 [default = 0];</code>
     * @return Whether the cacheCapacity field is set.
     */
    boolean hasCacheCapacity();
    /**
     * <code>optional uint64 cacheCapacity = 6 [default = 0];</code>
     * @return The cacheCapacity.
     */
    long getCacheCapacity();

    /**
     * <code>optional uint64 cacheUsed = 7 [default = 0];</code>
     * @return Whether the cacheUsed field is set.
     */
    boolean hasCacheUsed();
    /**
     * <code>optional uint64 cacheUsed = 7 [default = 0];</code>
     * @return The cacheUsed.
     */
    long getCacheUsed();

    /**
     * <code>optional .org.apache.hadoop.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;</code>
     * @return Whether the volumeFailureSummary field is set.
     */
    boolean hasVolumeFailureSummary();
    /**
     * <code>optional .org.apache.hadoop.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;</code>
     * @return The volumeFailureSummary.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto getVolumeFailureSummary();
    /**
     * <code>optional .org.apache.hadoop.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProtoOrBuilder getVolumeFailureSummaryOrBuilder();
  }
  /**
   * <pre>
   **
   * registration - datanode registration information
   * capacity - total storage capacity available at the datanode
   * dfsUsed - storage used by HDFS
   * remaining - remaining storage available for HDFS
   * blockPoolUsed - storage used by the block pool
   * xmitsInProgress - number of transfers from this datanode to others
   * xceiverCount - number of active transceiver threads
   * failedVolumes - number of failed volumes.  This is redundant with the
   *     information included in volumeFailureSummary, but the field is retained
   *     for backwards compatibility.
   * cacheCapacity - total cache capacity available at the datanode
   * cacheUsed - amount of cache used
   * volumeFailureSummary - info about volume failures
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.HeartbeatRequestProto}
   */
  public static final class HeartbeatRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.HeartbeatRequestProto)
      HeartbeatRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use HeartbeatRequestProto.newBuilder() to construct.
    private HeartbeatRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private HeartbeatRequestProto() {
      reports_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new HeartbeatRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private HeartbeatRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = registration_.toBuilder();
              }
              registration_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(registration_);
                registration_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                reports_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              reports_.add(
                  input.readMessage(org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto.PARSER, extensionRegistry));
              break;
            }
            case 24: {
              bitField0_ |= 0x00000002;
              xmitsInProgress_ = input.readUInt32();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000004;
              xceiverCount_ = input.readUInt32();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000008;
              failedVolumes_ = input.readUInt32();
              break;
            }
            case 48: {
              bitField0_ |= 0x00000010;
              cacheCapacity_ = input.readUInt64();
              break;
            }
            case 56: {
              bitField0_ |= 0x00000020;
              cacheUsed_ = input.readUInt64();
              break;
            }
            case 66: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000040) != 0)) {
                subBuilder = volumeFailureSummary_.toBuilder();
              }
              volumeFailureSummary_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(volumeFailureSummary_);
                volumeFailureSummary_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000040;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          reports_ = java.util.Collections.unmodifiableList(reports_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int REGISTRATION_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto registration_;
    /**
     * <pre>
     * Datanode info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return Whether the registration field is set.
     */
    @java.lang.Override
    public boolean hasRegistration() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <pre>
     * Datanode info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return The registration.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration() {
      return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
    }
    /**
     * <pre>
     * Datanode info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder() {
      return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
    }

    public static final int REPORTS_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto> reports_;
    /**
     * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto> getReportsList() {
      return reports_;
    }
    /**
     * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProtoOrBuilder>
        getReportsOrBuilderList() {
      return reports_;
    }
    /**
     * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
     */
    @java.lang.Override
    public int getReportsCount() {
      return reports_.size();
    }
    /**
     * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto getReports(int index) {
      return reports_.get(index);
    }
    /**
     * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProtoOrBuilder getReportsOrBuilder(
        int index) {
      return reports_.get(index);
    }

    public static final int XMITSINPROGRESS_FIELD_NUMBER = 3;
    private int xmitsInProgress_;
    /**
     * <code>optional uint32 xmitsInProgress = 3 [default = 0];</code>
     * @return Whether the xmitsInProgress field is set.
     */
    @java.lang.Override
    public boolean hasXmitsInProgress() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional uint32 xmitsInProgress = 3 [default = 0];</code>
     * @return The xmitsInProgress.
     */
    @java.lang.Override
    public int getXmitsInProgress() {
      return xmitsInProgress_;
    }

    public static final int XCEIVERCOUNT_FIELD_NUMBER = 4;
    private int xceiverCount_;
    /**
     * <code>optional uint32 xceiverCount = 4 [default = 0];</code>
     * @return Whether the xceiverCount field is set.
     */
    @java.lang.Override
    public boolean hasXceiverCount() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional uint32 xceiverCount = 4 [default = 0];</code>
     * @return The xceiverCount.
     */
    @java.lang.Override
    public int getXceiverCount() {
      return xceiverCount_;
    }

    public static final int FAILEDVOLUMES_FIELD_NUMBER = 5;
    private int failedVolumes_;
    /**
     * <code>optional uint32 failedVolumes = 5 [default = 0];</code>
     * @return Whether the failedVolumes field is set.
     */
    @java.lang.Override
    public boolean hasFailedVolumes() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional uint32 failedVolumes = 5 [default = 0];</code>
     * @return The failedVolumes.
     */
    @java.lang.Override
    public int getFailedVolumes() {
      return failedVolumes_;
    }

    public static final int CACHECAPACITY_FIELD_NUMBER = 6;
    private long cacheCapacity_;
    /**
     * <code>optional uint64 cacheCapacity = 6 [default = 0];</code>
     * @return Whether the cacheCapacity field is set.
     */
    @java.lang.Override
    public boolean hasCacheCapacity() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>optional uint64 cacheCapacity = 6 [default = 0];</code>
     * @return The cacheCapacity.
     */
    @java.lang.Override
    public long getCacheCapacity() {
      return cacheCapacity_;
    }

    public static final int CACHEUSED_FIELD_NUMBER = 7;
    private long cacheUsed_;
    /**
     * <code>optional uint64 cacheUsed = 7 [default = 0];</code>
     * @return Whether the cacheUsed field is set.
     */
    @java.lang.Override
    public boolean hasCacheUsed() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>optional uint64 cacheUsed = 7 [default = 0];</code>
     * @return The cacheUsed.
     */
    @java.lang.Override
    public long getCacheUsed() {
      return cacheUsed_;
    }

    public static final int VOLUMEFAILURESUMMARY_FIELD_NUMBER = 8;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto volumeFailureSummary_;
    /**
     * <code>optional .org.apache.hadoop.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;</code>
     * @return Whether the volumeFailureSummary field is set.
     */
    @java.lang.Override
    public boolean hasVolumeFailureSummary() {
      return ((bitField0_ & 0x00000040) != 0);
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;</code>
     * @return The volumeFailureSummary.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto getVolumeFailureSummary() {
      return volumeFailureSummary_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.getDefaultInstance() : volumeFailureSummary_;
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProtoOrBuilder getVolumeFailureSummaryOrBuilder() {
      return volumeFailureSummary_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.getDefaultInstance() : volumeFailureSummary_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasRegistration()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getRegistration().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      for (int i = 0; i < getReportsCount(); i++) {
        if (!getReports(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasVolumeFailureSummary()) {
        if (!getVolumeFailureSummary().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getRegistration());
      }
      for (int i = 0; i < reports_.size(); i++) {
        output.writeMessage(2, reports_.get(i));
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeUInt32(3, xmitsInProgress_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeUInt32(4, xceiverCount_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeUInt32(5, failedVolumes_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        output.writeUInt64(6, cacheCapacity_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        output.writeUInt64(7, cacheUsed_);
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        output.writeMessage(8, getVolumeFailureSummary());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getRegistration());
      }
      for (int i = 0; i < reports_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, reports_.get(i));
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(3, xmitsInProgress_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(4, xceiverCount_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(5, failedVolumes_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(6, cacheCapacity_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(7, cacheUsed_);
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(8, getVolumeFailureSummary());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto) obj;

      if (hasRegistration() != other.hasRegistration()) return false;
      if (hasRegistration()) {
        if (!getRegistration()
            .equals(other.getRegistration())) return false;
      }
      if (!getReportsList()
          .equals(other.getReportsList())) return false;
      if (hasXmitsInProgress() != other.hasXmitsInProgress()) return false;
      if (hasXmitsInProgress()) {
        if (getXmitsInProgress()
            != other.getXmitsInProgress()) return false;
      }
      if (hasXceiverCount() != other.hasXceiverCount()) return false;
      if (hasXceiverCount()) {
        if (getXceiverCount()
            != other.getXceiverCount()) return false;
      }
      if (hasFailedVolumes() != other.hasFailedVolumes()) return false;
      if (hasFailedVolumes()) {
        if (getFailedVolumes()
            != other.getFailedVolumes()) return false;
      }
      if (hasCacheCapacity() != other.hasCacheCapacity()) return false;
      if (hasCacheCapacity()) {
        if (getCacheCapacity()
            != other.getCacheCapacity()) return false;
      }
      if (hasCacheUsed() != other.hasCacheUsed()) return false;
      if (hasCacheUsed()) {
        if (getCacheUsed()
            != other.getCacheUsed()) return false;
      }
      if (hasVolumeFailureSummary() != other.hasVolumeFailureSummary()) return false;
      if (hasVolumeFailureSummary()) {
        if (!getVolumeFailureSummary()
            .equals(other.getVolumeFailureSummary())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasRegistration()) {
        hash = (37 * hash) + REGISTRATION_FIELD_NUMBER;
        hash = (53 * hash) + getRegistration().hashCode();
      }
      if (getReportsCount() > 0) {
        hash = (37 * hash) + REPORTS_FIELD_NUMBER;
        hash = (53 * hash) + getReportsList().hashCode();
      }
      if (hasXmitsInProgress()) {
        hash = (37 * hash) + XMITSINPROGRESS_FIELD_NUMBER;
        hash = (53 * hash) + getXmitsInProgress();
      }
      if (hasXceiverCount()) {
        hash = (37 * hash) + XCEIVERCOUNT_FIELD_NUMBER;
        hash = (53 * hash) + getXceiverCount();
      }
      if (hasFailedVolumes()) {
        hash = (37 * hash) + FAILEDVOLUMES_FIELD_NUMBER;
        hash = (53 * hash) + getFailedVolumes();
      }
      if (hasCacheCapacity()) {
        hash = (37 * hash) + CACHECAPACITY_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getCacheCapacity());
      }
      if (hasCacheUsed()) {
        hash = (37 * hash) + CACHEUSED_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getCacheUsed());
      }
      if (hasVolumeFailureSummary()) {
        hash = (37 * hash) + VOLUMEFAILURESUMMARY_FIELD_NUMBER;
        hash = (53 * hash) + getVolumeFailureSummary().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * registration - datanode registration information
     * capacity - total storage capacity available at the datanode
     * dfsUsed - storage used by HDFS
     * remaining - remaining storage available for HDFS
     * blockPoolUsed - storage used by the block pool
     * xmitsInProgress - number of transfers from this datanode to others
     * xceiverCount - number of active transceiver threads
     * failedVolumes - number of failed volumes.  This is redundant with the
     *     information included in volumeFailureSummary, but the field is retained
     *     for backwards compatibility.
     * cacheCapacity - total cache capacity available at the datanode
     * cacheUsed - amount of cache used
     * volumeFailureSummary - info about volume failures
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.HeartbeatRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.HeartbeatRequestProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getRegistrationFieldBuilder();
          getReportsFieldBuilder();
          getVolumeFailureSummaryFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (registrationBuilder_ == null) {
          registration_ = null;
        } else {
          registrationBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (reportsBuilder_ == null) {
          reports_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          reportsBuilder_.clear();
        }
        xmitsInProgress_ = 0;
        bitField0_ = (bitField0_ & ~0x00000004);
        xceiverCount_ = 0;
        bitField0_ = (bitField0_ & ~0x00000008);
        failedVolumes_ = 0;
        bitField0_ = (bitField0_ & ~0x00000010);
        cacheCapacity_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000020);
        cacheUsed_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000040);
        if (volumeFailureSummaryBuilder_ == null) {
          volumeFailureSummary_ = null;
        } else {
          volumeFailureSummaryBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (registrationBuilder_ == null) {
            result.registration_ = registration_;
          } else {
            result.registration_ = registrationBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (reportsBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0)) {
            reports_ = java.util.Collections.unmodifiableList(reports_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.reports_ = reports_;
        } else {
          result.reports_ = reportsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.xmitsInProgress_ = xmitsInProgress_;
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          result.xceiverCount_ = xceiverCount_;
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000010) != 0)) {
          result.failedVolumes_ = failedVolumes_;
          to_bitField0_ |= 0x00000008;
        }
        if (((from_bitField0_ & 0x00000020) != 0)) {
          result.cacheCapacity_ = cacheCapacity_;
          to_bitField0_ |= 0x00000010;
        }
        if (((from_bitField0_ & 0x00000040) != 0)) {
          result.cacheUsed_ = cacheUsed_;
          to_bitField0_ |= 0x00000020;
        }
        if (((from_bitField0_ & 0x00000080) != 0)) {
          if (volumeFailureSummaryBuilder_ == null) {
            result.volumeFailureSummary_ = volumeFailureSummary_;
          } else {
            result.volumeFailureSummary_ = volumeFailureSummaryBuilder_.build();
          }
          to_bitField0_ |= 0x00000040;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto.getDefaultInstance()) return this;
        if (other.hasRegistration()) {
          mergeRegistration(other.getRegistration());
        }
        if (reportsBuilder_ == null) {
          if (!other.reports_.isEmpty()) {
            if (reports_.isEmpty()) {
              reports_ = other.reports_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureReportsIsMutable();
              reports_.addAll(other.reports_);
            }
            onChanged();
          }
        } else {
          if (!other.reports_.isEmpty()) {
            if (reportsBuilder_.isEmpty()) {
              reportsBuilder_.dispose();
              reportsBuilder_ = null;
              reports_ = other.reports_;
              bitField0_ = (bitField0_ & ~0x00000002);
              reportsBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getReportsFieldBuilder() : null;
            } else {
              reportsBuilder_.addAllMessages(other.reports_);
            }
          }
        }
        if (other.hasXmitsInProgress()) {
          setXmitsInProgress(other.getXmitsInProgress());
        }
        if (other.hasXceiverCount()) {
          setXceiverCount(other.getXceiverCount());
        }
        if (other.hasFailedVolumes()) {
          setFailedVolumes(other.getFailedVolumes());
        }
        if (other.hasCacheCapacity()) {
          setCacheCapacity(other.getCacheCapacity());
        }
        if (other.hasCacheUsed()) {
          setCacheUsed(other.getCacheUsed());
        }
        if (other.hasVolumeFailureSummary()) {
          mergeVolumeFailureSummary(other.getVolumeFailureSummary());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasRegistration()) {
          return false;
        }
        if (!getRegistration().isInitialized()) {
          return false;
        }
        for (int i = 0; i < getReportsCount(); i++) {
          if (!getReports(i).isInitialized()) {
            return false;
          }
        }
        if (hasVolumeFailureSummary()) {
          if (!getVolumeFailureSummary().isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto registration_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder> registrationBuilder_;
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       * @return Whether the registration field is set.
       */
      public boolean hasRegistration() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       * @return The registration.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration() {
        if (registrationBuilder_ == null) {
          return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
        } else {
          return registrationBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder setRegistration(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto value) {
        if (registrationBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          registration_ = value;
          onChanged();
        } else {
          registrationBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder setRegistration(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder builderForValue) {
        if (registrationBuilder_ == null) {
          registration_ = builderForValue.build();
          onChanged();
        } else {
          registrationBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder mergeRegistration(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto value) {
        if (registrationBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              registration_ != null &&
              registration_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance()) {
            registration_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.newBuilder(registration_).mergeFrom(value).buildPartial();
          } else {
            registration_ = value;
          }
          onChanged();
        } else {
          registrationBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder clearRegistration() {
        if (registrationBuilder_ == null) {
          registration_ = null;
          onChanged();
        } else {
          registrationBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder getRegistrationBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getRegistrationFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder() {
        if (registrationBuilder_ != null) {
          return registrationBuilder_.getMessageOrBuilder();
        } else {
          return registration_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
        }
      }
      /**
       * <pre>
       * Datanode info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder>
          getRegistrationFieldBuilder() {
        if (registrationBuilder_ == null) {
          registrationBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder>(
                  getRegistration(),
                  getParentForChildren(),
                  isClean());
          registration_ = null;
        }
        return registrationBuilder_;
      }

      private java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto> reports_ =
        java.util.Collections.emptyList();
      private void ensureReportsIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          reports_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto>(reports_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProtoOrBuilder> reportsBuilder_;

      /**
       * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto> getReportsList() {
        if (reportsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(reports_);
        } else {
          return reportsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
       */
      public int getReportsCount() {
        if (reportsBuilder_ == null) {
          return reports_.size();
        } else {
          return reportsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto getReports(int index) {
        if (reportsBuilder_ == null) {
          return reports_.get(index);
        } else {
          return reportsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
       */
      public Builder setReports(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto value) {
        if (reportsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureReportsIsMutable();
          reports_.set(index, value);
          onChanged();
        } else {
          reportsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
       */
      public Builder setReports(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto.Builder builderForValue) {
        if (reportsBuilder_ == null) {
          ensureReportsIsMutable();
          reports_.set(index, builderForValue.build());
          onChanged();
        } else {
          reportsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
       */
      public Builder addReports(org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto value) {
        if (reportsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureReportsIsMutable();
          reports_.add(value);
          onChanged();
        } else {
          reportsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
       */
      public Builder addReports(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto value) {
        if (reportsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureReportsIsMutable();
          reports_.add(index, value);
          onChanged();
        } else {
          reportsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
       */
      public Builder addReports(
          org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto.Builder builderForValue) {
        if (reportsBuilder_ == null) {
          ensureReportsIsMutable();
          reports_.add(builderForValue.build());
          onChanged();
        } else {
          reportsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
       */
      public Builder addReports(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto.Builder builderForValue) {
        if (reportsBuilder_ == null) {
          ensureReportsIsMutable();
          reports_.add(index, builderForValue.build());
          onChanged();
        } else {
          reportsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
       */
      public Builder addAllReports(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto> values) {
        if (reportsBuilder_ == null) {
          ensureReportsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, reports_);
          onChanged();
        } else {
          reportsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
       */
      public Builder clearReports() {
        if (reportsBuilder_ == null) {
          reports_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          reportsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
       */
      public Builder removeReports(int index) {
        if (reportsBuilder_ == null) {
          ensureReportsIsMutable();
          reports_.remove(index);
          onChanged();
        } else {
          reportsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto.Builder getReportsBuilder(
          int index) {
        return getReportsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProtoOrBuilder getReportsOrBuilder(
          int index) {
        if (reportsBuilder_ == null) {
          return reports_.get(index);  } else {
          return reportsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProtoOrBuilder>
           getReportsOrBuilderList() {
        if (reportsBuilder_ != null) {
          return reportsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(reports_);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto.Builder addReportsBuilder() {
        return getReportsFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto.Builder addReportsBuilder(
          int index) {
        return getReportsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.StorageReportProto reports = 2;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto.Builder>
           getReportsBuilderList() {
        return getReportsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProtoOrBuilder>
          getReportsFieldBuilder() {
        if (reportsBuilder_ == null) {
          reportsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.StorageReportProtoOrBuilder>(
                  reports_,
                  ((bitField0_ & 0x00000002) != 0),
                  getParentForChildren(),
                  isClean());
          reports_ = null;
        }
        return reportsBuilder_;
      }

      private int xmitsInProgress_ ;
      /**
       * <code>optional uint32 xmitsInProgress = 3 [default = 0];</code>
       * @return Whether the xmitsInProgress field is set.
       */
      @java.lang.Override
      public boolean hasXmitsInProgress() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional uint32 xmitsInProgress = 3 [default = 0];</code>
       * @return The xmitsInProgress.
       */
      @java.lang.Override
      public int getXmitsInProgress() {
        return xmitsInProgress_;
      }
      /**
       * <code>optional uint32 xmitsInProgress = 3 [default = 0];</code>
       * @param value The xmitsInProgress to set.
       * @return This builder for chaining.
       */
      public Builder setXmitsInProgress(int value) {
        bitField0_ |= 0x00000004;
        xmitsInProgress_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 xmitsInProgress = 3 [default = 0];</code>
       * @return This builder for chaining.
       */
      public Builder clearXmitsInProgress() {
        bitField0_ = (bitField0_ & ~0x00000004);
        xmitsInProgress_ = 0;
        onChanged();
        return this;
      }

      private int xceiverCount_ ;
      /**
       * <code>optional uint32 xceiverCount = 4 [default = 0];</code>
       * @return Whether the xceiverCount field is set.
       */
      @java.lang.Override
      public boolean hasXceiverCount() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional uint32 xceiverCount = 4 [default = 0];</code>
       * @return The xceiverCount.
       */
      @java.lang.Override
      public int getXceiverCount() {
        return xceiverCount_;
      }
      /**
       * <code>optional uint32 xceiverCount = 4 [default = 0];</code>
       * @param value The xceiverCount to set.
       * @return This builder for chaining.
       */
      public Builder setXceiverCount(int value) {
        bitField0_ |= 0x00000008;
        xceiverCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 xceiverCount = 4 [default = 0];</code>
       * @return This builder for chaining.
       */
      public Builder clearXceiverCount() {
        bitField0_ = (bitField0_ & ~0x00000008);
        xceiverCount_ = 0;
        onChanged();
        return this;
      }

      private int failedVolumes_ ;
      /**
       * <code>optional uint32 failedVolumes = 5 [default = 0];</code>
       * @return Whether the failedVolumes field is set.
       */
      @java.lang.Override
      public boolean hasFailedVolumes() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>optional uint32 failedVolumes = 5 [default = 0];</code>
       * @return The failedVolumes.
       */
      @java.lang.Override
      public int getFailedVolumes() {
        return failedVolumes_;
      }
      /**
       * <code>optional uint32 failedVolumes = 5 [default = 0];</code>
       * @param value The failedVolumes to set.
       * @return This builder for chaining.
       */
      public Builder setFailedVolumes(int value) {
        bitField0_ |= 0x00000010;
        failedVolumes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 failedVolumes = 5 [default = 0];</code>
       * @return This builder for chaining.
       */
      public Builder clearFailedVolumes() {
        bitField0_ = (bitField0_ & ~0x00000010);
        failedVolumes_ = 0;
        onChanged();
        return this;
      }

      private long cacheCapacity_ ;
      /**
       * <code>optional uint64 cacheCapacity = 6 [default = 0];</code>
       * @return Whether the cacheCapacity field is set.
       */
      @java.lang.Override
      public boolean hasCacheCapacity() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>optional uint64 cacheCapacity = 6 [default = 0];</code>
       * @return The cacheCapacity.
       */
      @java.lang.Override
      public long getCacheCapacity() {
        return cacheCapacity_;
      }
      /**
       * <code>optional uint64 cacheCapacity = 6 [default = 0];</code>
       * @param value The cacheCapacity to set.
       * @return This builder for chaining.
       */
      public Builder setCacheCapacity(long value) {
        bitField0_ |= 0x00000020;
        cacheCapacity_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 cacheCapacity = 6 [default = 0];</code>
       * @return This builder for chaining.
       */
      public Builder clearCacheCapacity() {
        bitField0_ = (bitField0_ & ~0x00000020);
        cacheCapacity_ = 0L;
        onChanged();
        return this;
      }

      private long cacheUsed_ ;
      /**
       * <code>optional uint64 cacheUsed = 7 [default = 0];</code>
       * @return Whether the cacheUsed field is set.
       */
      @java.lang.Override
      public boolean hasCacheUsed() {
        return ((bitField0_ & 0x00000040) != 0);
      }
      /**
       * <code>optional uint64 cacheUsed = 7 [default = 0];</code>
       * @return The cacheUsed.
       */
      @java.lang.Override
      public long getCacheUsed() {
        return cacheUsed_;
      }
      /**
       * <code>optional uint64 cacheUsed = 7 [default = 0];</code>
       * @param value The cacheUsed to set.
       * @return This builder for chaining.
       */
      public Builder setCacheUsed(long value) {
        bitField0_ |= 0x00000040;
        cacheUsed_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 cacheUsed = 7 [default = 0];</code>
       * @return This builder for chaining.
       */
      public Builder clearCacheUsed() {
        bitField0_ = (bitField0_ & ~0x00000040);
        cacheUsed_ = 0L;
        onChanged();
        return this;
      }

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto volumeFailureSummary_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProtoOrBuilder> volumeFailureSummaryBuilder_;
      /**
       * <code>optional .org.apache.hadoop.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;</code>
       * @return Whether the volumeFailureSummary field is set.
       */
      public boolean hasVolumeFailureSummary() {
        return ((bitField0_ & 0x00000080) != 0);
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;</code>
       * @return The volumeFailureSummary.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto getVolumeFailureSummary() {
        if (volumeFailureSummaryBuilder_ == null) {
          return volumeFailureSummary_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.getDefaultInstance() : volumeFailureSummary_;
        } else {
          return volumeFailureSummaryBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;</code>
       */
      public Builder setVolumeFailureSummary(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto value) {
        if (volumeFailureSummaryBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          volumeFailureSummary_ = value;
          onChanged();
        } else {
          volumeFailureSummaryBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;</code>
       */
      public Builder setVolumeFailureSummary(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.Builder builderForValue) {
        if (volumeFailureSummaryBuilder_ == null) {
          volumeFailureSummary_ = builderForValue.build();
          onChanged();
        } else {
          volumeFailureSummaryBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;</code>
       */
      public Builder mergeVolumeFailureSummary(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto value) {
        if (volumeFailureSummaryBuilder_ == null) {
          if (((bitField0_ & 0x00000080) != 0) &&
              volumeFailureSummary_ != null &&
              volumeFailureSummary_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.getDefaultInstance()) {
            volumeFailureSummary_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.newBuilder(volumeFailureSummary_).mergeFrom(value).buildPartial();
          } else {
            volumeFailureSummary_ = value;
          }
          onChanged();
        } else {
          volumeFailureSummaryBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;</code>
       */
      public Builder clearVolumeFailureSummary() {
        if (volumeFailureSummaryBuilder_ == null) {
          volumeFailureSummary_ = null;
          onChanged();
        } else {
          volumeFailureSummaryBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.Builder getVolumeFailureSummaryBuilder() {
        bitField0_ |= 0x00000080;
        onChanged();
        return getVolumeFailureSummaryFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProtoOrBuilder getVolumeFailureSummaryOrBuilder() {
        if (volumeFailureSummaryBuilder_ != null) {
          return volumeFailureSummaryBuilder_.getMessageOrBuilder();
        } else {
          return volumeFailureSummary_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.getDefaultInstance() : volumeFailureSummary_;
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProtoOrBuilder>
          getVolumeFailureSummaryFieldBuilder() {
        if (volumeFailureSummaryBuilder_ == null) {
          volumeFailureSummaryBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.VolumeFailureSummaryProtoOrBuilder>(
                  getVolumeFailureSummary(),
                  getParentForChildren(),
                  isClean());
          volumeFailureSummary_ = null;
        }
        return volumeFailureSummaryBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.HeartbeatRequestProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.HeartbeatRequestProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<HeartbeatRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<HeartbeatRequestProto>() {
      @java.lang.Override
      public HeartbeatRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new HeartbeatRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<HeartbeatRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<HeartbeatRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface HeartbeatResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.HeartbeatResponseProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * Returned commands can be null
     * </pre>
     *
     * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto>
        getCmdsList();
    /**
     * <pre>
     * Returned commands can be null
     * </pre>
     *
     * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto getCmds(int index);
    /**
     * <pre>
     * Returned commands can be null
     * </pre>
     *
     * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
     */
    int getCmdsCount();
    /**
     * <pre>
     * Returned commands can be null
     * </pre>
     *
     * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder>
        getCmdsOrBuilderList();
    /**
     * <pre>
     * Returned commands can be null
     * </pre>
     *
     * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder getCmdsOrBuilder(
        int index);

    /**
     * <code>optional .org.apache.hadoop.RollingUpgradeStatusProto rollingUpgradeStatus = 2;</code>
     * @return Whether the rollingUpgradeStatus field is set.
     */
    boolean hasRollingUpgradeStatus();
    /**
     * <code>optional .org.apache.hadoop.RollingUpgradeStatusProto rollingUpgradeStatus = 2;</code>
     * @return The rollingUpgradeStatus.
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto getRollingUpgradeStatus();
    /**
     * <code>optional .org.apache.hadoop.RollingUpgradeStatusProto rollingUpgradeStatus = 2;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProtoOrBuilder getRollingUpgradeStatusOrBuilder();
  }
  /**
   * <pre>
   **
   * cmds - Commands from namenode to datanode.
   * haStatus - Status (from an HA perspective) of the NN sending this response
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.HeartbeatResponseProto}
   */
  public static final class HeartbeatResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.HeartbeatResponseProto)
      HeartbeatResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use HeartbeatResponseProto.newBuilder() to construct.
    private HeartbeatResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private HeartbeatResponseProto() {
      cmds_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new HeartbeatResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private HeartbeatResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                cmds_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              cmds_.add(
                  input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.PARSER, extensionRegistry));
              break;
            }
            case 18: {
              org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = rollingUpgradeStatus_.toBuilder();
              }
              rollingUpgradeStatus_ = input.readMessage(org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(rollingUpgradeStatus_);
                rollingUpgradeStatus_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          cmds_ = java.util.Collections.unmodifiableList(cmds_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto.Builder.class);
    }

    private int bitField0_;
    public static final int CMDS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto> cmds_;
    /**
     * <pre>
     * Returned commands can be null
     * </pre>
     *
     * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto> getCmdsList() {
      return cmds_;
    }
    /**
     * <pre>
     * Returned commands can be null
     * </pre>
     *
     * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder>
        getCmdsOrBuilderList() {
      return cmds_;
    }
    /**
     * <pre>
     * Returned commands can be null
     * </pre>
     *
     * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
     */
    @java.lang.Override
    public int getCmdsCount() {
      return cmds_.size();
    }
    /**
     * <pre>
     * Returned commands can be null
     * </pre>
     *
     * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto getCmds(int index) {
      return cmds_.get(index);
    }
    /**
     * <pre>
     * Returned commands can be null
     * </pre>
     *
     * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder getCmdsOrBuilder(
        int index) {
      return cmds_.get(index);
    }

    public static final int ROLLINGUPGRADESTATUS_FIELD_NUMBER = 2;
    private org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto rollingUpgradeStatus_;
    /**
     * <code>optional .org.apache.hadoop.RollingUpgradeStatusProto rollingUpgradeStatus = 2;</code>
     * @return Whether the rollingUpgradeStatus field is set.
     */
    @java.lang.Override
    public boolean hasRollingUpgradeStatus() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .org.apache.hadoop.RollingUpgradeStatusProto rollingUpgradeStatus = 2;</code>
     * @return The rollingUpgradeStatus.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto getRollingUpgradeStatus() {
      return rollingUpgradeStatus_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto.getDefaultInstance() : rollingUpgradeStatus_;
    }
    /**
     * <code>optional .org.apache.hadoop.RollingUpgradeStatusProto rollingUpgradeStatus = 2;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProtoOrBuilder getRollingUpgradeStatusOrBuilder() {
      return rollingUpgradeStatus_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto.getDefaultInstance() : rollingUpgradeStatus_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      for (int i = 0; i < getCmdsCount(); i++) {
        if (!getCmds(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasRollingUpgradeStatus()) {
        if (!getRollingUpgradeStatus().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < cmds_.size(); i++) {
        output.writeMessage(1, cmds_.get(i));
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(2, getRollingUpgradeStatus());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < cmds_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, cmds_.get(i));
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, getRollingUpgradeStatus());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto) obj;

      if (!getCmdsList()
          .equals(other.getCmdsList())) return false;
      if (hasRollingUpgradeStatus() != other.hasRollingUpgradeStatus()) return false;
      if (hasRollingUpgradeStatus()) {
        if (!getRollingUpgradeStatus()
            .equals(other.getRollingUpgradeStatus())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getCmdsCount() > 0) {
        hash = (37 * hash) + CMDS_FIELD_NUMBER;
        hash = (53 * hash) + getCmdsList().hashCode();
      }
      if (hasRollingUpgradeStatus()) {
        hash = (37 * hash) + ROLLINGUPGRADESTATUS_FIELD_NUMBER;
        hash = (53 * hash) + getRollingUpgradeStatus().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * cmds - Commands from namenode to datanode.
     * haStatus - Status (from an HA perspective) of the NN sending this response
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.HeartbeatResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.HeartbeatResponseProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getCmdsFieldBuilder();
          getRollingUpgradeStatusFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (cmdsBuilder_ == null) {
          cmds_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          cmdsBuilder_.clear();
        }
        if (rollingUpgradeStatusBuilder_ == null) {
          rollingUpgradeStatus_ = null;
        } else {
          rollingUpgradeStatusBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (cmdsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            cmds_ = java.util.Collections.unmodifiableList(cmds_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.cmds_ = cmds_;
        } else {
          result.cmds_ = cmdsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          if (rollingUpgradeStatusBuilder_ == null) {
            result.rollingUpgradeStatus_ = rollingUpgradeStatus_;
          } else {
            result.rollingUpgradeStatus_ = rollingUpgradeStatusBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto.getDefaultInstance()) return this;
        if (cmdsBuilder_ == null) {
          if (!other.cmds_.isEmpty()) {
            if (cmds_.isEmpty()) {
              cmds_ = other.cmds_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureCmdsIsMutable();
              cmds_.addAll(other.cmds_);
            }
            onChanged();
          }
        } else {
          if (!other.cmds_.isEmpty()) {
            if (cmdsBuilder_.isEmpty()) {
              cmdsBuilder_.dispose();
              cmdsBuilder_ = null;
              cmds_ = other.cmds_;
              bitField0_ = (bitField0_ & ~0x00000001);
              cmdsBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getCmdsFieldBuilder() : null;
            } else {
              cmdsBuilder_.addAllMessages(other.cmds_);
            }
          }
        }
        if (other.hasRollingUpgradeStatus()) {
          mergeRollingUpgradeStatus(other.getRollingUpgradeStatus());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        for (int i = 0; i < getCmdsCount(); i++) {
          if (!getCmds(i).isInitialized()) {
            return false;
          }
        }
        if (hasRollingUpgradeStatus()) {
          if (!getRollingUpgradeStatus().isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto> cmds_ =
        java.util.Collections.emptyList();
      private void ensureCmdsIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          cmds_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto>(cmds_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder> cmdsBuilder_;

      /**
       * <pre>
       * Returned commands can be null
       * </pre>
       *
       * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto> getCmdsList() {
        if (cmdsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(cmds_);
        } else {
          return cmdsBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       * Returned commands can be null
       * </pre>
       *
       * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
       */
      public int getCmdsCount() {
        if (cmdsBuilder_ == null) {
          return cmds_.size();
        } else {
          return cmdsBuilder_.getCount();
        }
      }
      /**
       * <pre>
       * Returned commands can be null
       * </pre>
       *
       * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto getCmds(int index) {
        if (cmdsBuilder_ == null) {
          return cmds_.get(index);
        } else {
          return cmdsBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       * Returned commands can be null
       * </pre>
       *
       * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
       */
      public Builder setCmds(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto value) {
        if (cmdsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureCmdsIsMutable();
          cmds_.set(index, value);
          onChanged();
        } else {
          cmdsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       * Returned commands can be null
       * </pre>
       *
       * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
       */
      public Builder setCmds(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder builderForValue) {
        if (cmdsBuilder_ == null) {
          ensureCmdsIsMutable();
          cmds_.set(index, builderForValue.build());
          onChanged();
        } else {
          cmdsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       * Returned commands can be null
       * </pre>
       *
       * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
       */
      public Builder addCmds(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto value) {
        if (cmdsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureCmdsIsMutable();
          cmds_.add(value);
          onChanged();
        } else {
          cmdsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       * Returned commands can be null
       * </pre>
       *
       * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
       */
      public Builder addCmds(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto value) {
        if (cmdsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureCmdsIsMutable();
          cmds_.add(index, value);
          onChanged();
        } else {
          cmdsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       * Returned commands can be null
       * </pre>
       *
       * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
       */
      public Builder addCmds(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder builderForValue) {
        if (cmdsBuilder_ == null) {
          ensureCmdsIsMutable();
          cmds_.add(builderForValue.build());
          onChanged();
        } else {
          cmdsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       * Returned commands can be null
       * </pre>
       *
       * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
       */
      public Builder addCmds(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder builderForValue) {
        if (cmdsBuilder_ == null) {
          ensureCmdsIsMutable();
          cmds_.add(index, builderForValue.build());
          onChanged();
        } else {
          cmdsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       * Returned commands can be null
       * </pre>
       *
       * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
       */
      public Builder addAllCmds(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto> values) {
        if (cmdsBuilder_ == null) {
          ensureCmdsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, cmds_);
          onChanged();
        } else {
          cmdsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       * Returned commands can be null
       * </pre>
       *
       * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
       */
      public Builder clearCmds() {
        if (cmdsBuilder_ == null) {
          cmds_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          cmdsBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       * Returned commands can be null
       * </pre>
       *
       * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
       */
      public Builder removeCmds(int index) {
        if (cmdsBuilder_ == null) {
          ensureCmdsIsMutable();
          cmds_.remove(index);
          onChanged();
        } else {
          cmdsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       * Returned commands can be null
       * </pre>
       *
       * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder getCmdsBuilder(
          int index) {
        return getCmdsFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       * Returned commands can be null
       * </pre>
       *
       * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder getCmdsOrBuilder(
          int index) {
        if (cmdsBuilder_ == null) {
          return cmds_.get(index);  } else {
          return cmdsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       * Returned commands can be null
       * </pre>
       *
       * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder>
           getCmdsOrBuilderList() {
        if (cmdsBuilder_ != null) {
          return cmdsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(cmds_);
        }
      }
      /**
       * <pre>
       * Returned commands can be null
       * </pre>
       *
       * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder addCmdsBuilder() {
        return getCmdsFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.getDefaultInstance());
      }
      /**
       * <pre>
       * Returned commands can be null
       * </pre>
       *
       * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder addCmdsBuilder(
          int index) {
        return getCmdsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.getDefaultInstance());
      }
      /**
       * <pre>
       * Returned commands can be null
       * </pre>
       *
       * <code>repeated .org.apache.hadoop.datanode.DatanodeCommandProto cmds = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder>
           getCmdsBuilderList() {
        return getCmdsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder>
          getCmdsFieldBuilder() {
        if (cmdsBuilder_ == null) {
          cmdsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder>(
                  cmds_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          cmds_ = null;
        }
        return cmdsBuilder_;
      }

      private org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto rollingUpgradeStatus_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProtoOrBuilder> rollingUpgradeStatusBuilder_;
      /**
       * <code>optional .org.apache.hadoop.RollingUpgradeStatusProto rollingUpgradeStatus = 2;</code>
       * @return Whether the rollingUpgradeStatus field is set.
       */
      public boolean hasRollingUpgradeStatus() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional .org.apache.hadoop.RollingUpgradeStatusProto rollingUpgradeStatus = 2;</code>
       * @return The rollingUpgradeStatus.
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto getRollingUpgradeStatus() {
        if (rollingUpgradeStatusBuilder_ == null) {
          return rollingUpgradeStatus_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto.getDefaultInstance() : rollingUpgradeStatus_;
        } else {
          return rollingUpgradeStatusBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .org.apache.hadoop.RollingUpgradeStatusProto rollingUpgradeStatus = 2;</code>
       */
      public Builder setRollingUpgradeStatus(org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto value) {
        if (rollingUpgradeStatusBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          rollingUpgradeStatus_ = value;
          onChanged();
        } else {
          rollingUpgradeStatusBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.RollingUpgradeStatusProto rollingUpgradeStatus = 2;</code>
       */
      public Builder setRollingUpgradeStatus(
          org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto.Builder builderForValue) {
        if (rollingUpgradeStatusBuilder_ == null) {
          rollingUpgradeStatus_ = builderForValue.build();
          onChanged();
        } else {
          rollingUpgradeStatusBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.RollingUpgradeStatusProto rollingUpgradeStatus = 2;</code>
       */
      public Builder mergeRollingUpgradeStatus(org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto value) {
        if (rollingUpgradeStatusBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0) &&
              rollingUpgradeStatus_ != null &&
              rollingUpgradeStatus_ != org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto.getDefaultInstance()) {
            rollingUpgradeStatus_ =
              org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto.newBuilder(rollingUpgradeStatus_).mergeFrom(value).buildPartial();
          } else {
            rollingUpgradeStatus_ = value;
          }
          onChanged();
        } else {
          rollingUpgradeStatusBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.RollingUpgradeStatusProto rollingUpgradeStatus = 2;</code>
       */
      public Builder clearRollingUpgradeStatus() {
        if (rollingUpgradeStatusBuilder_ == null) {
          rollingUpgradeStatus_ = null;
          onChanged();
        } else {
          rollingUpgradeStatusBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.RollingUpgradeStatusProto rollingUpgradeStatus = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto.Builder getRollingUpgradeStatusBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getRollingUpgradeStatusFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .org.apache.hadoop.RollingUpgradeStatusProto rollingUpgradeStatus = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProtoOrBuilder getRollingUpgradeStatusOrBuilder() {
        if (rollingUpgradeStatusBuilder_ != null) {
          return rollingUpgradeStatusBuilder_.getMessageOrBuilder();
        } else {
          return rollingUpgradeStatus_ == null ?
              org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto.getDefaultInstance() : rollingUpgradeStatus_;
        }
      }
      /**
       * <code>optional .org.apache.hadoop.RollingUpgradeStatusProto rollingUpgradeStatus = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProtoOrBuilder>
          getRollingUpgradeStatusFieldBuilder() {
        if (rollingUpgradeStatusBuilder_ == null) {
          rollingUpgradeStatusBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.RollingUpgradeStatusProtoOrBuilder>(
                  getRollingUpgradeStatus(),
                  getParentForChildren(),
                  isClean());
          rollingUpgradeStatus_ = null;
        }
        return rollingUpgradeStatusBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.HeartbeatResponseProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.HeartbeatResponseProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<HeartbeatResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<HeartbeatResponseProto>() {
      @java.lang.Override
      public HeartbeatResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new HeartbeatResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<HeartbeatResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<HeartbeatResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface BlockReportRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.BlockReportRequestProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return Whether the registration field is set.
     */
    boolean hasRegistration();
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return The registration.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration();
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder();

    /**
     * <code>required string blockPoolId = 2;</code>
     * @return Whether the blockPoolId field is set.
     */
    boolean hasBlockPoolId();
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The blockPoolId.
     */
    java.lang.String getBlockPoolId();
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The bytes for blockPoolId.
     */
    com.google.protobuf.ByteString
        getBlockPoolIdBytes();

    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto>
        getReportsList();
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto getReports(int index);
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
     */
    int getReportsCount();
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProtoOrBuilder>
        getReportsOrBuilderList();
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProtoOrBuilder getReportsOrBuilder(
        int index);

    /**
     * <code>optional .org.apache.hadoop.datanode.BlockReportContextProto context = 4;</code>
     * @return Whether the context field is set.
     */
    boolean hasContext();
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockReportContextProto context = 4;</code>
     * @return The context.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto getContext();
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockReportContextProto context = 4;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProtoOrBuilder getContextOrBuilder();
  }
  /**
   * <pre>
   **
   * registration - datanode registration information
   * blockPoolID  - block pool ID of the reported blocks
   * blocks       - each block is represented as multiple longs in the array.
   *                first long represents block ID
   *                second long represents length
   *                third long represents gen stamp
   *                fourth long (if under construction) represents replica state
   * context      - An optional field containing information about the context
   *                of this block report.
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.BlockReportRequestProto}
   */
  public static final class BlockReportRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.BlockReportRequestProto)
      BlockReportRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use BlockReportRequestProto.newBuilder() to construct.
    private BlockReportRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private BlockReportRequestProto() {
      blockPoolId_ = "";
      reports_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new BlockReportRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private BlockReportRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = registration_.toBuilder();
              }
              registration_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(registration_);
                registration_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000002;
              blockPoolId_ = bs;
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                reports_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto>();
                mutable_bitField0_ |= 0x00000004;
              }
              reports_.add(
                  input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.PARSER, extensionRegistry));
              break;
            }
            case 34: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000004) != 0)) {
                subBuilder = context_.toBuilder();
              }
              context_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(context_);
                context_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000004;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          reports_ = java.util.Collections.unmodifiableList(reports_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int REGISTRATION_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto registration_;
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return Whether the registration field is set.
     */
    @java.lang.Override
    public boolean hasRegistration() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return The registration.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration() {
      return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
    }
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder() {
      return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
    }

    public static final int BLOCKPOOLID_FIELD_NUMBER = 2;
    private volatile java.lang.Object blockPoolId_;
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return Whether the blockPoolId field is set.
     */
    @java.lang.Override
    public boolean hasBlockPoolId() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The blockPoolId.
     */
    @java.lang.Override
    public java.lang.String getBlockPoolId() {
      java.lang.Object ref = blockPoolId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          blockPoolId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The bytes for blockPoolId.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getBlockPoolIdBytes() {
      java.lang.Object ref = blockPoolId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        blockPoolId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int REPORTS_FIELD_NUMBER = 3;
    private java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto> reports_;
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto> getReportsList() {
      return reports_;
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProtoOrBuilder>
        getReportsOrBuilderList() {
      return reports_;
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
     */
    @java.lang.Override
    public int getReportsCount() {
      return reports_.size();
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto getReports(int index) {
      return reports_.get(index);
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProtoOrBuilder getReportsOrBuilder(
        int index) {
      return reports_.get(index);
    }

    public static final int CONTEXT_FIELD_NUMBER = 4;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto context_;
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockReportContextProto context = 4;</code>
     * @return Whether the context field is set.
     */
    @java.lang.Override
    public boolean hasContext() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockReportContextProto context = 4;</code>
     * @return The context.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto getContext() {
      return context_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.getDefaultInstance() : context_;
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.BlockReportContextProto context = 4;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProtoOrBuilder getContextOrBuilder() {
      return context_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.getDefaultInstance() : context_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasRegistration()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasBlockPoolId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getRegistration().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      for (int i = 0; i < getReportsCount(); i++) {
        if (!getReports(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasContext()) {
        if (!getContext().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getRegistration());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, blockPoolId_);
      }
      for (int i = 0; i < reports_.size(); i++) {
        output.writeMessage(3, reports_.get(i));
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeMessage(4, getContext());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getRegistration());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, blockPoolId_);
      }
      for (int i = 0; i < reports_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, reports_.get(i));
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, getContext());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto) obj;

      if (hasRegistration() != other.hasRegistration()) return false;
      if (hasRegistration()) {
        if (!getRegistration()
            .equals(other.getRegistration())) return false;
      }
      if (hasBlockPoolId() != other.hasBlockPoolId()) return false;
      if (hasBlockPoolId()) {
        if (!getBlockPoolId()
            .equals(other.getBlockPoolId())) return false;
      }
      if (!getReportsList()
          .equals(other.getReportsList())) return false;
      if (hasContext() != other.hasContext()) return false;
      if (hasContext()) {
        if (!getContext()
            .equals(other.getContext())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasRegistration()) {
        hash = (37 * hash) + REGISTRATION_FIELD_NUMBER;
        hash = (53 * hash) + getRegistration().hashCode();
      }
      if (hasBlockPoolId()) {
        hash = (37 * hash) + BLOCKPOOLID_FIELD_NUMBER;
        hash = (53 * hash) + getBlockPoolId().hashCode();
      }
      if (getReportsCount() > 0) {
        hash = (37 * hash) + REPORTS_FIELD_NUMBER;
        hash = (53 * hash) + getReportsList().hashCode();
      }
      if (hasContext()) {
        hash = (37 * hash) + CONTEXT_FIELD_NUMBER;
        hash = (53 * hash) + getContext().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * registration - datanode registration information
     * blockPoolID  - block pool ID of the reported blocks
     * blocks       - each block is represented as multiple longs in the array.
     *                first long represents block ID
     *                second long represents length
     *                third long represents gen stamp
     *                fourth long (if under construction) represents replica state
     * context      - An optional field containing information about the context
     *                of this block report.
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.BlockReportRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.BlockReportRequestProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getRegistrationFieldBuilder();
          getReportsFieldBuilder();
          getContextFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (registrationBuilder_ == null) {
          registration_ = null;
        } else {
          registrationBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        blockPoolId_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        if (reportsBuilder_ == null) {
          reports_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          reportsBuilder_.clear();
        }
        if (contextBuilder_ == null) {
          context_ = null;
        } else {
          contextBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (registrationBuilder_ == null) {
            result.registration_ = registration_;
          } else {
            result.registration_ = registrationBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.blockPoolId_ = blockPoolId_;
        if (reportsBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0)) {
            reports_ = java.util.Collections.unmodifiableList(reports_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.reports_ = reports_;
        } else {
          result.reports_ = reportsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          if (contextBuilder_ == null) {
            result.context_ = context_;
          } else {
            result.context_ = contextBuilder_.build();
          }
          to_bitField0_ |= 0x00000004;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto.getDefaultInstance()) return this;
        if (other.hasRegistration()) {
          mergeRegistration(other.getRegistration());
        }
        if (other.hasBlockPoolId()) {
          bitField0_ |= 0x00000002;
          blockPoolId_ = other.blockPoolId_;
          onChanged();
        }
        if (reportsBuilder_ == null) {
          if (!other.reports_.isEmpty()) {
            if (reports_.isEmpty()) {
              reports_ = other.reports_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureReportsIsMutable();
              reports_.addAll(other.reports_);
            }
            onChanged();
          }
        } else {
          if (!other.reports_.isEmpty()) {
            if (reportsBuilder_.isEmpty()) {
              reportsBuilder_.dispose();
              reportsBuilder_ = null;
              reports_ = other.reports_;
              bitField0_ = (bitField0_ & ~0x00000004);
              reportsBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getReportsFieldBuilder() : null;
            } else {
              reportsBuilder_.addAllMessages(other.reports_);
            }
          }
        }
        if (other.hasContext()) {
          mergeContext(other.getContext());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasRegistration()) {
          return false;
        }
        if (!hasBlockPoolId()) {
          return false;
        }
        if (!getRegistration().isInitialized()) {
          return false;
        }
        for (int i = 0; i < getReportsCount(); i++) {
          if (!getReports(i).isInitialized()) {
            return false;
          }
        }
        if (hasContext()) {
          if (!getContext().isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto registration_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder> registrationBuilder_;
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       * @return Whether the registration field is set.
       */
      public boolean hasRegistration() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       * @return The registration.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration() {
        if (registrationBuilder_ == null) {
          return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
        } else {
          return registrationBuilder_.getMessage();
        }
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder setRegistration(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto value) {
        if (registrationBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          registration_ = value;
          onChanged();
        } else {
          registrationBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder setRegistration(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder builderForValue) {
        if (registrationBuilder_ == null) {
          registration_ = builderForValue.build();
          onChanged();
        } else {
          registrationBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder mergeRegistration(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto value) {
        if (registrationBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              registration_ != null &&
              registration_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance()) {
            registration_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.newBuilder(registration_).mergeFrom(value).buildPartial();
          } else {
            registration_ = value;
          }
          onChanged();
        } else {
          registrationBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder clearRegistration() {
        if (registrationBuilder_ == null) {
          registration_ = null;
          onChanged();
        } else {
          registrationBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder getRegistrationBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getRegistrationFieldBuilder().getBuilder();
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder() {
        if (registrationBuilder_ != null) {
          return registrationBuilder_.getMessageOrBuilder();
        } else {
          return registration_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
        }
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder>
          getRegistrationFieldBuilder() {
        if (registrationBuilder_ == null) {
          registrationBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder>(
                  getRegistration(),
                  getParentForChildren(),
                  isClean());
          registration_ = null;
        }
        return registrationBuilder_;
      }

      private java.lang.Object blockPoolId_ = "";
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return Whether the blockPoolId field is set.
       */
      public boolean hasBlockPoolId() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return The blockPoolId.
       */
      public java.lang.String getBlockPoolId() {
        java.lang.Object ref = blockPoolId_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            blockPoolId_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return The bytes for blockPoolId.
       */
      public com.google.protobuf.ByteString
          getBlockPoolIdBytes() {
        java.lang.Object ref = blockPoolId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          blockPoolId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @param value The blockPoolId to set.
       * @return This builder for chaining.
       */
      public Builder setBlockPoolId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        blockPoolId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearBlockPoolId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        blockPoolId_ = getDefaultInstance().getBlockPoolId();
        onChanged();
        return this;
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @param value The bytes for blockPoolId to set.
       * @return This builder for chaining.
       */
      public Builder setBlockPoolIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        blockPoolId_ = value;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto> reports_ =
        java.util.Collections.emptyList();
      private void ensureReportsIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          reports_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto>(reports_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProtoOrBuilder> reportsBuilder_;

      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto> getReportsList() {
        if (reportsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(reports_);
        } else {
          return reportsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
       */
      public int getReportsCount() {
        if (reportsBuilder_ == null) {
          return reports_.size();
        } else {
          return reportsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto getReports(int index) {
        if (reportsBuilder_ == null) {
          return reports_.get(index);
        } else {
          return reportsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
       */
      public Builder setReports(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto value) {
        if (reportsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureReportsIsMutable();
          reports_.set(index, value);
          onChanged();
        } else {
          reportsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
       */
      public Builder setReports(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.Builder builderForValue) {
        if (reportsBuilder_ == null) {
          ensureReportsIsMutable();
          reports_.set(index, builderForValue.build());
          onChanged();
        } else {
          reportsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
       */
      public Builder addReports(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto value) {
        if (reportsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureReportsIsMutable();
          reports_.add(value);
          onChanged();
        } else {
          reportsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
       */
      public Builder addReports(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto value) {
        if (reportsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureReportsIsMutable();
          reports_.add(index, value);
          onChanged();
        } else {
          reportsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
       */
      public Builder addReports(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.Builder builderForValue) {
        if (reportsBuilder_ == null) {
          ensureReportsIsMutable();
          reports_.add(builderForValue.build());
          onChanged();
        } else {
          reportsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
       */
      public Builder addReports(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.Builder builderForValue) {
        if (reportsBuilder_ == null) {
          ensureReportsIsMutable();
          reports_.add(index, builderForValue.build());
          onChanged();
        } else {
          reportsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
       */
      public Builder addAllReports(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto> values) {
        if (reportsBuilder_ == null) {
          ensureReportsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, reports_);
          onChanged();
        } else {
          reportsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
       */
      public Builder clearReports() {
        if (reportsBuilder_ == null) {
          reports_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          reportsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
       */
      public Builder removeReports(int index) {
        if (reportsBuilder_ == null) {
          ensureReportsIsMutable();
          reports_.remove(index);
          onChanged();
        } else {
          reportsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.Builder getReportsBuilder(
          int index) {
        return getReportsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProtoOrBuilder getReportsOrBuilder(
          int index) {
        if (reportsBuilder_ == null) {
          return reports_.get(index);  } else {
          return reportsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProtoOrBuilder>
           getReportsOrBuilderList() {
        if (reportsBuilder_ != null) {
          return reportsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(reports_);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.Builder addReportsBuilder() {
        return getReportsFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.Builder addReportsBuilder(
          int index) {
        return getReportsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageBlockReportProto reports = 3;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.Builder>
           getReportsBuilderList() {
        return getReportsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProtoOrBuilder>
          getReportsFieldBuilder() {
        if (reportsBuilder_ == null) {
          reportsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProtoOrBuilder>(
                  reports_,
                  ((bitField0_ & 0x00000004) != 0),
                  getParentForChildren(),
                  isClean());
          reports_ = null;
        }
        return reportsBuilder_;
      }

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto context_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProtoOrBuilder> contextBuilder_;
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockReportContextProto context = 4;</code>
       * @return Whether the context field is set.
       */
      public boolean hasContext() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockReportContextProto context = 4;</code>
       * @return The context.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto getContext() {
        if (contextBuilder_ == null) {
          return context_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.getDefaultInstance() : context_;
        } else {
          return contextBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockReportContextProto context = 4;</code>
       */
      public Builder setContext(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto value) {
        if (contextBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          context_ = value;
          onChanged();
        } else {
          contextBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockReportContextProto context = 4;</code>
       */
      public Builder setContext(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.Builder builderForValue) {
        if (contextBuilder_ == null) {
          context_ = builderForValue.build();
          onChanged();
        } else {
          contextBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockReportContextProto context = 4;</code>
       */
      public Builder mergeContext(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto value) {
        if (contextBuilder_ == null) {
          if (((bitField0_ & 0x00000008) != 0) &&
              context_ != null &&
              context_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.getDefaultInstance()) {
            context_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.newBuilder(context_).mergeFrom(value).buildPartial();
          } else {
            context_ = value;
          }
          onChanged();
        } else {
          contextBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockReportContextProto context = 4;</code>
       */
      public Builder clearContext() {
        if (contextBuilder_ == null) {
          context_ = null;
          onChanged();
        } else {
          contextBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockReportContextProto context = 4;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.Builder getContextBuilder() {
        bitField0_ |= 0x00000008;
        onChanged();
        return getContextFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockReportContextProto context = 4;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProtoOrBuilder getContextOrBuilder() {
        if (contextBuilder_ != null) {
          return contextBuilder_.getMessageOrBuilder();
        } else {
          return context_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.getDefaultInstance() : context_;
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.BlockReportContextProto context = 4;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProtoOrBuilder>
          getContextFieldBuilder() {
        if (contextBuilder_ == null) {
          contextBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProtoOrBuilder>(
                  getContext(),
                  getParentForChildren(),
                  isClean());
          context_ = null;
        }
        return contextBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.BlockReportRequestProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.BlockReportRequestProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<BlockReportRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<BlockReportRequestProto>() {
      @java.lang.Override
      public BlockReportRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new BlockReportRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<BlockReportRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<BlockReportRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface BlockReportContextProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.BlockReportContextProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * The total number of RPCs this block report is broken into.
     * </pre>
     *
     * <code>required int32 totalRpcs = 1;</code>
     * @return Whether the totalRpcs field is set.
     */
    boolean hasTotalRpcs();
    /**
     * <pre>
     * The total number of RPCs this block report is broken into.
     * </pre>
     *
     * <code>required int32 totalRpcs = 1;</code>
     * @return The totalRpcs.
     */
    int getTotalRpcs();

    /**
     * <pre>
     * The index of the current RPC (zero-based)
     * </pre>
     *
     * <code>required int32 curRpc = 2;</code>
     * @return Whether the curRpc field is set.
     */
    boolean hasCurRpc();
    /**
     * <pre>
     * The index of the current RPC (zero-based)
     * </pre>
     *
     * <code>required int32 curRpc = 2;</code>
     * @return The curRpc.
     */
    int getCurRpc();

    /**
     * <pre>
     * The unique 64-bit ID of this block report
     * </pre>
     *
     * <code>required int64 id = 3;</code>
     * @return Whether the id field is set.
     */
    boolean hasId();
    /**
     * <pre>
     * The unique 64-bit ID of this block report
     * </pre>
     *
     * <code>required int64 id = 3;</code>
     * @return The id.
     */
    long getId();
  }
  /**
   * Protobuf type {@code org.apache.hadoop.datanode.BlockReportContextProto}
   */
  public static final class BlockReportContextProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.BlockReportContextProto)
      BlockReportContextProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use BlockReportContextProto.newBuilder() to construct.
    private BlockReportContextProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private BlockReportContextProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new BlockReportContextProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private BlockReportContextProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              bitField0_ |= 0x00000001;
              totalRpcs_ = input.readInt32();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              curRpc_ = input.readInt32();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              id_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportContextProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportContextProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.Builder.class);
    }

    private int bitField0_;
    public static final int TOTALRPCS_FIELD_NUMBER = 1;
    private int totalRpcs_;
    /**
     * <pre>
     * The total number of RPCs this block report is broken into.
     * </pre>
     *
     * <code>required int32 totalRpcs = 1;</code>
     * @return Whether the totalRpcs field is set.
     */
    @java.lang.Override
    public boolean hasTotalRpcs() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <pre>
     * The total number of RPCs this block report is broken into.
     * </pre>
     *
     * <code>required int32 totalRpcs = 1;</code>
     * @return The totalRpcs.
     */
    @java.lang.Override
    public int getTotalRpcs() {
      return totalRpcs_;
    }

    public static final int CURRPC_FIELD_NUMBER = 2;
    private int curRpc_;
    /**
     * <pre>
     * The index of the current RPC (zero-based)
     * </pre>
     *
     * <code>required int32 curRpc = 2;</code>
     * @return Whether the curRpc field is set.
     */
    @java.lang.Override
    public boolean hasCurRpc() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <pre>
     * The index of the current RPC (zero-based)
     * </pre>
     *
     * <code>required int32 curRpc = 2;</code>
     * @return The curRpc.
     */
    @java.lang.Override
    public int getCurRpc() {
      return curRpc_;
    }

    public static final int ID_FIELD_NUMBER = 3;
    private long id_;
    /**
     * <pre>
     * The unique 64-bit ID of this block report
     * </pre>
     *
     * <code>required int64 id = 3;</code>
     * @return Whether the id field is set.
     */
    @java.lang.Override
    public boolean hasId() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <pre>
     * The unique 64-bit ID of this block report
     * </pre>
     *
     * <code>required int64 id = 3;</code>
     * @return The id.
     */
    @java.lang.Override
    public long getId() {
      return id_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasTotalRpcs()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasCurRpc()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeInt32(1, totalRpcs_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeInt32(2, curRpc_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeInt64(3, id_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(1, totalRpcs_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, curRpc_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, id_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto) obj;

      if (hasTotalRpcs() != other.hasTotalRpcs()) return false;
      if (hasTotalRpcs()) {
        if (getTotalRpcs()
            != other.getTotalRpcs()) return false;
      }
      if (hasCurRpc() != other.hasCurRpc()) return false;
      if (hasCurRpc()) {
        if (getCurRpc()
            != other.getCurRpc()) return false;
      }
      if (hasId() != other.hasId()) return false;
      if (hasId()) {
        if (getId()
            != other.getId()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasTotalRpcs()) {
        hash = (37 * hash) + TOTALRPCS_FIELD_NUMBER;
        hash = (53 * hash) + getTotalRpcs();
      }
      if (hasCurRpc()) {
        hash = (37 * hash) + CURRPC_FIELD_NUMBER;
        hash = (53 * hash) + getCurRpc();
      }
      if (hasId()) {
        hash = (37 * hash) + ID_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getId());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.hadoop.datanode.BlockReportContextProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.BlockReportContextProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportContextProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportContextProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        totalRpcs_ = 0;
        bitField0_ = (bitField0_ & ~0x00000001);
        curRpc_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        id_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportContextProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.totalRpcs_ = totalRpcs_;
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.curRpc_ = curRpc_;
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.id_ = id_;
          to_bitField0_ |= 0x00000004;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto.getDefaultInstance()) return this;
        if (other.hasTotalRpcs()) {
          setTotalRpcs(other.getTotalRpcs());
        }
        if (other.hasCurRpc()) {
          setCurRpc(other.getCurRpc());
        }
        if (other.hasId()) {
          setId(other.getId());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasTotalRpcs()) {
          return false;
        }
        if (!hasCurRpc()) {
          return false;
        }
        if (!hasId()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int totalRpcs_ ;
      /**
       * <pre>
       * The total number of RPCs this block report is broken into.
       * </pre>
       *
       * <code>required int32 totalRpcs = 1;</code>
       * @return Whether the totalRpcs field is set.
       */
      @java.lang.Override
      public boolean hasTotalRpcs() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * The total number of RPCs this block report is broken into.
       * </pre>
       *
       * <code>required int32 totalRpcs = 1;</code>
       * @return The totalRpcs.
       */
      @java.lang.Override
      public int getTotalRpcs() {
        return totalRpcs_;
      }
      /**
       * <pre>
       * The total number of RPCs this block report is broken into.
       * </pre>
       *
       * <code>required int32 totalRpcs = 1;</code>
       * @param value The totalRpcs to set.
       * @return This builder for chaining.
       */
      public Builder setTotalRpcs(int value) {
        bitField0_ |= 0x00000001;
        totalRpcs_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * The total number of RPCs this block report is broken into.
       * </pre>
       *
       * <code>required int32 totalRpcs = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearTotalRpcs() {
        bitField0_ = (bitField0_ & ~0x00000001);
        totalRpcs_ = 0;
        onChanged();
        return this;
      }

      private int curRpc_ ;
      /**
       * <pre>
       * The index of the current RPC (zero-based)
       * </pre>
       *
       * <code>required int32 curRpc = 2;</code>
       * @return Whether the curRpc field is set.
       */
      @java.lang.Override
      public boolean hasCurRpc() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <pre>
       * The index of the current RPC (zero-based)
       * </pre>
       *
       * <code>required int32 curRpc = 2;</code>
       * @return The curRpc.
       */
      @java.lang.Override
      public int getCurRpc() {
        return curRpc_;
      }
      /**
       * <pre>
       * The index of the current RPC (zero-based)
       * </pre>
       *
       * <code>required int32 curRpc = 2;</code>
       * @param value The curRpc to set.
       * @return This builder for chaining.
       */
      public Builder setCurRpc(int value) {
        bitField0_ |= 0x00000002;
        curRpc_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * The index of the current RPC (zero-based)
       * </pre>
       *
       * <code>required int32 curRpc = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearCurRpc() {
        bitField0_ = (bitField0_ & ~0x00000002);
        curRpc_ = 0;
        onChanged();
        return this;
      }

      private long id_ ;
      /**
       * <pre>
       * The unique 64-bit ID of this block report
       * </pre>
       *
       * <code>required int64 id = 3;</code>
       * @return Whether the id field is set.
       */
      @java.lang.Override
      public boolean hasId() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <pre>
       * The unique 64-bit ID of this block report
       * </pre>
       *
       * <code>required int64 id = 3;</code>
       * @return The id.
       */
      @java.lang.Override
      public long getId() {
        return id_;
      }
      /**
       * <pre>
       * The unique 64-bit ID of this block report
       * </pre>
       *
       * <code>required int64 id = 3;</code>
       * @param value The id to set.
       * @return This builder for chaining.
       */
      public Builder setId(long value) {
        bitField0_ |= 0x00000004;
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * The unique 64-bit ID of this block report
       * </pre>
       *
       * <code>required int64 id = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearId() {
        bitField0_ = (bitField0_ & ~0x00000004);
        id_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.BlockReportContextProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.BlockReportContextProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<BlockReportContextProto>
        PARSER = new com.google.protobuf.AbstractParser<BlockReportContextProto>() {
      @java.lang.Override
      public BlockReportContextProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new BlockReportContextProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<BlockReportContextProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<BlockReportContextProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportContextProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface StorageBlockReportProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.StorageBlockReportProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required .org.apache.hadoop.DatanodeStorageProto storage = 1;</code>
     * @return Whether the storage field is set.
     */
    boolean hasStorage();
    /**
     * <code>required .org.apache.hadoop.DatanodeStorageProto storage = 1;</code>
     * @return The storage.
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto getStorage();
    /**
     * <code>required .org.apache.hadoop.DatanodeStorageProto storage = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder getStorageOrBuilder();

    /**
     * <code>required .org.apache.hadoop.datanode.BlockReportProto report = 2;</code>
     * @return Whether the report field is set.
     */
    boolean hasReport();
    /**
     * <code>required .org.apache.hadoop.datanode.BlockReportProto report = 2;</code>
     * @return The report.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto getReport();
    /**
     * <code>required .org.apache.hadoop.datanode.BlockReportProto report = 2;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProtoOrBuilder getReportOrBuilder();
  }
  /**
   * <pre>
   **
   * Report of blocks in a storage
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.StorageBlockReportProto}
   */
  public static final class StorageBlockReportProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.StorageBlockReportProto)
      StorageBlockReportProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use StorageBlockReportProto.newBuilder() to construct.
    private StorageBlockReportProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private StorageBlockReportProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new StorageBlockReportProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private StorageBlockReportProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = storage_.toBuilder();
              }
              storage_ = input.readMessage(org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(storage_);
                storage_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) != 0)) {
                subBuilder = report_.toBuilder();
              }
              report_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(report_);
                report_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageBlockReportProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageBlockReportProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.Builder.class);
    }

    private int bitField0_;
    public static final int STORAGE_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto storage_;
    /**
     * <code>required .org.apache.hadoop.DatanodeStorageProto storage = 1;</code>
     * @return Whether the storage field is set.
     */
    @java.lang.Override
    public boolean hasStorage() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required .org.apache.hadoop.DatanodeStorageProto storage = 1;</code>
     * @return The storage.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto getStorage() {
      return storage_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.getDefaultInstance() : storage_;
    }
    /**
     * <code>required .org.apache.hadoop.DatanodeStorageProto storage = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder getStorageOrBuilder() {
      return storage_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.getDefaultInstance() : storage_;
    }

    public static final int REPORT_FIELD_NUMBER = 2;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto report_;
    /**
     * <code>required .org.apache.hadoop.datanode.BlockReportProto report = 2;</code>
     * @return Whether the report field is set.
     */
    @java.lang.Override
    public boolean hasReport() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>required .org.apache.hadoop.datanode.BlockReportProto report = 2;</code>
     * @return The report.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto getReport() {
      return report_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.getDefaultInstance() : report_;
    }
    /**
     * <code>required .org.apache.hadoop.datanode.BlockReportProto report = 2;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProtoOrBuilder getReportOrBuilder() {
      return report_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.getDefaultInstance() : report_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasStorage()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasReport()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getStorage().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getReport().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getStorage());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeMessage(2, getReport());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getStorage());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, getReport());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto) obj;

      if (hasStorage() != other.hasStorage()) return false;
      if (hasStorage()) {
        if (!getStorage()
            .equals(other.getStorage())) return false;
      }
      if (hasReport() != other.hasReport()) return false;
      if (hasReport()) {
        if (!getReport()
            .equals(other.getReport())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasStorage()) {
        hash = (37 * hash) + STORAGE_FIELD_NUMBER;
        hash = (53 * hash) + getStorage().hashCode();
      }
      if (hasReport()) {
        hash = (37 * hash) + REPORT_FIELD_NUMBER;
        hash = (53 * hash) + getReport().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * Report of blocks in a storage
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.StorageBlockReportProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.StorageBlockReportProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageBlockReportProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageBlockReportProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getStorageFieldBuilder();
          getReportFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (storageBuilder_ == null) {
          storage_ = null;
        } else {
          storageBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (reportBuilder_ == null) {
          report_ = null;
        } else {
          reportBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageBlockReportProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (storageBuilder_ == null) {
            result.storage_ = storage_;
          } else {
            result.storage_ = storageBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          if (reportBuilder_ == null) {
            result.report_ = report_;
          } else {
            result.report_ = reportBuilder_.build();
          }
          to_bitField0_ |= 0x00000002;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto.getDefaultInstance()) return this;
        if (other.hasStorage()) {
          mergeStorage(other.getStorage());
        }
        if (other.hasReport()) {
          mergeReport(other.getReport());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasStorage()) {
          return false;
        }
        if (!hasReport()) {
          return false;
        }
        if (!getStorage().isInitialized()) {
          return false;
        }
        if (!getReport().isInitialized()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto storage_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder> storageBuilder_;
      /**
       * <code>required .org.apache.hadoop.DatanodeStorageProto storage = 1;</code>
       * @return Whether the storage field is set.
       */
      public boolean hasStorage() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required .org.apache.hadoop.DatanodeStorageProto storage = 1;</code>
       * @return The storage.
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto getStorage() {
        if (storageBuilder_ == null) {
          return storage_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.getDefaultInstance() : storage_;
        } else {
          return storageBuilder_.getMessage();
        }
      }
      /**
       * <code>required .org.apache.hadoop.DatanodeStorageProto storage = 1;</code>
       */
      public Builder setStorage(org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto value) {
        if (storageBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          storage_ = value;
          onChanged();
        } else {
          storageBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.DatanodeStorageProto storage = 1;</code>
       */
      public Builder setStorage(
          org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder builderForValue) {
        if (storageBuilder_ == null) {
          storage_ = builderForValue.build();
          onChanged();
        } else {
          storageBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.DatanodeStorageProto storage = 1;</code>
       */
      public Builder mergeStorage(org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto value) {
        if (storageBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              storage_ != null &&
              storage_ != org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.getDefaultInstance()) {
            storage_ =
              org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.newBuilder(storage_).mergeFrom(value).buildPartial();
          } else {
            storage_ = value;
          }
          onChanged();
        } else {
          storageBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.DatanodeStorageProto storage = 1;</code>
       */
      public Builder clearStorage() {
        if (storageBuilder_ == null) {
          storage_ = null;
          onChanged();
        } else {
          storageBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.DatanodeStorageProto storage = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder getStorageBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getStorageFieldBuilder().getBuilder();
      }
      /**
       * <code>required .org.apache.hadoop.DatanodeStorageProto storage = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder getStorageOrBuilder() {
        if (storageBuilder_ != null) {
          return storageBuilder_.getMessageOrBuilder();
        } else {
          return storage_ == null ?
              org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.getDefaultInstance() : storage_;
        }
      }
      /**
       * <code>required .org.apache.hadoop.DatanodeStorageProto storage = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder>
          getStorageFieldBuilder() {
        if (storageBuilder_ == null) {
          storageBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder>(
                  getStorage(),
                  getParentForChildren(),
                  isClean());
          storage_ = null;
        }
        return storageBuilder_;
      }

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto report_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProtoOrBuilder> reportBuilder_;
      /**
       * <code>required .org.apache.hadoop.datanode.BlockReportProto report = 2;</code>
       * @return Whether the report field is set.
       */
      public boolean hasReport() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>required .org.apache.hadoop.datanode.BlockReportProto report = 2;</code>
       * @return The report.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto getReport() {
        if (reportBuilder_ == null) {
          return report_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.getDefaultInstance() : report_;
        } else {
          return reportBuilder_.getMessage();
        }
      }
      /**
       * <code>required .org.apache.hadoop.datanode.BlockReportProto report = 2;</code>
       */
      public Builder setReport(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto value) {
        if (reportBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          report_ = value;
          onChanged();
        } else {
          reportBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.BlockReportProto report = 2;</code>
       */
      public Builder setReport(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.Builder builderForValue) {
        if (reportBuilder_ == null) {
          report_ = builderForValue.build();
          onChanged();
        } else {
          reportBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.BlockReportProto report = 2;</code>
       */
      public Builder mergeReport(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto value) {
        if (reportBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0) &&
              report_ != null &&
              report_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.getDefaultInstance()) {
            report_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.newBuilder(report_).mergeFrom(value).buildPartial();
          } else {
            report_ = value;
          }
          onChanged();
        } else {
          reportBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.BlockReportProto report = 2;</code>
       */
      public Builder clearReport() {
        if (reportBuilder_ == null) {
          report_ = null;
          onChanged();
        } else {
          reportBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.BlockReportProto report = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.Builder getReportBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getReportFieldBuilder().getBuilder();
      }
      /**
       * <code>required .org.apache.hadoop.datanode.BlockReportProto report = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProtoOrBuilder getReportOrBuilder() {
        if (reportBuilder_ != null) {
          return reportBuilder_.getMessageOrBuilder();
        } else {
          return report_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.getDefaultInstance() : report_;
        }
      }
      /**
       * <code>required .org.apache.hadoop.datanode.BlockReportProto report = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProtoOrBuilder>
          getReportFieldBuilder() {
        if (reportBuilder_ == null) {
          reportBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProtoOrBuilder>(
                  getReport(),
                  getParentForChildren(),
                  isClean());
          report_ = null;
        }
        return reportBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.StorageBlockReportProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.StorageBlockReportProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<StorageBlockReportProto>
        PARSER = new com.google.protobuf.AbstractParser<StorageBlockReportProto>() {
      @java.lang.Override
      public StorageBlockReportProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StorageBlockReportProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<StorageBlockReportProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<StorageBlockReportProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageBlockReportProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface BlockReportBucketProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.BlockReportBucketProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated uint64 blocks = 1 [packed = true];</code>
     * @return A list containing the blocks.
     */
    java.util.List<java.lang.Long> getBlocksList();
    /**
     * <code>repeated uint64 blocks = 1 [packed = true];</code>
     * @return The count of blocks.
     */
    int getBlocksCount();
    /**
     * <code>repeated uint64 blocks = 1 [packed = true];</code>
     * @param index The index of the element to return.
     * @return The blocks at the given index.
     */
    long getBlocks(int index);

    /**
     * <code>required bytes hash = 2;</code>
     * @return Whether the hash field is set.
     */
    boolean hasHash();
    /**
     * <code>required bytes hash = 2;</code>
     * @return The hash.
     */
    com.google.protobuf.ByteString getHash();

    /**
     * <code>optional uint64 numberOfBlocks = 3;</code>
     * @return Whether the numberOfBlocks field is set.
     */
    boolean hasNumberOfBlocks();
    /**
     * <code>optional uint64 numberOfBlocks = 3;</code>
     * @return The numberOfBlocks.
     */
    long getNumberOfBlocks();

    /**
     * <code>repeated bytes blocksBuffers = 4;</code>
     * @return A list containing the blocksBuffers.
     */
    java.util.List<com.google.protobuf.ByteString> getBlocksBuffersList();
    /**
     * <code>repeated bytes blocksBuffers = 4;</code>
     * @return The count of blocksBuffers.
     */
    int getBlocksBuffersCount();
    /**
     * <code>repeated bytes blocksBuffers = 4;</code>
     * @param index The index of the element to return.
     * @return The blocksBuffers at the given index.
     */
    com.google.protobuf.ByteString getBlocksBuffers(int index);

    /**
     * <code>required bool skip = 5 [default = false];</code>
     * @return Whether the skip field is set.
     */
    boolean hasSkip();
    /**
     * <code>required bool skip = 5 [default = false];</code>
     * @return The skip.
     */
    boolean getSkip();
  }
  /**
   * Protobuf type {@code org.apache.hadoop.datanode.BlockReportBucketProto}
   */
  public static final class BlockReportBucketProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.BlockReportBucketProto)
      BlockReportBucketProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use BlockReportBucketProto.newBuilder() to construct.
    private BlockReportBucketProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private BlockReportBucketProto() {
      blocks_ = emptyLongList();
      hash_ = com.google.protobuf.ByteString.EMPTY;
      blocksBuffers_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new BlockReportBucketProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private BlockReportBucketProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                blocks_ = newLongList();
                mutable_bitField0_ |= 0x00000001;
              }
              blocks_.addLong(input.readUInt64());
              break;
            }
            case 10: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                blocks_ = newLongList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                blocks_.addLong(input.readUInt64());
              }
              input.popLimit(limit);
              break;
            }
            case 18: {
              bitField0_ |= 0x00000001;
              hash_ = input.readBytes();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000002;
              numberOfBlocks_ = input.readUInt64();
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000008) != 0)) {
                blocksBuffers_ = new java.util.ArrayList<com.google.protobuf.ByteString>();
                mutable_bitField0_ |= 0x00000008;
              }
              blocksBuffers_.add(input.readBytes());
              break;
            }
            case 40: {
              bitField0_ |= 0x00000004;
              skip_ = input.readBool();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          blocks_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000008) != 0)) {
          blocksBuffers_ = java.util.Collections.unmodifiableList(blocksBuffers_); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportBucketProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportBucketProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.Builder.class);
    }

    private int bitField0_;
    public static final int BLOCKS_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.LongList blocks_;
    /**
     * <code>repeated uint64 blocks = 1 [packed = true];</code>
     * @return A list containing the blocks.
     */
    @java.lang.Override
    public java.util.List<java.lang.Long>
        getBlocksList() {
      return blocks_;
    }
    /**
     * <code>repeated uint64 blocks = 1 [packed = true];</code>
     * @return The count of blocks.
     */
    public int getBlocksCount() {
      return blocks_.size();
    }
    /**
     * <code>repeated uint64 blocks = 1 [packed = true];</code>
     * @param index The index of the element to return.
     * @return The blocks at the given index.
     */
    public long getBlocks(int index) {
      return blocks_.getLong(index);
    }
    private int blocksMemoizedSerializedSize = -1;

    public static final int HASH_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString hash_;
    /**
     * <code>required bytes hash = 2;</code>
     * @return Whether the hash field is set.
     */
    @java.lang.Override
    public boolean hasHash() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required bytes hash = 2;</code>
     * @return The hash.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString getHash() {
      return hash_;
    }

    public static final int NUMBEROFBLOCKS_FIELD_NUMBER = 3;
    private long numberOfBlocks_;
    /**
     * <code>optional uint64 numberOfBlocks = 3;</code>
     * @return Whether the numberOfBlocks field is set.
     */
    @java.lang.Override
    public boolean hasNumberOfBlocks() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional uint64 numberOfBlocks = 3;</code>
     * @return The numberOfBlocks.
     */
    @java.lang.Override
    public long getNumberOfBlocks() {
      return numberOfBlocks_;
    }

    public static final int BLOCKSBUFFERS_FIELD_NUMBER = 4;
    private java.util.List<com.google.protobuf.ByteString> blocksBuffers_;
    /**
     * <code>repeated bytes blocksBuffers = 4;</code>
     * @return A list containing the blocksBuffers.
     */
    @java.lang.Override
    public java.util.List<com.google.protobuf.ByteString>
        getBlocksBuffersList() {
      return blocksBuffers_;
    }
    /**
     * <code>repeated bytes blocksBuffers = 4;</code>
     * @return The count of blocksBuffers.
     */
    public int getBlocksBuffersCount() {
      return blocksBuffers_.size();
    }
    /**
     * <code>repeated bytes blocksBuffers = 4;</code>
     * @param index The index of the element to return.
     * @return The blocksBuffers at the given index.
     */
    public com.google.protobuf.ByteString getBlocksBuffers(int index) {
      return blocksBuffers_.get(index);
    }

    public static final int SKIP_FIELD_NUMBER = 5;
    private boolean skip_;
    /**
     * <code>required bool skip = 5 [default = false];</code>
     * @return Whether the skip field is set.
     */
    @java.lang.Override
    public boolean hasSkip() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>required bool skip = 5 [default = false];</code>
     * @return The skip.
     */
    @java.lang.Override
    public boolean getSkip() {
      return skip_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasHash()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasSkip()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (getBlocksList().size() > 0) {
        output.writeUInt32NoTag(10);
        output.writeUInt32NoTag(blocksMemoizedSerializedSize);
      }
      for (int i = 0; i < blocks_.size(); i++) {
        output.writeUInt64NoTag(blocks_.getLong(i));
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeBytes(2, hash_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeUInt64(3, numberOfBlocks_);
      }
      for (int i = 0; i < blocksBuffers_.size(); i++) {
        output.writeBytes(4, blocksBuffers_.get(i));
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeBool(5, skip_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        for (int i = 0; i < blocks_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeUInt64SizeNoTag(blocks_.getLong(i));
        }
        size += dataSize;
        if (!getBlocksList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        blocksMemoizedSerializedSize = dataSize;
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, hash_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, numberOfBlocks_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < blocksBuffers_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeBytesSizeNoTag(blocksBuffers_.get(i));
        }
        size += dataSize;
        size += 1 * getBlocksBuffersList().size();
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(5, skip_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto) obj;

      if (!getBlocksList()
          .equals(other.getBlocksList())) return false;
      if (hasHash() != other.hasHash()) return false;
      if (hasHash()) {
        if (!getHash()
            .equals(other.getHash())) return false;
      }
      if (hasNumberOfBlocks() != other.hasNumberOfBlocks()) return false;
      if (hasNumberOfBlocks()) {
        if (getNumberOfBlocks()
            != other.getNumberOfBlocks()) return false;
      }
      if (!getBlocksBuffersList()
          .equals(other.getBlocksBuffersList())) return false;
      if (hasSkip() != other.hasSkip()) return false;
      if (hasSkip()) {
        if (getSkip()
            != other.getSkip()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getBlocksCount() > 0) {
        hash = (37 * hash) + BLOCKS_FIELD_NUMBER;
        hash = (53 * hash) + getBlocksList().hashCode();
      }
      if (hasHash()) {
        hash = (37 * hash) + HASH_FIELD_NUMBER;
        hash = (53 * hash) + getHash().hashCode();
      }
      if (hasNumberOfBlocks()) {
        hash = (37 * hash) + NUMBEROFBLOCKS_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getNumberOfBlocks());
      }
      if (getBlocksBuffersCount() > 0) {
        hash = (37 * hash) + BLOCKSBUFFERS_FIELD_NUMBER;
        hash = (53 * hash) + getBlocksBuffersList().hashCode();
      }
      if (hasSkip()) {
        hash = (37 * hash) + SKIP_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getSkip());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.hadoop.datanode.BlockReportBucketProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.BlockReportBucketProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportBucketProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportBucketProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        blocks_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000001);
        hash_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        numberOfBlocks_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        blocksBuffers_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000008);
        skip_ = false;
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportBucketProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((bitField0_ & 0x00000001) != 0)) {
          blocks_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.blocks_ = blocks_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.hash_ = hash_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.numberOfBlocks_ = numberOfBlocks_;
          to_bitField0_ |= 0x00000002;
        }
        if (((bitField0_ & 0x00000008) != 0)) {
          blocksBuffers_ = java.util.Collections.unmodifiableList(blocksBuffers_);
          bitField0_ = (bitField0_ & ~0x00000008);
        }
        result.blocksBuffers_ = blocksBuffers_;
        if (((from_bitField0_ & 0x00000010) != 0)) {
          result.skip_ = skip_;
          to_bitField0_ |= 0x00000004;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.getDefaultInstance()) return this;
        if (!other.blocks_.isEmpty()) {
          if (blocks_.isEmpty()) {
            blocks_ = other.blocks_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureBlocksIsMutable();
            blocks_.addAll(other.blocks_);
          }
          onChanged();
        }
        if (other.hasHash()) {
          setHash(other.getHash());
        }
        if (other.hasNumberOfBlocks()) {
          setNumberOfBlocks(other.getNumberOfBlocks());
        }
        if (!other.blocksBuffers_.isEmpty()) {
          if (blocksBuffers_.isEmpty()) {
            blocksBuffers_ = other.blocksBuffers_;
            bitField0_ = (bitField0_ & ~0x00000008);
          } else {
            ensureBlocksBuffersIsMutable();
            blocksBuffers_.addAll(other.blocksBuffers_);
          }
          onChanged();
        }
        if (other.hasSkip()) {
          setSkip(other.getSkip());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasHash()) {
          return false;
        }
        if (!hasSkip()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.Internal.LongList blocks_ = emptyLongList();
      private void ensureBlocksIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          blocks_ = mutableCopy(blocks_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <code>repeated uint64 blocks = 1 [packed = true];</code>
       * @return A list containing the blocks.
       */
      public java.util.List<java.lang.Long>
          getBlocksList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(blocks_) : blocks_;
      }
      /**
       * <code>repeated uint64 blocks = 1 [packed = true];</code>
       * @return The count of blocks.
       */
      public int getBlocksCount() {
        return blocks_.size();
      }
      /**
       * <code>repeated uint64 blocks = 1 [packed = true];</code>
       * @param index The index of the element to return.
       * @return The blocks at the given index.
       */
      public long getBlocks(int index) {
        return blocks_.getLong(index);
      }
      /**
       * <code>repeated uint64 blocks = 1 [packed = true];</code>
       * @param index The index to set the value at.
       * @param value The blocks to set.
       * @return This builder for chaining.
       */
      public Builder setBlocks(
          int index, long value) {
        ensureBlocksIsMutable();
        blocks_.setLong(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint64 blocks = 1 [packed = true];</code>
       * @param value The blocks to add.
       * @return This builder for chaining.
       */
      public Builder addBlocks(long value) {
        ensureBlocksIsMutable();
        blocks_.addLong(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint64 blocks = 1 [packed = true];</code>
       * @param values The blocks to add.
       * @return This builder for chaining.
       */
      public Builder addAllBlocks(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureBlocksIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, blocks_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint64 blocks = 1 [packed = true];</code>
       * @return This builder for chaining.
       */
      public Builder clearBlocks() {
        blocks_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      private com.google.protobuf.ByteString hash_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes hash = 2;</code>
       * @return Whether the hash field is set.
       */
      @java.lang.Override
      public boolean hasHash() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>required bytes hash = 2;</code>
       * @return The hash.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString getHash() {
        return hash_;
      }
      /**
       * <code>required bytes hash = 2;</code>
       * @param value The hash to set.
       * @return This builder for chaining.
       */
      public Builder setHash(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        hash_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes hash = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearHash() {
        bitField0_ = (bitField0_ & ~0x00000002);
        hash_ = getDefaultInstance().getHash();
        onChanged();
        return this;
      }

      private long numberOfBlocks_ ;
      /**
       * <code>optional uint64 numberOfBlocks = 3;</code>
       * @return Whether the numberOfBlocks field is set.
       */
      @java.lang.Override
      public boolean hasNumberOfBlocks() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional uint64 numberOfBlocks = 3;</code>
       * @return The numberOfBlocks.
       */
      @java.lang.Override
      public long getNumberOfBlocks() {
        return numberOfBlocks_;
      }
      /**
       * <code>optional uint64 numberOfBlocks = 3;</code>
       * @param value The numberOfBlocks to set.
       * @return This builder for chaining.
       */
      public Builder setNumberOfBlocks(long value) {
        bitField0_ |= 0x00000004;
        numberOfBlocks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 numberOfBlocks = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumberOfBlocks() {
        bitField0_ = (bitField0_ & ~0x00000004);
        numberOfBlocks_ = 0L;
        onChanged();
        return this;
      }

      private java.util.List<com.google.protobuf.ByteString> blocksBuffers_ = java.util.Collections.emptyList();
      private void ensureBlocksBuffersIsMutable() {
        if (!((bitField0_ & 0x00000008) != 0)) {
          blocksBuffers_ = new java.util.ArrayList<com.google.protobuf.ByteString>(blocksBuffers_);
          bitField0_ |= 0x00000008;
         }
      }
      /**
       * <code>repeated bytes blocksBuffers = 4;</code>
       * @return A list containing the blocksBuffers.
       */
      public java.util.List<com.google.protobuf.ByteString>
          getBlocksBuffersList() {
        return ((bitField0_ & 0x00000008) != 0) ?
                 java.util.Collections.unmodifiableList(blocksBuffers_) : blocksBuffers_;
      }
      /**
       * <code>repeated bytes blocksBuffers = 4;</code>
       * @return The count of blocksBuffers.
       */
      public int getBlocksBuffersCount() {
        return blocksBuffers_.size();
      }
      /**
       * <code>repeated bytes blocksBuffers = 4;</code>
       * @param index The index of the element to return.
       * @return The blocksBuffers at the given index.
       */
      public com.google.protobuf.ByteString getBlocksBuffers(int index) {
        return blocksBuffers_.get(index);
      }
      /**
       * <code>repeated bytes blocksBuffers = 4;</code>
       * @param index The index to set the value at.
       * @param value The blocksBuffers to set.
       * @return This builder for chaining.
       */
      public Builder setBlocksBuffers(
          int index, com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureBlocksBuffersIsMutable();
        blocksBuffers_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated bytes blocksBuffers = 4;</code>
       * @param value The blocksBuffers to add.
       * @return This builder for chaining.
       */
      public Builder addBlocksBuffers(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureBlocksBuffersIsMutable();
        blocksBuffers_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated bytes blocksBuffers = 4;</code>
       * @param values The blocksBuffers to add.
       * @return This builder for chaining.
       */
      public Builder addAllBlocksBuffers(
          java.lang.Iterable<? extends com.google.protobuf.ByteString> values) {
        ensureBlocksBuffersIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, blocksBuffers_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated bytes blocksBuffers = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearBlocksBuffers() {
        blocksBuffers_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000008);
        onChanged();
        return this;
      }

      private boolean skip_ ;
      /**
       * <code>required bool skip = 5 [default = false];</code>
       * @return Whether the skip field is set.
       */
      @java.lang.Override
      public boolean hasSkip() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>required bool skip = 5 [default = false];</code>
       * @return The skip.
       */
      @java.lang.Override
      public boolean getSkip() {
        return skip_;
      }
      /**
       * <code>required bool skip = 5 [default = false];</code>
       * @param value The skip to set.
       * @return This builder for chaining.
       */
      public Builder setSkip(boolean value) {
        bitField0_ |= 0x00000010;
        skip_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bool skip = 5 [default = false];</code>
       * @return This builder for chaining.
       */
      public Builder clearSkip() {
        bitField0_ = (bitField0_ & ~0x00000010);
        skip_ = false;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.BlockReportBucketProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.BlockReportBucketProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<BlockReportBucketProto>
        PARSER = new com.google.protobuf.AbstractParser<BlockReportBucketProto>() {
      @java.lang.Override
      public BlockReportBucketProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new BlockReportBucketProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<BlockReportBucketProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<BlockReportBucketProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface BlockReportProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.BlockReportProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto>
        getBucketsList();
    /**
     * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto getBuckets(int index);
    /**
     * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
     */
    int getBucketsCount();
    /**
     * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProtoOrBuilder>
        getBucketsOrBuilderList();
    /**
     * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProtoOrBuilder getBucketsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code org.apache.hadoop.datanode.BlockReportProto}
   */
  public static final class BlockReportProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.BlockReportProto)
      BlockReportProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use BlockReportProto.newBuilder() to construct.
    private BlockReportProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private BlockReportProto() {
      buckets_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new BlockReportProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private BlockReportProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                buckets_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              buckets_.add(
                  input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.PARSER, extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          buckets_ = java.util.Collections.unmodifiableList(buckets_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.Builder.class);
    }

    public static final int BUCKETS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto> buckets_;
    /**
     * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto> getBucketsList() {
      return buckets_;
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProtoOrBuilder>
        getBucketsOrBuilderList() {
      return buckets_;
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
     */
    @java.lang.Override
    public int getBucketsCount() {
      return buckets_.size();
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto getBuckets(int index) {
      return buckets_.get(index);
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProtoOrBuilder getBucketsOrBuilder(
        int index) {
      return buckets_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      for (int i = 0; i < getBucketsCount(); i++) {
        if (!getBuckets(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < buckets_.size(); i++) {
        output.writeMessage(1, buckets_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < buckets_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, buckets_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto) obj;

      if (!getBucketsList()
          .equals(other.getBucketsList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getBucketsCount() > 0) {
        hash = (37 * hash) + BUCKETS_FIELD_NUMBER;
        hash = (53 * hash) + getBucketsList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.hadoop.datanode.BlockReportProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.BlockReportProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getBucketsFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (bucketsBuilder_ == null) {
          buckets_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          bucketsBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto(this);
        int from_bitField0_ = bitField0_;
        if (bucketsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            buckets_ = java.util.Collections.unmodifiableList(buckets_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.buckets_ = buckets_;
        } else {
          result.buckets_ = bucketsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto.getDefaultInstance()) return this;
        if (bucketsBuilder_ == null) {
          if (!other.buckets_.isEmpty()) {
            if (buckets_.isEmpty()) {
              buckets_ = other.buckets_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureBucketsIsMutable();
              buckets_.addAll(other.buckets_);
            }
            onChanged();
          }
        } else {
          if (!other.buckets_.isEmpty()) {
            if (bucketsBuilder_.isEmpty()) {
              bucketsBuilder_.dispose();
              bucketsBuilder_ = null;
              buckets_ = other.buckets_;
              bitField0_ = (bitField0_ & ~0x00000001);
              bucketsBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getBucketsFieldBuilder() : null;
            } else {
              bucketsBuilder_.addAllMessages(other.buckets_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        for (int i = 0; i < getBucketsCount(); i++) {
          if (!getBuckets(i).isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto> buckets_ =
        java.util.Collections.emptyList();
      private void ensureBucketsIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          buckets_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto>(buckets_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProtoOrBuilder> bucketsBuilder_;

      /**
       * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto> getBucketsList() {
        if (bucketsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(buckets_);
        } else {
          return bucketsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
       */
      public int getBucketsCount() {
        if (bucketsBuilder_ == null) {
          return buckets_.size();
        } else {
          return bucketsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto getBuckets(int index) {
        if (bucketsBuilder_ == null) {
          return buckets_.get(index);
        } else {
          return bucketsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
       */
      public Builder setBuckets(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto value) {
        if (bucketsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBucketsIsMutable();
          buckets_.set(index, value);
          onChanged();
        } else {
          bucketsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
       */
      public Builder setBuckets(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.Builder builderForValue) {
        if (bucketsBuilder_ == null) {
          ensureBucketsIsMutable();
          buckets_.set(index, builderForValue.build());
          onChanged();
        } else {
          bucketsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
       */
      public Builder addBuckets(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto value) {
        if (bucketsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBucketsIsMutable();
          buckets_.add(value);
          onChanged();
        } else {
          bucketsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
       */
      public Builder addBuckets(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto value) {
        if (bucketsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBucketsIsMutable();
          buckets_.add(index, value);
          onChanged();
        } else {
          bucketsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
       */
      public Builder addBuckets(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.Builder builderForValue) {
        if (bucketsBuilder_ == null) {
          ensureBucketsIsMutable();
          buckets_.add(builderForValue.build());
          onChanged();
        } else {
          bucketsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
       */
      public Builder addBuckets(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.Builder builderForValue) {
        if (bucketsBuilder_ == null) {
          ensureBucketsIsMutable();
          buckets_.add(index, builderForValue.build());
          onChanged();
        } else {
          bucketsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
       */
      public Builder addAllBuckets(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto> values) {
        if (bucketsBuilder_ == null) {
          ensureBucketsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, buckets_);
          onChanged();
        } else {
          bucketsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
       */
      public Builder clearBuckets() {
        if (bucketsBuilder_ == null) {
          buckets_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          bucketsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
       */
      public Builder removeBuckets(int index) {
        if (bucketsBuilder_ == null) {
          ensureBucketsIsMutable();
          buckets_.remove(index);
          onChanged();
        } else {
          bucketsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.Builder getBucketsBuilder(
          int index) {
        return getBucketsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProtoOrBuilder getBucketsOrBuilder(
          int index) {
        if (bucketsBuilder_ == null) {
          return buckets_.get(index);  } else {
          return bucketsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProtoOrBuilder>
           getBucketsOrBuilderList() {
        if (bucketsBuilder_ != null) {
          return bucketsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(buckets_);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.Builder addBucketsBuilder() {
        return getBucketsFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.Builder addBucketsBuilder(
          int index) {
        return getBucketsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.BlockReportBucketProto buckets = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.Builder>
           getBucketsBuilderList() {
        return getBucketsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProtoOrBuilder>
          getBucketsFieldBuilder() {
        if (bucketsBuilder_ == null) {
          bucketsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportBucketProtoOrBuilder>(
                  buckets_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          buckets_ = null;
        }
        return bucketsBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.BlockReportProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.BlockReportProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<BlockReportProto>
        PARSER = new com.google.protobuf.AbstractParser<BlockReportProto>() {
      @java.lang.Override
      public BlockReportProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new BlockReportProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<BlockReportProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<BlockReportProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface BlockReportResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.BlockReportResponseProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
     * @return Whether the cmd field is set.
     */
    boolean hasCmd();
    /**
     * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
     * @return The cmd.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto getCmd();
    /**
     * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder getCmdOrBuilder();
  }
  /**
   * <pre>
   **
   * cmd - Command from namenode to the datanode
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.BlockReportResponseProto}
   */
  public static final class BlockReportResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.BlockReportResponseProto)
      BlockReportResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use BlockReportResponseProto.newBuilder() to construct.
    private BlockReportResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private BlockReportResponseProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new BlockReportResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private BlockReportResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = cmd_.toBuilder();
              }
              cmd_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(cmd_);
                cmd_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto.Builder.class);
    }

    private int bitField0_;
    public static final int CMD_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto cmd_;
    /**
     * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
     * @return Whether the cmd field is set.
     */
    @java.lang.Override
    public boolean hasCmd() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
     * @return The cmd.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto getCmd() {
      return cmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.getDefaultInstance() : cmd_;
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder getCmdOrBuilder() {
      return cmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.getDefaultInstance() : cmd_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (hasCmd()) {
        if (!getCmd().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getCmd());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getCmd());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto) obj;

      if (hasCmd() != other.hasCmd()) return false;
      if (hasCmd()) {
        if (!getCmd()
            .equals(other.getCmd())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasCmd()) {
        hash = (37 * hash) + CMD_FIELD_NUMBER;
        hash = (53 * hash) + getCmd().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * cmd - Command from namenode to the datanode
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.BlockReportResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.BlockReportResponseProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getCmdFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (cmdBuilder_ == null) {
          cmd_ = null;
        } else {
          cmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (cmdBuilder_ == null) {
            result.cmd_ = cmd_;
          } else {
            result.cmd_ = cmdBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto.getDefaultInstance()) return this;
        if (other.hasCmd()) {
          mergeCmd(other.getCmd());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (hasCmd()) {
          if (!getCmd().isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto cmd_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder> cmdBuilder_;
      /**
       * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
       * @return Whether the cmd field is set.
       */
      public boolean hasCmd() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
       * @return The cmd.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto getCmd() {
        if (cmdBuilder_ == null) {
          return cmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.getDefaultInstance() : cmd_;
        } else {
          return cmdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
       */
      public Builder setCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto value) {
        if (cmdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          cmd_ = value;
          onChanged();
        } else {
          cmdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
       */
      public Builder setCmd(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder builderForValue) {
        if (cmdBuilder_ == null) {
          cmd_ = builderForValue.build();
          onChanged();
        } else {
          cmdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
       */
      public Builder mergeCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto value) {
        if (cmdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              cmd_ != null &&
              cmd_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.getDefaultInstance()) {
            cmd_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.newBuilder(cmd_).mergeFrom(value).buildPartial();
          } else {
            cmd_ = value;
          }
          onChanged();
        } else {
          cmdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
       */
      public Builder clearCmd() {
        if (cmdBuilder_ == null) {
          cmd_ = null;
          onChanged();
        } else {
          cmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder getCmdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getCmdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder getCmdOrBuilder() {
        if (cmdBuilder_ != null) {
          return cmdBuilder_.getMessageOrBuilder();
        } else {
          return cmd_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.getDefaultInstance() : cmd_;
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder>
          getCmdFieldBuilder() {
        if (cmdBuilder_ == null) {
          cmdBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder>(
                  getCmd(),
                  getParentForChildren(),
                  isClean());
          cmd_ = null;
        }
        return cmdBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.BlockReportResponseProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.BlockReportResponseProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<BlockReportResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<BlockReportResponseProto>() {
      @java.lang.Override
      public BlockReportResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new BlockReportResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<BlockReportResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<BlockReportResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface CacheReportRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.CacheReportRequestProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return Whether the registration field is set.
     */
    boolean hasRegistration();
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return The registration.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration();
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder();

    /**
     * <code>required string blockPoolId = 2;</code>
     * @return Whether the blockPoolId field is set.
     */
    boolean hasBlockPoolId();
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The blockPoolId.
     */
    java.lang.String getBlockPoolId();
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The bytes for blockPoolId.
     */
    com.google.protobuf.ByteString
        getBlockPoolIdBytes();

    /**
     * <code>repeated uint64 blocks = 3 [packed = true];</code>
     * @return A list containing the blocks.
     */
    java.util.List<java.lang.Long> getBlocksList();
    /**
     * <code>repeated uint64 blocks = 3 [packed = true];</code>
     * @return The count of blocks.
     */
    int getBlocksCount();
    /**
     * <code>repeated uint64 blocks = 3 [packed = true];</code>
     * @param index The index of the element to return.
     * @return The blocks at the given index.
     */
    long getBlocks(int index);

    /**
     * <code>optional uint64 cacheCapacity = 4 [default = 0];</code>
     * @return Whether the cacheCapacity field is set.
     */
    boolean hasCacheCapacity();
    /**
     * <code>optional uint64 cacheCapacity = 4 [default = 0];</code>
     * @return The cacheCapacity.
     */
    long getCacheCapacity();

    /**
     * <code>optional uint64 cacheUsed = 5 [default = 0];</code>
     * @return Whether the cacheUsed field is set.
     */
    boolean hasCacheUsed();
    /**
     * <code>optional uint64 cacheUsed = 5 [default = 0];</code>
     * @return The cacheUsed.
     */
    long getCacheUsed();
  }
  /**
   * <pre>
   **
   * registration - datanode registration information
   * blockPoolId  - block pool ID of the reported blocks
   * blocks       - representation of blocks as longs for efficiency reasons
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.CacheReportRequestProto}
   */
  public static final class CacheReportRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.CacheReportRequestProto)
      CacheReportRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use CacheReportRequestProto.newBuilder() to construct.
    private CacheReportRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private CacheReportRequestProto() {
      blockPoolId_ = "";
      blocks_ = emptyLongList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new CacheReportRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private CacheReportRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = registration_.toBuilder();
              }
              registration_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(registration_);
                registration_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000002;
              blockPoolId_ = bs;
              break;
            }
            case 24: {
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                blocks_ = newLongList();
                mutable_bitField0_ |= 0x00000004;
              }
              blocks_.addLong(input.readUInt64());
              break;
            }
            case 26: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000004) != 0) && input.getBytesUntilLimit() > 0) {
                blocks_ = newLongList();
                mutable_bitField0_ |= 0x00000004;
              }
              while (input.getBytesUntilLimit() > 0) {
                blocks_.addLong(input.readUInt64());
              }
              input.popLimit(limit);
              break;
            }
            case 32: {
              bitField0_ |= 0x00000004;
              cacheCapacity_ = input.readUInt64();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000008;
              cacheUsed_ = input.readUInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          blocks_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int REGISTRATION_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto registration_;
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return Whether the registration field is set.
     */
    @java.lang.Override
    public boolean hasRegistration() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return The registration.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration() {
      return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
    }
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder() {
      return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
    }

    public static final int BLOCKPOOLID_FIELD_NUMBER = 2;
    private volatile java.lang.Object blockPoolId_;
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return Whether the blockPoolId field is set.
     */
    @java.lang.Override
    public boolean hasBlockPoolId() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The blockPoolId.
     */
    @java.lang.Override
    public java.lang.String getBlockPoolId() {
      java.lang.Object ref = blockPoolId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          blockPoolId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The bytes for blockPoolId.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getBlockPoolIdBytes() {
      java.lang.Object ref = blockPoolId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        blockPoolId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int BLOCKS_FIELD_NUMBER = 3;
    private com.google.protobuf.Internal.LongList blocks_;
    /**
     * <code>repeated uint64 blocks = 3 [packed = true];</code>
     * @return A list containing the blocks.
     */
    @java.lang.Override
    public java.util.List<java.lang.Long>
        getBlocksList() {
      return blocks_;
    }
    /**
     * <code>repeated uint64 blocks = 3 [packed = true];</code>
     * @return The count of blocks.
     */
    public int getBlocksCount() {
      return blocks_.size();
    }
    /**
     * <code>repeated uint64 blocks = 3 [packed = true];</code>
     * @param index The index of the element to return.
     * @return The blocks at the given index.
     */
    public long getBlocks(int index) {
      return blocks_.getLong(index);
    }
    private int blocksMemoizedSerializedSize = -1;

    public static final int CACHECAPACITY_FIELD_NUMBER = 4;
    private long cacheCapacity_;
    /**
     * <code>optional uint64 cacheCapacity = 4 [default = 0];</code>
     * @return Whether the cacheCapacity field is set.
     */
    @java.lang.Override
    public boolean hasCacheCapacity() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional uint64 cacheCapacity = 4 [default = 0];</code>
     * @return The cacheCapacity.
     */
    @java.lang.Override
    public long getCacheCapacity() {
      return cacheCapacity_;
    }

    public static final int CACHEUSED_FIELD_NUMBER = 5;
    private long cacheUsed_;
    /**
     * <code>optional uint64 cacheUsed = 5 [default = 0];</code>
     * @return Whether the cacheUsed field is set.
     */
    @java.lang.Override
    public boolean hasCacheUsed() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional uint64 cacheUsed = 5 [default = 0];</code>
     * @return The cacheUsed.
     */
    @java.lang.Override
    public long getCacheUsed() {
      return cacheUsed_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasRegistration()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasBlockPoolId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getRegistration().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getRegistration());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, blockPoolId_);
      }
      if (getBlocksList().size() > 0) {
        output.writeUInt32NoTag(26);
        output.writeUInt32NoTag(blocksMemoizedSerializedSize);
      }
      for (int i = 0; i < blocks_.size(); i++) {
        output.writeUInt64NoTag(blocks_.getLong(i));
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeUInt64(4, cacheCapacity_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeUInt64(5, cacheUsed_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getRegistration());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, blockPoolId_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < blocks_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeUInt64SizeNoTag(blocks_.getLong(i));
        }
        size += dataSize;
        if (!getBlocksList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        blocksMemoizedSerializedSize = dataSize;
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(4, cacheCapacity_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(5, cacheUsed_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto) obj;

      if (hasRegistration() != other.hasRegistration()) return false;
      if (hasRegistration()) {
        if (!getRegistration()
            .equals(other.getRegistration())) return false;
      }
      if (hasBlockPoolId() != other.hasBlockPoolId()) return false;
      if (hasBlockPoolId()) {
        if (!getBlockPoolId()
            .equals(other.getBlockPoolId())) return false;
      }
      if (!getBlocksList()
          .equals(other.getBlocksList())) return false;
      if (hasCacheCapacity() != other.hasCacheCapacity()) return false;
      if (hasCacheCapacity()) {
        if (getCacheCapacity()
            != other.getCacheCapacity()) return false;
      }
      if (hasCacheUsed() != other.hasCacheUsed()) return false;
      if (hasCacheUsed()) {
        if (getCacheUsed()
            != other.getCacheUsed()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasRegistration()) {
        hash = (37 * hash) + REGISTRATION_FIELD_NUMBER;
        hash = (53 * hash) + getRegistration().hashCode();
      }
      if (hasBlockPoolId()) {
        hash = (37 * hash) + BLOCKPOOLID_FIELD_NUMBER;
        hash = (53 * hash) + getBlockPoolId().hashCode();
      }
      if (getBlocksCount() > 0) {
        hash = (37 * hash) + BLOCKS_FIELD_NUMBER;
        hash = (53 * hash) + getBlocksList().hashCode();
      }
      if (hasCacheCapacity()) {
        hash = (37 * hash) + CACHECAPACITY_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getCacheCapacity());
      }
      if (hasCacheUsed()) {
        hash = (37 * hash) + CACHEUSED_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getCacheUsed());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * registration - datanode registration information
     * blockPoolId  - block pool ID of the reported blocks
     * blocks       - representation of blocks as longs for efficiency reasons
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.CacheReportRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.CacheReportRequestProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getRegistrationFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (registrationBuilder_ == null) {
          registration_ = null;
        } else {
          registrationBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        blockPoolId_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        blocks_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000004);
        cacheCapacity_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        cacheUsed_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (registrationBuilder_ == null) {
            result.registration_ = registration_;
          } else {
            result.registration_ = registrationBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.blockPoolId_ = blockPoolId_;
        if (((bitField0_ & 0x00000004) != 0)) {
          blocks_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000004);
        }
        result.blocks_ = blocks_;
        if (((from_bitField0_ & 0x00000008) != 0)) {
          result.cacheCapacity_ = cacheCapacity_;
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000010) != 0)) {
          result.cacheUsed_ = cacheUsed_;
          to_bitField0_ |= 0x00000008;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto.getDefaultInstance()) return this;
        if (other.hasRegistration()) {
          mergeRegistration(other.getRegistration());
        }
        if (other.hasBlockPoolId()) {
          bitField0_ |= 0x00000002;
          blockPoolId_ = other.blockPoolId_;
          onChanged();
        }
        if (!other.blocks_.isEmpty()) {
          if (blocks_.isEmpty()) {
            blocks_ = other.blocks_;
            bitField0_ = (bitField0_ & ~0x00000004);
          } else {
            ensureBlocksIsMutable();
            blocks_.addAll(other.blocks_);
          }
          onChanged();
        }
        if (other.hasCacheCapacity()) {
          setCacheCapacity(other.getCacheCapacity());
        }
        if (other.hasCacheUsed()) {
          setCacheUsed(other.getCacheUsed());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasRegistration()) {
          return false;
        }
        if (!hasBlockPoolId()) {
          return false;
        }
        if (!getRegistration().isInitialized()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto registration_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder> registrationBuilder_;
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       * @return Whether the registration field is set.
       */
      public boolean hasRegistration() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       * @return The registration.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration() {
        if (registrationBuilder_ == null) {
          return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
        } else {
          return registrationBuilder_.getMessage();
        }
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder setRegistration(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto value) {
        if (registrationBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          registration_ = value;
          onChanged();
        } else {
          registrationBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder setRegistration(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder builderForValue) {
        if (registrationBuilder_ == null) {
          registration_ = builderForValue.build();
          onChanged();
        } else {
          registrationBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder mergeRegistration(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto value) {
        if (registrationBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              registration_ != null &&
              registration_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance()) {
            registration_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.newBuilder(registration_).mergeFrom(value).buildPartial();
          } else {
            registration_ = value;
          }
          onChanged();
        } else {
          registrationBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder clearRegistration() {
        if (registrationBuilder_ == null) {
          registration_ = null;
          onChanged();
        } else {
          registrationBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder getRegistrationBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getRegistrationFieldBuilder().getBuilder();
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder() {
        if (registrationBuilder_ != null) {
          return registrationBuilder_.getMessageOrBuilder();
        } else {
          return registration_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
        }
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder>
          getRegistrationFieldBuilder() {
        if (registrationBuilder_ == null) {
          registrationBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder>(
                  getRegistration(),
                  getParentForChildren(),
                  isClean());
          registration_ = null;
        }
        return registrationBuilder_;
      }

      private java.lang.Object blockPoolId_ = "";
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return Whether the blockPoolId field is set.
       */
      public boolean hasBlockPoolId() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return The blockPoolId.
       */
      public java.lang.String getBlockPoolId() {
        java.lang.Object ref = blockPoolId_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            blockPoolId_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return The bytes for blockPoolId.
       */
      public com.google.protobuf.ByteString
          getBlockPoolIdBytes() {
        java.lang.Object ref = blockPoolId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          blockPoolId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @param value The blockPoolId to set.
       * @return This builder for chaining.
       */
      public Builder setBlockPoolId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        blockPoolId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearBlockPoolId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        blockPoolId_ = getDefaultInstance().getBlockPoolId();
        onChanged();
        return this;
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @param value The bytes for blockPoolId to set.
       * @return This builder for chaining.
       */
      public Builder setBlockPoolIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        blockPoolId_ = value;
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.LongList blocks_ = emptyLongList();
      private void ensureBlocksIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          blocks_ = mutableCopy(blocks_);
          bitField0_ |= 0x00000004;
         }
      }
      /**
       * <code>repeated uint64 blocks = 3 [packed = true];</code>
       * @return A list containing the blocks.
       */
      public java.util.List<java.lang.Long>
          getBlocksList() {
        return ((bitField0_ & 0x00000004) != 0) ?
                 java.util.Collections.unmodifiableList(blocks_) : blocks_;
      }
      /**
       * <code>repeated uint64 blocks = 3 [packed = true];</code>
       * @return The count of blocks.
       */
      public int getBlocksCount() {
        return blocks_.size();
      }
      /**
       * <code>repeated uint64 blocks = 3 [packed = true];</code>
       * @param index The index of the element to return.
       * @return The blocks at the given index.
       */
      public long getBlocks(int index) {
        return blocks_.getLong(index);
      }
      /**
       * <code>repeated uint64 blocks = 3 [packed = true];</code>
       * @param index The index to set the value at.
       * @param value The blocks to set.
       * @return This builder for chaining.
       */
      public Builder setBlocks(
          int index, long value) {
        ensureBlocksIsMutable();
        blocks_.setLong(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint64 blocks = 3 [packed = true];</code>
       * @param value The blocks to add.
       * @return This builder for chaining.
       */
      public Builder addBlocks(long value) {
        ensureBlocksIsMutable();
        blocks_.addLong(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint64 blocks = 3 [packed = true];</code>
       * @param values The blocks to add.
       * @return This builder for chaining.
       */
      public Builder addAllBlocks(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureBlocksIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, blocks_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint64 blocks = 3 [packed = true];</code>
       * @return This builder for chaining.
       */
      public Builder clearBlocks() {
        blocks_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000004);
        onChanged();
        return this;
      }

      private long cacheCapacity_ ;
      /**
       * <code>optional uint64 cacheCapacity = 4 [default = 0];</code>
       * @return Whether the cacheCapacity field is set.
       */
      @java.lang.Override
      public boolean hasCacheCapacity() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional uint64 cacheCapacity = 4 [default = 0];</code>
       * @return The cacheCapacity.
       */
      @java.lang.Override
      public long getCacheCapacity() {
        return cacheCapacity_;
      }
      /**
       * <code>optional uint64 cacheCapacity = 4 [default = 0];</code>
       * @param value The cacheCapacity to set.
       * @return This builder for chaining.
       */
      public Builder setCacheCapacity(long value) {
        bitField0_ |= 0x00000008;
        cacheCapacity_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 cacheCapacity = 4 [default = 0];</code>
       * @return This builder for chaining.
       */
      public Builder clearCacheCapacity() {
        bitField0_ = (bitField0_ & ~0x00000008);
        cacheCapacity_ = 0L;
        onChanged();
        return this;
      }

      private long cacheUsed_ ;
      /**
       * <code>optional uint64 cacheUsed = 5 [default = 0];</code>
       * @return Whether the cacheUsed field is set.
       */
      @java.lang.Override
      public boolean hasCacheUsed() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>optional uint64 cacheUsed = 5 [default = 0];</code>
       * @return The cacheUsed.
       */
      @java.lang.Override
      public long getCacheUsed() {
        return cacheUsed_;
      }
      /**
       * <code>optional uint64 cacheUsed = 5 [default = 0];</code>
       * @param value The cacheUsed to set.
       * @return This builder for chaining.
       */
      public Builder setCacheUsed(long value) {
        bitField0_ |= 0x00000010;
        cacheUsed_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 cacheUsed = 5 [default = 0];</code>
       * @return This builder for chaining.
       */
      public Builder clearCacheUsed() {
        bitField0_ = (bitField0_ & ~0x00000010);
        cacheUsed_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.CacheReportRequestProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.CacheReportRequestProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<CacheReportRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<CacheReportRequestProto>() {
      @java.lang.Override
      public CacheReportRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new CacheReportRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<CacheReportRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<CacheReportRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface CacheReportResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.CacheReportResponseProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
     * @return Whether the cmd field is set.
     */
    boolean hasCmd();
    /**
     * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
     * @return The cmd.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto getCmd();
    /**
     * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder getCmdOrBuilder();
  }
  /**
   * Protobuf type {@code org.apache.hadoop.datanode.CacheReportResponseProto}
   */
  public static final class CacheReportResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.CacheReportResponseProto)
      CacheReportResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use CacheReportResponseProto.newBuilder() to construct.
    private CacheReportResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private CacheReportResponseProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new CacheReportResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private CacheReportResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = cmd_.toBuilder();
              }
              cmd_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(cmd_);
                cmd_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto.Builder.class);
    }

    private int bitField0_;
    public static final int CMD_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto cmd_;
    /**
     * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
     * @return Whether the cmd field is set.
     */
    @java.lang.Override
    public boolean hasCmd() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
     * @return The cmd.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto getCmd() {
      return cmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.getDefaultInstance() : cmd_;
    }
    /**
     * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder getCmdOrBuilder() {
      return cmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.getDefaultInstance() : cmd_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (hasCmd()) {
        if (!getCmd().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getCmd());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getCmd());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto) obj;

      if (hasCmd() != other.hasCmd()) return false;
      if (hasCmd()) {
        if (!getCmd()
            .equals(other.getCmd())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasCmd()) {
        hash = (37 * hash) + CMD_FIELD_NUMBER;
        hash = (53 * hash) + getCmd().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.hadoop.datanode.CacheReportResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.CacheReportResponseProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getCmdFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (cmdBuilder_ == null) {
          cmd_ = null;
        } else {
          cmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (cmdBuilder_ == null) {
            result.cmd_ = cmd_;
          } else {
            result.cmd_ = cmdBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto.getDefaultInstance()) return this;
        if (other.hasCmd()) {
          mergeCmd(other.getCmd());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (hasCmd()) {
          if (!getCmd().isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto cmd_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder> cmdBuilder_;
      /**
       * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
       * @return Whether the cmd field is set.
       */
      public boolean hasCmd() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
       * @return The cmd.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto getCmd() {
        if (cmdBuilder_ == null) {
          return cmd_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.getDefaultInstance() : cmd_;
        } else {
          return cmdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
       */
      public Builder setCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto value) {
        if (cmdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          cmd_ = value;
          onChanged();
        } else {
          cmdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
       */
      public Builder setCmd(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder builderForValue) {
        if (cmdBuilder_ == null) {
          cmd_ = builderForValue.build();
          onChanged();
        } else {
          cmdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
       */
      public Builder mergeCmd(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto value) {
        if (cmdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              cmd_ != null &&
              cmd_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.getDefaultInstance()) {
            cmd_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.newBuilder(cmd_).mergeFrom(value).buildPartial();
          } else {
            cmd_ = value;
          }
          onChanged();
        } else {
          cmdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
       */
      public Builder clearCmd() {
        if (cmdBuilder_ == null) {
          cmd_ = null;
          onChanged();
        } else {
          cmdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder getCmdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getCmdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder getCmdOrBuilder() {
        if (cmdBuilder_ != null) {
          return cmdBuilder_.getMessageOrBuilder();
        } else {
          return cmd_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.getDefaultInstance() : cmd_;
        }
      }
      /**
       * <code>optional .org.apache.hadoop.datanode.DatanodeCommandProto cmd = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder>
          getCmdFieldBuilder() {
        if (cmdBuilder_ == null) {
          cmdBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeCommandProtoOrBuilder>(
                  getCmd(),
                  getParentForChildren(),
                  isClean());
          cmd_ = null;
        }
        return cmdBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.CacheReportResponseProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.CacheReportResponseProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<CacheReportResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<CacheReportResponseProto>() {
      @java.lang.Override
      public CacheReportResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new CacheReportResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<CacheReportResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<CacheReportResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ReceivedDeletedBlockInfoProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required .org.apache.hadoop.BlockProto block = 1;</code>
     * @return Whether the block field is set.
     */
    boolean hasBlock();
    /**
     * <code>required .org.apache.hadoop.BlockProto block = 1;</code>
     * @return The block.
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto getBlock();
    /**
     * <code>required .org.apache.hadoop.BlockProto block = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProtoOrBuilder getBlockOrBuilder();

    /**
     * <code>required .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto.BlockStatus status = 3;</code>
     * @return Whether the status field is set.
     */
    boolean hasStatus();
    /**
     * <code>required .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto.BlockStatus status = 3;</code>
     * @return The status.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.BlockStatus getStatus();

    /**
     * <code>optional string deleteHint = 2;</code>
     * @return Whether the deleteHint field is set.
     */
    boolean hasDeleteHint();
    /**
     * <code>optional string deleteHint = 2;</code>
     * @return The deleteHint.
     */
    java.lang.String getDeleteHint();
    /**
     * <code>optional string deleteHint = 2;</code>
     * @return The bytes for deleteHint.
     */
    com.google.protobuf.ByteString
        getDeleteHintBytes();
  }
  /**
   * <pre>
   **
   * Data structure to send received or deleted block information
   * from datanode to namenode.
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto}
   */
  public static final class ReceivedDeletedBlockInfoProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto)
      ReceivedDeletedBlockInfoProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ReceivedDeletedBlockInfoProto.newBuilder() to construct.
    private ReceivedDeletedBlockInfoProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ReceivedDeletedBlockInfoProto() {
      status_ = 1;
      deleteHint_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ReceivedDeletedBlockInfoProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ReceivedDeletedBlockInfoProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = block_.toBuilder();
              }
              block_ = input.readMessage(org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(block_);
                block_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000004;
              deleteHint_ = bs;
              break;
            }
            case 24: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.BlockStatus value = org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.BlockStatus.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(3, rawValue);
              } else {
                bitField0_ |= 0x00000002;
                status_ = rawValue;
              }
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReceivedDeletedBlockInfoProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReceivedDeletedBlockInfoProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.Builder.class);
    }

    /**
     * Protobuf enum {@code org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto.BlockStatus}
     */
    public enum BlockStatus
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <code>CREATING = 1;</code>
       */
      CREATING(1),
      /**
       * <code>APPENDING = 2;</code>
       */
      APPENDING(2),
      /**
       * <code>RECOVERING_APPEND = 3;</code>
       */
      RECOVERING_APPEND(3),
      /**
       * <code>RECEIVED = 4;</code>
       */
      RECEIVED(4),
      /**
       * <code>UPDATE_RECOVERED = 5;</code>
       */
      UPDATE_RECOVERED(5),
      /**
       * <code>DELETED = 6;</code>
       */
      DELETED(6),
      ;

      /**
       * <code>CREATING = 1;</code>
       */
      public static final int CREATING_VALUE = 1;
      /**
       * <code>APPENDING = 2;</code>
       */
      public static final int APPENDING_VALUE = 2;
      /**
       * <code>RECOVERING_APPEND = 3;</code>
       */
      public static final int RECOVERING_APPEND_VALUE = 3;
      /**
       * <code>RECEIVED = 4;</code>
       */
      public static final int RECEIVED_VALUE = 4;
      /**
       * <code>UPDATE_RECOVERED = 5;</code>
       */
      public static final int UPDATE_RECOVERED_VALUE = 5;
      /**
       * <code>DELETED = 6;</code>
       */
      public static final int DELETED_VALUE = 6;


      public final int getNumber() {
        return value;
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static BlockStatus valueOf(int value) {
        return forNumber(value);
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       */
      public static BlockStatus forNumber(int value) {
        switch (value) {
          case 1: return CREATING;
          case 2: return APPENDING;
          case 3: return RECOVERING_APPEND;
          case 4: return RECEIVED;
          case 5: return UPDATE_RECOVERED;
          case 6: return DELETED;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<BlockStatus>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          BlockStatus> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<BlockStatus>() {
              public BlockStatus findValueByNumber(int number) {
                return BlockStatus.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.getDescriptor().getEnumTypes().get(0);
      }

      private static final BlockStatus[] VALUES = values();

      public static BlockStatus valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private BlockStatus(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto.BlockStatus)
    }

    private int bitField0_;
    public static final int BLOCK_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto block_;
    /**
     * <code>required .org.apache.hadoop.BlockProto block = 1;</code>
     * @return Whether the block field is set.
     */
    @java.lang.Override
    public boolean hasBlock() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required .org.apache.hadoop.BlockProto block = 1;</code>
     * @return The block.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto getBlock() {
      return block_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.getDefaultInstance() : block_;
    }
    /**
     * <code>required .org.apache.hadoop.BlockProto block = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProtoOrBuilder getBlockOrBuilder() {
      return block_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.getDefaultInstance() : block_;
    }

    public static final int STATUS_FIELD_NUMBER = 3;
    private int status_;
    /**
     * <code>required .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto.BlockStatus status = 3;</code>
     * @return Whether the status field is set.
     */
    @java.lang.Override public boolean hasStatus() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>required .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto.BlockStatus status = 3;</code>
     * @return The status.
     */
    @java.lang.Override public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.BlockStatus getStatus() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.BlockStatus result = org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.BlockStatus.valueOf(status_);
      return result == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.BlockStatus.CREATING : result;
    }

    public static final int DELETEHINT_FIELD_NUMBER = 2;
    private volatile java.lang.Object deleteHint_;
    /**
     * <code>optional string deleteHint = 2;</code>
     * @return Whether the deleteHint field is set.
     */
    @java.lang.Override
    public boolean hasDeleteHint() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional string deleteHint = 2;</code>
     * @return The deleteHint.
     */
    @java.lang.Override
    public java.lang.String getDeleteHint() {
      java.lang.Object ref = deleteHint_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          deleteHint_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string deleteHint = 2;</code>
     * @return The bytes for deleteHint.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getDeleteHintBytes() {
      java.lang.Object ref = deleteHint_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        deleteHint_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasBlock()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasStatus()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getBlock().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getBlock());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, deleteHint_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeEnum(3, status_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getBlock());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, deleteHint_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(3, status_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto) obj;

      if (hasBlock() != other.hasBlock()) return false;
      if (hasBlock()) {
        if (!getBlock()
            .equals(other.getBlock())) return false;
      }
      if (hasStatus() != other.hasStatus()) return false;
      if (hasStatus()) {
        if (status_ != other.status_) return false;
      }
      if (hasDeleteHint() != other.hasDeleteHint()) return false;
      if (hasDeleteHint()) {
        if (!getDeleteHint()
            .equals(other.getDeleteHint())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasBlock()) {
        hash = (37 * hash) + BLOCK_FIELD_NUMBER;
        hash = (53 * hash) + getBlock().hashCode();
      }
      if (hasStatus()) {
        hash = (37 * hash) + STATUS_FIELD_NUMBER;
        hash = (53 * hash) + status_;
      }
      if (hasDeleteHint()) {
        hash = (37 * hash) + DELETEHINT_FIELD_NUMBER;
        hash = (53 * hash) + getDeleteHint().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * Data structure to send received or deleted block information
     * from datanode to namenode.
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReceivedDeletedBlockInfoProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReceivedDeletedBlockInfoProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getBlockFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (blockBuilder_ == null) {
          block_ = null;
        } else {
          blockBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        status_ = 1;
        bitField0_ = (bitField0_ & ~0x00000002);
        deleteHint_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReceivedDeletedBlockInfoProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (blockBuilder_ == null) {
            result.block_ = block_;
          } else {
            result.block_ = blockBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.status_ = status_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.deleteHint_ = deleteHint_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.getDefaultInstance()) return this;
        if (other.hasBlock()) {
          mergeBlock(other.getBlock());
        }
        if (other.hasStatus()) {
          setStatus(other.getStatus());
        }
        if (other.hasDeleteHint()) {
          bitField0_ |= 0x00000004;
          deleteHint_ = other.deleteHint_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasBlock()) {
          return false;
        }
        if (!hasStatus()) {
          return false;
        }
        if (!getBlock().isInitialized()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto block_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProtoOrBuilder> blockBuilder_;
      /**
       * <code>required .org.apache.hadoop.BlockProto block = 1;</code>
       * @return Whether the block field is set.
       */
      public boolean hasBlock() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required .org.apache.hadoop.BlockProto block = 1;</code>
       * @return The block.
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto getBlock() {
        if (blockBuilder_ == null) {
          return block_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.getDefaultInstance() : block_;
        } else {
          return blockBuilder_.getMessage();
        }
      }
      /**
       * <code>required .org.apache.hadoop.BlockProto block = 1;</code>
       */
      public Builder setBlock(org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto value) {
        if (blockBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          block_ = value;
          onChanged();
        } else {
          blockBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.BlockProto block = 1;</code>
       */
      public Builder setBlock(
          org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.Builder builderForValue) {
        if (blockBuilder_ == null) {
          block_ = builderForValue.build();
          onChanged();
        } else {
          blockBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.BlockProto block = 1;</code>
       */
      public Builder mergeBlock(org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto value) {
        if (blockBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              block_ != null &&
              block_ != org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.getDefaultInstance()) {
            block_ =
              org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.newBuilder(block_).mergeFrom(value).buildPartial();
          } else {
            block_ = value;
          }
          onChanged();
        } else {
          blockBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.BlockProto block = 1;</code>
       */
      public Builder clearBlock() {
        if (blockBuilder_ == null) {
          block_ = null;
          onChanged();
        } else {
          blockBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.BlockProto block = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.Builder getBlockBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getBlockFieldBuilder().getBuilder();
      }
      /**
       * <code>required .org.apache.hadoop.BlockProto block = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProtoOrBuilder getBlockOrBuilder() {
        if (blockBuilder_ != null) {
          return blockBuilder_.getMessageOrBuilder();
        } else {
          return block_ == null ?
              org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.getDefaultInstance() : block_;
        }
      }
      /**
       * <code>required .org.apache.hadoop.BlockProto block = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProtoOrBuilder>
          getBlockFieldBuilder() {
        if (blockBuilder_ == null) {
          blockBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.BlockProtoOrBuilder>(
                  getBlock(),
                  getParentForChildren(),
                  isClean());
          block_ = null;
        }
        return blockBuilder_;
      }

      private int status_ = 1;
      /**
       * <code>required .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto.BlockStatus status = 3;</code>
       * @return Whether the status field is set.
       */
      @java.lang.Override public boolean hasStatus() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>required .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto.BlockStatus status = 3;</code>
       * @return The status.
       */
      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.BlockStatus getStatus() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.BlockStatus result = org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.BlockStatus.valueOf(status_);
        return result == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.BlockStatus.CREATING : result;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto.BlockStatus status = 3;</code>
       * @param value The status to set.
       * @return This builder for chaining.
       */
      public Builder setStatus(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.BlockStatus value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000002;
        status_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto.BlockStatus status = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearStatus() {
        bitField0_ = (bitField0_ & ~0x00000002);
        status_ = 1;
        onChanged();
        return this;
      }

      private java.lang.Object deleteHint_ = "";
      /**
       * <code>optional string deleteHint = 2;</code>
       * @return Whether the deleteHint field is set.
       */
      public boolean hasDeleteHint() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional string deleteHint = 2;</code>
       * @return The deleteHint.
       */
      public java.lang.String getDeleteHint() {
        java.lang.Object ref = deleteHint_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            deleteHint_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string deleteHint = 2;</code>
       * @return The bytes for deleteHint.
       */
      public com.google.protobuf.ByteString
          getDeleteHintBytes() {
        java.lang.Object ref = deleteHint_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          deleteHint_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string deleteHint = 2;</code>
       * @param value The deleteHint to set.
       * @return This builder for chaining.
       */
      public Builder setDeleteHint(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        deleteHint_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string deleteHint = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearDeleteHint() {
        bitField0_ = (bitField0_ & ~0x00000004);
        deleteHint_ = getDefaultInstance().getDeleteHint();
        onChanged();
        return this;
      }
      /**
       * <code>optional string deleteHint = 2;</code>
       * @param value The bytes for deleteHint to set.
       * @return This builder for chaining.
       */
      public Builder setDeleteHintBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        deleteHint_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<ReceivedDeletedBlockInfoProto>
        PARSER = new com.google.protobuf.AbstractParser<ReceivedDeletedBlockInfoProto>() {
      @java.lang.Override
      public ReceivedDeletedBlockInfoProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ReceivedDeletedBlockInfoProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ReceivedDeletedBlockInfoProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ReceivedDeletedBlockInfoProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface StorageReceivedDeletedBlocksProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required string storageUuid = 1 [deprecated = true];</code>
     * @return Whether the storageUuid field is set.
     */
    @java.lang.Deprecated boolean hasStorageUuid();
    /**
     * <code>required string storageUuid = 1 [deprecated = true];</code>
     * @return The storageUuid.
     */
    @java.lang.Deprecated java.lang.String getStorageUuid();
    /**
     * <code>required string storageUuid = 1 [deprecated = true];</code>
     * @return The bytes for storageUuid.
     */
    @java.lang.Deprecated com.google.protobuf.ByteString
        getStorageUuidBytes();

    /**
     * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto>
        getBlocksList();
    /**
     * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto getBlocks(int index);
    /**
     * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
     */
    int getBlocksCount();
    /**
     * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProtoOrBuilder>
        getBlocksOrBuilderList();
    /**
     * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProtoOrBuilder getBlocksOrBuilder(
        int index);

    /**
     * <pre>
     * supersedes storageUuid.
     * </pre>
     *
     * <code>optional .org.apache.hadoop.DatanodeStorageProto storage = 3;</code>
     * @return Whether the storage field is set.
     */
    boolean hasStorage();
    /**
     * <pre>
     * supersedes storageUuid.
     * </pre>
     *
     * <code>optional .org.apache.hadoop.DatanodeStorageProto storage = 3;</code>
     * @return The storage.
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto getStorage();
    /**
     * <pre>
     * supersedes storageUuid.
     * </pre>
     *
     * <code>optional .org.apache.hadoop.DatanodeStorageProto storage = 3;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder getStorageOrBuilder();
  }
  /**
   * <pre>
   **
   * List of blocks received and deleted for a storage.
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto}
   */
  public static final class StorageReceivedDeletedBlocksProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto)
      StorageReceivedDeletedBlocksProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use StorageReceivedDeletedBlocksProto.newBuilder() to construct.
    private StorageReceivedDeletedBlocksProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private StorageReceivedDeletedBlocksProto() {
      storageUuid_ = "";
      blocks_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new StorageReceivedDeletedBlocksProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private StorageReceivedDeletedBlocksProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              storageUuid_ = bs;
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                blocks_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              blocks_.add(
                  input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.PARSER, extensionRegistry));
              break;
            }
            case 26: {
              org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) != 0)) {
                subBuilder = storage_.toBuilder();
              }
              storage_ = input.readMessage(org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(storage_);
                storage_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          blocks_ = java.util.Collections.unmodifiableList(blocks_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageReceivedDeletedBlocksProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageReceivedDeletedBlocksProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.Builder.class);
    }

    private int bitField0_;
    public static final int STORAGEUUID_FIELD_NUMBER = 1;
    private volatile java.lang.Object storageUuid_;
    /**
     * <code>required string storageUuid = 1 [deprecated = true];</code>
     * @return Whether the storageUuid field is set.
     */
    @java.lang.Override
    @java.lang.Deprecated public boolean hasStorageUuid() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required string storageUuid = 1 [deprecated = true];</code>
     * @return The storageUuid.
     */
    @java.lang.Override
    @java.lang.Deprecated public java.lang.String getStorageUuid() {
      java.lang.Object ref = storageUuid_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          storageUuid_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string storageUuid = 1 [deprecated = true];</code>
     * @return The bytes for storageUuid.
     */
    @java.lang.Override
    @java.lang.Deprecated public com.google.protobuf.ByteString
        getStorageUuidBytes() {
      java.lang.Object ref = storageUuid_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        storageUuid_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int BLOCKS_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto> blocks_;
    /**
     * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto> getBlocksList() {
      return blocks_;
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProtoOrBuilder>
        getBlocksOrBuilderList() {
      return blocks_;
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
     */
    @java.lang.Override
    public int getBlocksCount() {
      return blocks_.size();
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto getBlocks(int index) {
      return blocks_.get(index);
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProtoOrBuilder getBlocksOrBuilder(
        int index) {
      return blocks_.get(index);
    }

    public static final int STORAGE_FIELD_NUMBER = 3;
    private org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto storage_;
    /**
     * <pre>
     * supersedes storageUuid.
     * </pre>
     *
     * <code>optional .org.apache.hadoop.DatanodeStorageProto storage = 3;</code>
     * @return Whether the storage field is set.
     */
    @java.lang.Override
    public boolean hasStorage() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <pre>
     * supersedes storageUuid.
     * </pre>
     *
     * <code>optional .org.apache.hadoop.DatanodeStorageProto storage = 3;</code>
     * @return The storage.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto getStorage() {
      return storage_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.getDefaultInstance() : storage_;
    }
    /**
     * <pre>
     * supersedes storageUuid.
     * </pre>
     *
     * <code>optional .org.apache.hadoop.DatanodeStorageProto storage = 3;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder getStorageOrBuilder() {
      return storage_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.getDefaultInstance() : storage_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasStorageUuid()) {
        memoizedIsInitialized = 0;
        return false;
      }
      for (int i = 0; i < getBlocksCount(); i++) {
        if (!getBlocks(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasStorage()) {
        if (!getStorage().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, storageUuid_);
      }
      for (int i = 0; i < blocks_.size(); i++) {
        output.writeMessage(2, blocks_.get(i));
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeMessage(3, getStorage());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, storageUuid_);
      }
      for (int i = 0; i < blocks_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, blocks_.get(i));
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, getStorage());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto) obj;

      if (hasStorageUuid() != other.hasStorageUuid()) return false;
      if (hasStorageUuid()) {
        if (!getStorageUuid()
            .equals(other.getStorageUuid())) return false;
      }
      if (!getBlocksList()
          .equals(other.getBlocksList())) return false;
      if (hasStorage() != other.hasStorage()) return false;
      if (hasStorage()) {
        if (!getStorage()
            .equals(other.getStorage())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasStorageUuid()) {
        hash = (37 * hash) + STORAGEUUID_FIELD_NUMBER;
        hash = (53 * hash) + getStorageUuid().hashCode();
      }
      if (getBlocksCount() > 0) {
        hash = (37 * hash) + BLOCKS_FIELD_NUMBER;
        hash = (53 * hash) + getBlocksList().hashCode();
      }
      if (hasStorage()) {
        hash = (37 * hash) + STORAGE_FIELD_NUMBER;
        hash = (53 * hash) + getStorage().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * List of blocks received and deleted for a storage.
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageReceivedDeletedBlocksProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageReceivedDeletedBlocksProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getBlocksFieldBuilder();
          getStorageFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        storageUuid_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        if (blocksBuilder_ == null) {
          blocks_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          blocksBuilder_.clear();
        }
        if (storageBuilder_ == null) {
          storage_ = null;
        } else {
          storageBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageReceivedDeletedBlocksProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.storageUuid_ = storageUuid_;
        if (blocksBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0)) {
            blocks_ = java.util.Collections.unmodifiableList(blocks_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.blocks_ = blocks_;
        } else {
          result.blocks_ = blocksBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          if (storageBuilder_ == null) {
            result.storage_ = storage_;
          } else {
            result.storage_ = storageBuilder_.build();
          }
          to_bitField0_ |= 0x00000002;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.getDefaultInstance()) return this;
        if (other.hasStorageUuid()) {
          bitField0_ |= 0x00000001;
          storageUuid_ = other.storageUuid_;
          onChanged();
        }
        if (blocksBuilder_ == null) {
          if (!other.blocks_.isEmpty()) {
            if (blocks_.isEmpty()) {
              blocks_ = other.blocks_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureBlocksIsMutable();
              blocks_.addAll(other.blocks_);
            }
            onChanged();
          }
        } else {
          if (!other.blocks_.isEmpty()) {
            if (blocksBuilder_.isEmpty()) {
              blocksBuilder_.dispose();
              blocksBuilder_ = null;
              blocks_ = other.blocks_;
              bitField0_ = (bitField0_ & ~0x00000002);
              blocksBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getBlocksFieldBuilder() : null;
            } else {
              blocksBuilder_.addAllMessages(other.blocks_);
            }
          }
        }
        if (other.hasStorage()) {
          mergeStorage(other.getStorage());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasStorageUuid()) {
          return false;
        }
        for (int i = 0; i < getBlocksCount(); i++) {
          if (!getBlocks(i).isInitialized()) {
            return false;
          }
        }
        if (hasStorage()) {
          if (!getStorage().isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object storageUuid_ = "";
      /**
       * <code>required string storageUuid = 1 [deprecated = true];</code>
       * @return Whether the storageUuid field is set.
       */
      @java.lang.Deprecated public boolean hasStorageUuid() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required string storageUuid = 1 [deprecated = true];</code>
       * @return The storageUuid.
       */
      @java.lang.Deprecated public java.lang.String getStorageUuid() {
        java.lang.Object ref = storageUuid_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            storageUuid_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string storageUuid = 1 [deprecated = true];</code>
       * @return The bytes for storageUuid.
       */
      @java.lang.Deprecated public com.google.protobuf.ByteString
          getStorageUuidBytes() {
        java.lang.Object ref = storageUuid_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          storageUuid_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string storageUuid = 1 [deprecated = true];</code>
       * @param value The storageUuid to set.
       * @return This builder for chaining.
       */
      @java.lang.Deprecated public Builder setStorageUuid(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        storageUuid_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string storageUuid = 1 [deprecated = true];</code>
       * @return This builder for chaining.
       */
      @java.lang.Deprecated public Builder clearStorageUuid() {
        bitField0_ = (bitField0_ & ~0x00000001);
        storageUuid_ = getDefaultInstance().getStorageUuid();
        onChanged();
        return this;
      }
      /**
       * <code>required string storageUuid = 1 [deprecated = true];</code>
       * @param value The bytes for storageUuid to set.
       * @return This builder for chaining.
       */
      @java.lang.Deprecated public Builder setStorageUuidBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        storageUuid_ = value;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto> blocks_ =
        java.util.Collections.emptyList();
      private void ensureBlocksIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          blocks_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto>(blocks_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProtoOrBuilder> blocksBuilder_;

      /**
       * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto> getBlocksList() {
        if (blocksBuilder_ == null) {
          return java.util.Collections.unmodifiableList(blocks_);
        } else {
          return blocksBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
       */
      public int getBlocksCount() {
        if (blocksBuilder_ == null) {
          return blocks_.size();
        } else {
          return blocksBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto getBlocks(int index) {
        if (blocksBuilder_ == null) {
          return blocks_.get(index);
        } else {
          return blocksBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
       */
      public Builder setBlocks(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto value) {
        if (blocksBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBlocksIsMutable();
          blocks_.set(index, value);
          onChanged();
        } else {
          blocksBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
       */
      public Builder setBlocks(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.Builder builderForValue) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.set(index, builderForValue.build());
          onChanged();
        } else {
          blocksBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
       */
      public Builder addBlocks(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto value) {
        if (blocksBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBlocksIsMutable();
          blocks_.add(value);
          onChanged();
        } else {
          blocksBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
       */
      public Builder addBlocks(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto value) {
        if (blocksBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBlocksIsMutable();
          blocks_.add(index, value);
          onChanged();
        } else {
          blocksBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
       */
      public Builder addBlocks(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.Builder builderForValue) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.add(builderForValue.build());
          onChanged();
        } else {
          blocksBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
       */
      public Builder addBlocks(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.Builder builderForValue) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.add(index, builderForValue.build());
          onChanged();
        } else {
          blocksBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
       */
      public Builder addAllBlocks(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto> values) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, blocks_);
          onChanged();
        } else {
          blocksBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
       */
      public Builder clearBlocks() {
        if (blocksBuilder_ == null) {
          blocks_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          blocksBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
       */
      public Builder removeBlocks(int index) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.remove(index);
          onChanged();
        } else {
          blocksBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.Builder getBlocksBuilder(
          int index) {
        return getBlocksFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProtoOrBuilder getBlocksOrBuilder(
          int index) {
        if (blocksBuilder_ == null) {
          return blocks_.get(index);  } else {
          return blocksBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProtoOrBuilder>
           getBlocksOrBuilderList() {
        if (blocksBuilder_ != null) {
          return blocksBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(blocks_);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.Builder addBlocksBuilder() {
        return getBlocksFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.Builder addBlocksBuilder(
          int index) {
        return getBlocksFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.ReceivedDeletedBlockInfoProto blocks = 2;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.Builder>
           getBlocksBuilderList() {
        return getBlocksFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProtoOrBuilder>
          getBlocksFieldBuilder() {
        if (blocksBuilder_ == null) {
          blocksBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReceivedDeletedBlockInfoProtoOrBuilder>(
                  blocks_,
                  ((bitField0_ & 0x00000002) != 0),
                  getParentForChildren(),
                  isClean());
          blocks_ = null;
        }
        return blocksBuilder_;
      }

      private org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto storage_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder> storageBuilder_;
      /**
       * <pre>
       * supersedes storageUuid.
       * </pre>
       *
       * <code>optional .org.apache.hadoop.DatanodeStorageProto storage = 3;</code>
       * @return Whether the storage field is set.
       */
      public boolean hasStorage() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <pre>
       * supersedes storageUuid.
       * </pre>
       *
       * <code>optional .org.apache.hadoop.DatanodeStorageProto storage = 3;</code>
       * @return The storage.
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto getStorage() {
        if (storageBuilder_ == null) {
          return storage_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.getDefaultInstance() : storage_;
        } else {
          return storageBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * supersedes storageUuid.
       * </pre>
       *
       * <code>optional .org.apache.hadoop.DatanodeStorageProto storage = 3;</code>
       */
      public Builder setStorage(org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto value) {
        if (storageBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          storage_ = value;
          onChanged();
        } else {
          storageBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <pre>
       * supersedes storageUuid.
       * </pre>
       *
       * <code>optional .org.apache.hadoop.DatanodeStorageProto storage = 3;</code>
       */
      public Builder setStorage(
          org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder builderForValue) {
        if (storageBuilder_ == null) {
          storage_ = builderForValue.build();
          onChanged();
        } else {
          storageBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <pre>
       * supersedes storageUuid.
       * </pre>
       *
       * <code>optional .org.apache.hadoop.DatanodeStorageProto storage = 3;</code>
       */
      public Builder mergeStorage(org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto value) {
        if (storageBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0) &&
              storage_ != null &&
              storage_ != org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.getDefaultInstance()) {
            storage_ =
              org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.newBuilder(storage_).mergeFrom(value).buildPartial();
          } else {
            storage_ = value;
          }
          onChanged();
        } else {
          storageBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <pre>
       * supersedes storageUuid.
       * </pre>
       *
       * <code>optional .org.apache.hadoop.DatanodeStorageProto storage = 3;</code>
       */
      public Builder clearStorage() {
        if (storageBuilder_ == null) {
          storage_ = null;
          onChanged();
        } else {
          storageBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }
      /**
       * <pre>
       * supersedes storageUuid.
       * </pre>
       *
       * <code>optional .org.apache.hadoop.DatanodeStorageProto storage = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder getStorageBuilder() {
        bitField0_ |= 0x00000004;
        onChanged();
        return getStorageFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * supersedes storageUuid.
       * </pre>
       *
       * <code>optional .org.apache.hadoop.DatanodeStorageProto storage = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder getStorageOrBuilder() {
        if (storageBuilder_ != null) {
          return storageBuilder_.getMessageOrBuilder();
        } else {
          return storage_ == null ?
              org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.getDefaultInstance() : storage_;
        }
      }
      /**
       * <pre>
       * supersedes storageUuid.
       * </pre>
       *
       * <code>optional .org.apache.hadoop.DatanodeStorageProto storage = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder>
          getStorageFieldBuilder() {
        if (storageBuilder_ == null) {
          storageBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder>(
                  getStorage(),
                  getParentForChildren(),
                  isClean());
          storage_ = null;
        }
        return storageBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<StorageReceivedDeletedBlocksProto>
        PARSER = new com.google.protobuf.AbstractParser<StorageReceivedDeletedBlocksProto>() {
      @java.lang.Override
      public StorageReceivedDeletedBlocksProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StorageReceivedDeletedBlocksProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<StorageReceivedDeletedBlocksProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<StorageReceivedDeletedBlocksProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface BlockReceivedAndDeletedRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.BlockReceivedAndDeletedRequestProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return Whether the registration field is set.
     */
    boolean hasRegistration();
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return The registration.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration();
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder();

    /**
     * <code>required string blockPoolId = 2;</code>
     * @return Whether the blockPoolId field is set.
     */
    boolean hasBlockPoolId();
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The blockPoolId.
     */
    java.lang.String getBlockPoolId();
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The bytes for blockPoolId.
     */
    com.google.protobuf.ByteString
        getBlockPoolIdBytes();

    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto>
        getBlocksList();
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto getBlocks(int index);
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
     */
    int getBlocksCount();
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProtoOrBuilder>
        getBlocksOrBuilderList();
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProtoOrBuilder getBlocksOrBuilder(
        int index);
  }
  /**
   * <pre>
   **
   * registration - datanode registration information
   * blockPoolID  - block pool ID of the reported blocks
   * blocks       - Received/deleted block list
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.BlockReceivedAndDeletedRequestProto}
   */
  public static final class BlockReceivedAndDeletedRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.BlockReceivedAndDeletedRequestProto)
      BlockReceivedAndDeletedRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use BlockReceivedAndDeletedRequestProto.newBuilder() to construct.
    private BlockReceivedAndDeletedRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private BlockReceivedAndDeletedRequestProto() {
      blockPoolId_ = "";
      blocks_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new BlockReceivedAndDeletedRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private BlockReceivedAndDeletedRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = registration_.toBuilder();
              }
              registration_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(registration_);
                registration_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000002;
              blockPoolId_ = bs;
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                blocks_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto>();
                mutable_bitField0_ |= 0x00000004;
              }
              blocks_.add(
                  input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.PARSER, extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          blocks_ = java.util.Collections.unmodifiableList(blocks_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int REGISTRATION_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto registration_;
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return Whether the registration field is set.
     */
    @java.lang.Override
    public boolean hasRegistration() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return The registration.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration() {
      return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
    }
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder() {
      return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
    }

    public static final int BLOCKPOOLID_FIELD_NUMBER = 2;
    private volatile java.lang.Object blockPoolId_;
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return Whether the blockPoolId field is set.
     */
    @java.lang.Override
    public boolean hasBlockPoolId() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The blockPoolId.
     */
    @java.lang.Override
    public java.lang.String getBlockPoolId() {
      java.lang.Object ref = blockPoolId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          blockPoolId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string blockPoolId = 2;</code>
     * @return The bytes for blockPoolId.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getBlockPoolIdBytes() {
      java.lang.Object ref = blockPoolId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        blockPoolId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int BLOCKS_FIELD_NUMBER = 3;
    private java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto> blocks_;
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto> getBlocksList() {
      return blocks_;
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProtoOrBuilder>
        getBlocksOrBuilderList() {
      return blocks_;
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
     */
    @java.lang.Override
    public int getBlocksCount() {
      return blocks_.size();
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto getBlocks(int index) {
      return blocks_.get(index);
    }
    /**
     * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProtoOrBuilder getBlocksOrBuilder(
        int index) {
      return blocks_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasRegistration()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasBlockPoolId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getRegistration().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      for (int i = 0; i < getBlocksCount(); i++) {
        if (!getBlocks(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getRegistration());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, blockPoolId_);
      }
      for (int i = 0; i < blocks_.size(); i++) {
        output.writeMessage(3, blocks_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getRegistration());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, blockPoolId_);
      }
      for (int i = 0; i < blocks_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, blocks_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto) obj;

      if (hasRegistration() != other.hasRegistration()) return false;
      if (hasRegistration()) {
        if (!getRegistration()
            .equals(other.getRegistration())) return false;
      }
      if (hasBlockPoolId() != other.hasBlockPoolId()) return false;
      if (hasBlockPoolId()) {
        if (!getBlockPoolId()
            .equals(other.getBlockPoolId())) return false;
      }
      if (!getBlocksList()
          .equals(other.getBlocksList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasRegistration()) {
        hash = (37 * hash) + REGISTRATION_FIELD_NUMBER;
        hash = (53 * hash) + getRegistration().hashCode();
      }
      if (hasBlockPoolId()) {
        hash = (37 * hash) + BLOCKPOOLID_FIELD_NUMBER;
        hash = (53 * hash) + getBlockPoolId().hashCode();
      }
      if (getBlocksCount() > 0) {
        hash = (37 * hash) + BLOCKS_FIELD_NUMBER;
        hash = (53 * hash) + getBlocksList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * registration - datanode registration information
     * blockPoolID  - block pool ID of the reported blocks
     * blocks       - Received/deleted block list
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.BlockReceivedAndDeletedRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.BlockReceivedAndDeletedRequestProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getRegistrationFieldBuilder();
          getBlocksFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (registrationBuilder_ == null) {
          registration_ = null;
        } else {
          registrationBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        blockPoolId_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        if (blocksBuilder_ == null) {
          blocks_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          blocksBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (registrationBuilder_ == null) {
            result.registration_ = registration_;
          } else {
            result.registration_ = registrationBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.blockPoolId_ = blockPoolId_;
        if (blocksBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0)) {
            blocks_ = java.util.Collections.unmodifiableList(blocks_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.blocks_ = blocks_;
        } else {
          result.blocks_ = blocksBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto.getDefaultInstance()) return this;
        if (other.hasRegistration()) {
          mergeRegistration(other.getRegistration());
        }
        if (other.hasBlockPoolId()) {
          bitField0_ |= 0x00000002;
          blockPoolId_ = other.blockPoolId_;
          onChanged();
        }
        if (blocksBuilder_ == null) {
          if (!other.blocks_.isEmpty()) {
            if (blocks_.isEmpty()) {
              blocks_ = other.blocks_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureBlocksIsMutable();
              blocks_.addAll(other.blocks_);
            }
            onChanged();
          }
        } else {
          if (!other.blocks_.isEmpty()) {
            if (blocksBuilder_.isEmpty()) {
              blocksBuilder_.dispose();
              blocksBuilder_ = null;
              blocks_ = other.blocks_;
              bitField0_ = (bitField0_ & ~0x00000004);
              blocksBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getBlocksFieldBuilder() : null;
            } else {
              blocksBuilder_.addAllMessages(other.blocks_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasRegistration()) {
          return false;
        }
        if (!hasBlockPoolId()) {
          return false;
        }
        if (!getRegistration().isInitialized()) {
          return false;
        }
        for (int i = 0; i < getBlocksCount(); i++) {
          if (!getBlocks(i).isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto registration_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder> registrationBuilder_;
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       * @return Whether the registration field is set.
       */
      public boolean hasRegistration() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       * @return The registration.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration() {
        if (registrationBuilder_ == null) {
          return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
        } else {
          return registrationBuilder_.getMessage();
        }
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder setRegistration(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto value) {
        if (registrationBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          registration_ = value;
          onChanged();
        } else {
          registrationBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder setRegistration(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder builderForValue) {
        if (registrationBuilder_ == null) {
          registration_ = builderForValue.build();
          onChanged();
        } else {
          registrationBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder mergeRegistration(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto value) {
        if (registrationBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              registration_ != null &&
              registration_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance()) {
            registration_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.newBuilder(registration_).mergeFrom(value).buildPartial();
          } else {
            registration_ = value;
          }
          onChanged();
        } else {
          registrationBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder clearRegistration() {
        if (registrationBuilder_ == null) {
          registration_ = null;
          onChanged();
        } else {
          registrationBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder getRegistrationBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getRegistrationFieldBuilder().getBuilder();
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder() {
        if (registrationBuilder_ != null) {
          return registrationBuilder_.getMessageOrBuilder();
        } else {
          return registration_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
        }
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder>
          getRegistrationFieldBuilder() {
        if (registrationBuilder_ == null) {
          registrationBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder>(
                  getRegistration(),
                  getParentForChildren(),
                  isClean());
          registration_ = null;
        }
        return registrationBuilder_;
      }

      private java.lang.Object blockPoolId_ = "";
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return Whether the blockPoolId field is set.
       */
      public boolean hasBlockPoolId() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return The blockPoolId.
       */
      public java.lang.String getBlockPoolId() {
        java.lang.Object ref = blockPoolId_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            blockPoolId_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return The bytes for blockPoolId.
       */
      public com.google.protobuf.ByteString
          getBlockPoolIdBytes() {
        java.lang.Object ref = blockPoolId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          blockPoolId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @param value The blockPoolId to set.
       * @return This builder for chaining.
       */
      public Builder setBlockPoolId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        blockPoolId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearBlockPoolId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        blockPoolId_ = getDefaultInstance().getBlockPoolId();
        onChanged();
        return this;
      }
      /**
       * <code>required string blockPoolId = 2;</code>
       * @param value The bytes for blockPoolId to set.
       * @return This builder for chaining.
       */
      public Builder setBlockPoolIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        blockPoolId_ = value;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto> blocks_ =
        java.util.Collections.emptyList();
      private void ensureBlocksIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          blocks_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto>(blocks_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProtoOrBuilder> blocksBuilder_;

      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto> getBlocksList() {
        if (blocksBuilder_ == null) {
          return java.util.Collections.unmodifiableList(blocks_);
        } else {
          return blocksBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
       */
      public int getBlocksCount() {
        if (blocksBuilder_ == null) {
          return blocks_.size();
        } else {
          return blocksBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto getBlocks(int index) {
        if (blocksBuilder_ == null) {
          return blocks_.get(index);
        } else {
          return blocksBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
       */
      public Builder setBlocks(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto value) {
        if (blocksBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBlocksIsMutable();
          blocks_.set(index, value);
          onChanged();
        } else {
          blocksBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
       */
      public Builder setBlocks(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.Builder builderForValue) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.set(index, builderForValue.build());
          onChanged();
        } else {
          blocksBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
       */
      public Builder addBlocks(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto value) {
        if (blocksBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBlocksIsMutable();
          blocks_.add(value);
          onChanged();
        } else {
          blocksBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
       */
      public Builder addBlocks(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto value) {
        if (blocksBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBlocksIsMutable();
          blocks_.add(index, value);
          onChanged();
        } else {
          blocksBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
       */
      public Builder addBlocks(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.Builder builderForValue) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.add(builderForValue.build());
          onChanged();
        } else {
          blocksBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
       */
      public Builder addBlocks(
          int index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.Builder builderForValue) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.add(index, builderForValue.build());
          onChanged();
        } else {
          blocksBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
       */
      public Builder addAllBlocks(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto> values) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, blocks_);
          onChanged();
        } else {
          blocksBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
       */
      public Builder clearBlocks() {
        if (blocksBuilder_ == null) {
          blocks_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          blocksBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
       */
      public Builder removeBlocks(int index) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.remove(index);
          onChanged();
        } else {
          blocksBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.Builder getBlocksBuilder(
          int index) {
        return getBlocksFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProtoOrBuilder getBlocksOrBuilder(
          int index) {
        if (blocksBuilder_ == null) {
          return blocks_.get(index);  } else {
          return blocksBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProtoOrBuilder>
           getBlocksOrBuilderList() {
        if (blocksBuilder_ != null) {
          return blocksBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(blocks_);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.Builder addBlocksBuilder() {
        return getBlocksFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.Builder addBlocksBuilder(
          int index) {
        return getBlocksFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.datanode.StorageReceivedDeletedBlocksProto blocks = 3;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.Builder>
           getBlocksBuilderList() {
        return getBlocksFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProtoOrBuilder>
          getBlocksFieldBuilder() {
        if (blocksBuilder_ == null) {
          blocksBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProtoOrBuilder>(
                  blocks_,
                  ((bitField0_ & 0x00000004) != 0),
                  getParentForChildren(),
                  isClean());
          blocks_ = null;
        }
        return blocksBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.BlockReceivedAndDeletedRequestProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.BlockReceivedAndDeletedRequestProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<BlockReceivedAndDeletedRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<BlockReceivedAndDeletedRequestProto>() {
      @java.lang.Override
      public BlockReceivedAndDeletedRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new BlockReceivedAndDeletedRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<BlockReceivedAndDeletedRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<BlockReceivedAndDeletedRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface BlockReceivedAndDeletedResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.BlockReceivedAndDeletedResponseProto)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * <pre>
   **
   * void response
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.BlockReceivedAndDeletedResponseProto}
   */
  public static final class BlockReceivedAndDeletedResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.BlockReceivedAndDeletedResponseProto)
      BlockReceivedAndDeletedResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use BlockReceivedAndDeletedResponseProto.newBuilder() to construct.
    private BlockReceivedAndDeletedResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private BlockReceivedAndDeletedResponseProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new BlockReceivedAndDeletedResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private BlockReceivedAndDeletedResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto) obj;

      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * void response
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.BlockReceivedAndDeletedResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.BlockReceivedAndDeletedResponseProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.BlockReceivedAndDeletedResponseProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.BlockReceivedAndDeletedResponseProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<BlockReceivedAndDeletedResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<BlockReceivedAndDeletedResponseProto>() {
      @java.lang.Override
      public BlockReceivedAndDeletedResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new BlockReceivedAndDeletedResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<BlockReceivedAndDeletedResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<BlockReceivedAndDeletedResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ErrorReportRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.ErrorReportRequestProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * Registartion info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registartion = 1;</code>
     * @return Whether the registartion field is set.
     */
    boolean hasRegistartion();
    /**
     * <pre>
     * Registartion info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registartion = 1;</code>
     * @return The registartion.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistartion();
    /**
     * <pre>
     * Registartion info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registartion = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistartionOrBuilder();

    /**
     * <pre>
     * Error code
     * </pre>
     *
     * <code>required uint32 errorCode = 2;</code>
     * @return Whether the errorCode field is set.
     */
    boolean hasErrorCode();
    /**
     * <pre>
     * Error code
     * </pre>
     *
     * <code>required uint32 errorCode = 2;</code>
     * @return The errorCode.
     */
    int getErrorCode();

    /**
     * <pre>
     * Error message
     * </pre>
     *
     * <code>required string msg = 3;</code>
     * @return Whether the msg field is set.
     */
    boolean hasMsg();
    /**
     * <pre>
     * Error message
     * </pre>
     *
     * <code>required string msg = 3;</code>
     * @return The msg.
     */
    java.lang.String getMsg();
    /**
     * <pre>
     * Error message
     * </pre>
     *
     * <code>required string msg = 3;</code>
     * @return The bytes for msg.
     */
    com.google.protobuf.ByteString
        getMsgBytes();
  }
  /**
   * <pre>
   **
   * registartion - Datanode reporting the error
   * errorCode - error code indicating the error
   * msg - Free text description of the error
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.ErrorReportRequestProto}
   */
  public static final class ErrorReportRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.ErrorReportRequestProto)
      ErrorReportRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ErrorReportRequestProto.newBuilder() to construct.
    private ErrorReportRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ErrorReportRequestProto() {
      msg_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ErrorReportRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ErrorReportRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = registartion_.toBuilder();
              }
              registartion_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(registartion_);
                registartion_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              errorCode_ = input.readUInt32();
              break;
            }
            case 26: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000004;
              msg_ = bs;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto.Builder.class);
    }

    /**
     * Protobuf enum {@code org.apache.hadoop.datanode.ErrorReportRequestProto.ErrorCode}
     */
    public enum ErrorCode
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <pre>
       * Error report to be logged at the namenode
       * </pre>
       *
       * <code>NOTIFY = 0;</code>
       */
      NOTIFY(0),
      /**
       * <pre>
       * DN has disk errors but still has valid volumes
       * </pre>
       *
       * <code>DISK_ERROR = 1;</code>
       */
      DISK_ERROR(1),
      /**
       * <pre>
       * Command from namenode has invalid block ID
       * </pre>
       *
       * <code>INVALID_BLOCK = 2;</code>
       */
      INVALID_BLOCK(2),
      /**
       * <pre>
       * No valid volumes left on datanode
       * </pre>
       *
       * <code>FATAL_DISK_ERROR = 3;</code>
       */
      FATAL_DISK_ERROR(3),
      ;

      /**
       * <pre>
       * Error report to be logged at the namenode
       * </pre>
       *
       * <code>NOTIFY = 0;</code>
       */
      public static final int NOTIFY_VALUE = 0;
      /**
       * <pre>
       * DN has disk errors but still has valid volumes
       * </pre>
       *
       * <code>DISK_ERROR = 1;</code>
       */
      public static final int DISK_ERROR_VALUE = 1;
      /**
       * <pre>
       * Command from namenode has invalid block ID
       * </pre>
       *
       * <code>INVALID_BLOCK = 2;</code>
       */
      public static final int INVALID_BLOCK_VALUE = 2;
      /**
       * <pre>
       * No valid volumes left on datanode
       * </pre>
       *
       * <code>FATAL_DISK_ERROR = 3;</code>
       */
      public static final int FATAL_DISK_ERROR_VALUE = 3;


      public final int getNumber() {
        return value;
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static ErrorCode valueOf(int value) {
        return forNumber(value);
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       */
      public static ErrorCode forNumber(int value) {
        switch (value) {
          case 0: return NOTIFY;
          case 1: return DISK_ERROR;
          case 2: return INVALID_BLOCK;
          case 3: return FATAL_DISK_ERROR;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<ErrorCode>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          ErrorCode> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<ErrorCode>() {
              public ErrorCode findValueByNumber(int number) {
                return ErrorCode.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto.getDescriptor().getEnumTypes().get(0);
      }

      private static final ErrorCode[] VALUES = values();

      public static ErrorCode valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private ErrorCode(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:org.apache.hadoop.datanode.ErrorReportRequestProto.ErrorCode)
    }

    private int bitField0_;
    public static final int REGISTARTION_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto registartion_;
    /**
     * <pre>
     * Registartion info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registartion = 1;</code>
     * @return Whether the registartion field is set.
     */
    @java.lang.Override
    public boolean hasRegistartion() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <pre>
     * Registartion info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registartion = 1;</code>
     * @return The registartion.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistartion() {
      return registartion_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registartion_;
    }
    /**
     * <pre>
     * Registartion info
     * </pre>
     *
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registartion = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistartionOrBuilder() {
      return registartion_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registartion_;
    }

    public static final int ERRORCODE_FIELD_NUMBER = 2;
    private int errorCode_;
    /**
     * <pre>
     * Error code
     * </pre>
     *
     * <code>required uint32 errorCode = 2;</code>
     * @return Whether the errorCode field is set.
     */
    @java.lang.Override
    public boolean hasErrorCode() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <pre>
     * Error code
     * </pre>
     *
     * <code>required uint32 errorCode = 2;</code>
     * @return The errorCode.
     */
    @java.lang.Override
    public int getErrorCode() {
      return errorCode_;
    }

    public static final int MSG_FIELD_NUMBER = 3;
    private volatile java.lang.Object msg_;
    /**
     * <pre>
     * Error message
     * </pre>
     *
     * <code>required string msg = 3;</code>
     * @return Whether the msg field is set.
     */
    @java.lang.Override
    public boolean hasMsg() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <pre>
     * Error message
     * </pre>
     *
     * <code>required string msg = 3;</code>
     * @return The msg.
     */
    @java.lang.Override
    public java.lang.String getMsg() {
      java.lang.Object ref = msg_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          msg_ = s;
        }
        return s;
      }
    }
    /**
     * <pre>
     * Error message
     * </pre>
     *
     * <code>required string msg = 3;</code>
     * @return The bytes for msg.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getMsgBytes() {
      java.lang.Object ref = msg_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        msg_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasRegistartion()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasErrorCode()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasMsg()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getRegistartion().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getRegistartion());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeUInt32(2, errorCode_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, msg_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getRegistartion());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(2, errorCode_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, msg_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto) obj;

      if (hasRegistartion() != other.hasRegistartion()) return false;
      if (hasRegistartion()) {
        if (!getRegistartion()
            .equals(other.getRegistartion())) return false;
      }
      if (hasErrorCode() != other.hasErrorCode()) return false;
      if (hasErrorCode()) {
        if (getErrorCode()
            != other.getErrorCode()) return false;
      }
      if (hasMsg() != other.hasMsg()) return false;
      if (hasMsg()) {
        if (!getMsg()
            .equals(other.getMsg())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasRegistartion()) {
        hash = (37 * hash) + REGISTARTION_FIELD_NUMBER;
        hash = (53 * hash) + getRegistartion().hashCode();
      }
      if (hasErrorCode()) {
        hash = (37 * hash) + ERRORCODE_FIELD_NUMBER;
        hash = (53 * hash) + getErrorCode();
      }
      if (hasMsg()) {
        hash = (37 * hash) + MSG_FIELD_NUMBER;
        hash = (53 * hash) + getMsg().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * registartion - Datanode reporting the error
     * errorCode - error code indicating the error
     * msg - Free text description of the error
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.ErrorReportRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.ErrorReportRequestProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getRegistartionFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (registartionBuilder_ == null) {
          registartion_ = null;
        } else {
          registartionBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        errorCode_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        msg_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (registartionBuilder_ == null) {
            result.registartion_ = registartion_;
          } else {
            result.registartion_ = registartionBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.errorCode_ = errorCode_;
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.msg_ = msg_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto.getDefaultInstance()) return this;
        if (other.hasRegistartion()) {
          mergeRegistartion(other.getRegistartion());
        }
        if (other.hasErrorCode()) {
          setErrorCode(other.getErrorCode());
        }
        if (other.hasMsg()) {
          bitField0_ |= 0x00000004;
          msg_ = other.msg_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasRegistartion()) {
          return false;
        }
        if (!hasErrorCode()) {
          return false;
        }
        if (!hasMsg()) {
          return false;
        }
        if (!getRegistartion().isInitialized()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto registartion_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder> registartionBuilder_;
      /**
       * <pre>
       * Registartion info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registartion = 1;</code>
       * @return Whether the registartion field is set.
       */
      public boolean hasRegistartion() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * Registartion info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registartion = 1;</code>
       * @return The registartion.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistartion() {
        if (registartionBuilder_ == null) {
          return registartion_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registartion_;
        } else {
          return registartionBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * Registartion info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registartion = 1;</code>
       */
      public Builder setRegistartion(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto value) {
        if (registartionBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          registartion_ = value;
          onChanged();
        } else {
          registartionBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <pre>
       * Registartion info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registartion = 1;</code>
       */
      public Builder setRegistartion(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder builderForValue) {
        if (registartionBuilder_ == null) {
          registartion_ = builderForValue.build();
          onChanged();
        } else {
          registartionBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <pre>
       * Registartion info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registartion = 1;</code>
       */
      public Builder mergeRegistartion(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto value) {
        if (registartionBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              registartion_ != null &&
              registartion_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance()) {
            registartion_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.newBuilder(registartion_).mergeFrom(value).buildPartial();
          } else {
            registartion_ = value;
          }
          onChanged();
        } else {
          registartionBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <pre>
       * Registartion info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registartion = 1;</code>
       */
      public Builder clearRegistartion() {
        if (registartionBuilder_ == null) {
          registartion_ = null;
          onChanged();
        } else {
          registartionBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <pre>
       * Registartion info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registartion = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder getRegistartionBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getRegistartionFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * Registartion info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registartion = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistartionOrBuilder() {
        if (registartionBuilder_ != null) {
          return registartionBuilder_.getMessageOrBuilder();
        } else {
          return registartion_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registartion_;
        }
      }
      /**
       * <pre>
       * Registartion info
       * </pre>
       *
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registartion = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder>
          getRegistartionFieldBuilder() {
        if (registartionBuilder_ == null) {
          registartionBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder>(
                  getRegistartion(),
                  getParentForChildren(),
                  isClean());
          registartion_ = null;
        }
        return registartionBuilder_;
      }

      private int errorCode_ ;
      /**
       * <pre>
       * Error code
       * </pre>
       *
       * <code>required uint32 errorCode = 2;</code>
       * @return Whether the errorCode field is set.
       */
      @java.lang.Override
      public boolean hasErrorCode() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <pre>
       * Error code
       * </pre>
       *
       * <code>required uint32 errorCode = 2;</code>
       * @return The errorCode.
       */
      @java.lang.Override
      public int getErrorCode() {
        return errorCode_;
      }
      /**
       * <pre>
       * Error code
       * </pre>
       *
       * <code>required uint32 errorCode = 2;</code>
       * @param value The errorCode to set.
       * @return This builder for chaining.
       */
      public Builder setErrorCode(int value) {
        bitField0_ |= 0x00000002;
        errorCode_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Error code
       * </pre>
       *
       * <code>required uint32 errorCode = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearErrorCode() {
        bitField0_ = (bitField0_ & ~0x00000002);
        errorCode_ = 0;
        onChanged();
        return this;
      }

      private java.lang.Object msg_ = "";
      /**
       * <pre>
       * Error message
       * </pre>
       *
       * <code>required string msg = 3;</code>
       * @return Whether the msg field is set.
       */
      public boolean hasMsg() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <pre>
       * Error message
       * </pre>
       *
       * <code>required string msg = 3;</code>
       * @return The msg.
       */
      public java.lang.String getMsg() {
        java.lang.Object ref = msg_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            msg_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       * Error message
       * </pre>
       *
       * <code>required string msg = 3;</code>
       * @return The bytes for msg.
       */
      public com.google.protobuf.ByteString
          getMsgBytes() {
        java.lang.Object ref = msg_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          msg_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       * Error message
       * </pre>
       *
       * <code>required string msg = 3;</code>
       * @param value The msg to set.
       * @return This builder for chaining.
       */
      public Builder setMsg(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        msg_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Error message
       * </pre>
       *
       * <code>required string msg = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearMsg() {
        bitField0_ = (bitField0_ & ~0x00000004);
        msg_ = getDefaultInstance().getMsg();
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Error message
       * </pre>
       *
       * <code>required string msg = 3;</code>
       * @param value The bytes for msg to set.
       * @return This builder for chaining.
       */
      public Builder setMsgBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        msg_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.ErrorReportRequestProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.ErrorReportRequestProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<ErrorReportRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<ErrorReportRequestProto>() {
      @java.lang.Override
      public ErrorReportRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ErrorReportRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ErrorReportRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ErrorReportRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ErrorReportResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.ErrorReportResponseProto)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * <pre>
   **
   * void response
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.ErrorReportResponseProto}
   */
  public static final class ErrorReportResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.ErrorReportResponseProto)
      ErrorReportResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ErrorReportResponseProto.newBuilder() to construct.
    private ErrorReportResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ErrorReportResponseProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ErrorReportResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ErrorReportResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto) obj;

      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * void response
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.ErrorReportResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.ErrorReportResponseProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.ErrorReportResponseProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.ErrorReportResponseProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<ErrorReportResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<ErrorReportResponseProto>() {
      @java.lang.Override
      public ErrorReportResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ErrorReportResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ErrorReportResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ErrorReportResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ReportBadBlocksRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.ReportBadBlocksRequestProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto>
        getBlocksList();
    /**
     * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto getBlocks(int index);
    /**
     * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
     */
    int getBlocksCount();
    /**
     * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProtoOrBuilder>
        getBlocksOrBuilderList();
    /**
     * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProtoOrBuilder getBlocksOrBuilder(
        int index);
  }
  /**
   * <pre>
   **
   * blocks - list of blocks that are reported as corrupt
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.ReportBadBlocksRequestProto}
   */
  public static final class ReportBadBlocksRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.ReportBadBlocksRequestProto)
      ReportBadBlocksRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ReportBadBlocksRequestProto.newBuilder() to construct.
    private ReportBadBlocksRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ReportBadBlocksRequestProto() {
      blocks_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ReportBadBlocksRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ReportBadBlocksRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                blocks_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              blocks_.add(
                  input.readMessage(org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto.PARSER, extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          blocks_ = java.util.Collections.unmodifiableList(blocks_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto.Builder.class);
    }

    public static final int BLOCKS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto> blocks_;
    /**
     * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto> getBlocksList() {
      return blocks_;
    }
    /**
     * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProtoOrBuilder>
        getBlocksOrBuilderList() {
      return blocks_;
    }
    /**
     * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
     */
    @java.lang.Override
    public int getBlocksCount() {
      return blocks_.size();
    }
    /**
     * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto getBlocks(int index) {
      return blocks_.get(index);
    }
    /**
     * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProtoOrBuilder getBlocksOrBuilder(
        int index) {
      return blocks_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      for (int i = 0; i < getBlocksCount(); i++) {
        if (!getBlocks(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < blocks_.size(); i++) {
        output.writeMessage(1, blocks_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < blocks_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, blocks_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto) obj;

      if (!getBlocksList()
          .equals(other.getBlocksList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getBlocksCount() > 0) {
        hash = (37 * hash) + BLOCKS_FIELD_NUMBER;
        hash = (53 * hash) + getBlocksList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * blocks - list of blocks that are reported as corrupt
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.ReportBadBlocksRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.ReportBadBlocksRequestProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getBlocksFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (blocksBuilder_ == null) {
          blocks_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          blocksBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto(this);
        int from_bitField0_ = bitField0_;
        if (blocksBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            blocks_ = java.util.Collections.unmodifiableList(blocks_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.blocks_ = blocks_;
        } else {
          result.blocks_ = blocksBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto.getDefaultInstance()) return this;
        if (blocksBuilder_ == null) {
          if (!other.blocks_.isEmpty()) {
            if (blocks_.isEmpty()) {
              blocks_ = other.blocks_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureBlocksIsMutable();
              blocks_.addAll(other.blocks_);
            }
            onChanged();
          }
        } else {
          if (!other.blocks_.isEmpty()) {
            if (blocksBuilder_.isEmpty()) {
              blocksBuilder_.dispose();
              blocksBuilder_ = null;
              blocks_ = other.blocks_;
              bitField0_ = (bitField0_ & ~0x00000001);
              blocksBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getBlocksFieldBuilder() : null;
            } else {
              blocksBuilder_.addAllMessages(other.blocks_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        for (int i = 0; i < getBlocksCount(); i++) {
          if (!getBlocks(i).isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto> blocks_ =
        java.util.Collections.emptyList();
      private void ensureBlocksIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          blocks_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto>(blocks_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProtoOrBuilder> blocksBuilder_;

      /**
       * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto> getBlocksList() {
        if (blocksBuilder_ == null) {
          return java.util.Collections.unmodifiableList(blocks_);
        } else {
          return blocksBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
       */
      public int getBlocksCount() {
        if (blocksBuilder_ == null) {
          return blocks_.size();
        } else {
          return blocksBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto getBlocks(int index) {
        if (blocksBuilder_ == null) {
          return blocks_.get(index);
        } else {
          return blocksBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
       */
      public Builder setBlocks(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto value) {
        if (blocksBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBlocksIsMutable();
          blocks_.set(index, value);
          onChanged();
        } else {
          blocksBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
       */
      public Builder setBlocks(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto.Builder builderForValue) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.set(index, builderForValue.build());
          onChanged();
        } else {
          blocksBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
       */
      public Builder addBlocks(org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto value) {
        if (blocksBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBlocksIsMutable();
          blocks_.add(value);
          onChanged();
        } else {
          blocksBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
       */
      public Builder addBlocks(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto value) {
        if (blocksBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBlocksIsMutable();
          blocks_.add(index, value);
          onChanged();
        } else {
          blocksBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
       */
      public Builder addBlocks(
          org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto.Builder builderForValue) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.add(builderForValue.build());
          onChanged();
        } else {
          blocksBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
       */
      public Builder addBlocks(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto.Builder builderForValue) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.add(index, builderForValue.build());
          onChanged();
        } else {
          blocksBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
       */
      public Builder addAllBlocks(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto> values) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, blocks_);
          onChanged();
        } else {
          blocksBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
       */
      public Builder clearBlocks() {
        if (blocksBuilder_ == null) {
          blocks_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          blocksBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
       */
      public Builder removeBlocks(int index) {
        if (blocksBuilder_ == null) {
          ensureBlocksIsMutable();
          blocks_.remove(index);
          onChanged();
        } else {
          blocksBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto.Builder getBlocksBuilder(
          int index) {
        return getBlocksFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProtoOrBuilder getBlocksOrBuilder(
          int index) {
        if (blocksBuilder_ == null) {
          return blocks_.get(index);  } else {
          return blocksBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProtoOrBuilder>
           getBlocksOrBuilderList() {
        if (blocksBuilder_ != null) {
          return blocksBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(blocks_);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto.Builder addBlocksBuilder() {
        return getBlocksFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto.Builder addBlocksBuilder(
          int index) {
        return getBlocksFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.LocatedBlockProto blocks = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto.Builder>
           getBlocksBuilderList() {
        return getBlocksFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProtoOrBuilder>
          getBlocksFieldBuilder() {
        if (blocksBuilder_ == null) {
          blocksBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.LocatedBlockProtoOrBuilder>(
                  blocks_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          blocks_ = null;
        }
        return blocksBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.ReportBadBlocksRequestProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.ReportBadBlocksRequestProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<ReportBadBlocksRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<ReportBadBlocksRequestProto>() {
      @java.lang.Override
      public ReportBadBlocksRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ReportBadBlocksRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ReportBadBlocksRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ReportBadBlocksRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ReportBadBlocksResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.ReportBadBlocksResponseProto)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * <pre>
   **
   * void response
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.ReportBadBlocksResponseProto}
   */
  public static final class ReportBadBlocksResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.ReportBadBlocksResponseProto)
      ReportBadBlocksResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ReportBadBlocksResponseProto.newBuilder() to construct.
    private ReportBadBlocksResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ReportBadBlocksResponseProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ReportBadBlocksResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ReportBadBlocksResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto) obj;

      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * void response
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.ReportBadBlocksResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.ReportBadBlocksResponseProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.ReportBadBlocksResponseProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.ReportBadBlocksResponseProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<ReportBadBlocksResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<ReportBadBlocksResponseProto>() {
      @java.lang.Override
      public ReportBadBlocksResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ReportBadBlocksResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ReportBadBlocksResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ReportBadBlocksResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface CommitBlockSynchronizationRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.CommitBlockSynchronizationRequestProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required .org.apache.hadoop.ExtendedBlockProto block = 1;</code>
     * @return Whether the block field is set.
     */
    boolean hasBlock();
    /**
     * <code>required .org.apache.hadoop.ExtendedBlockProto block = 1;</code>
     * @return The block.
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto getBlock();
    /**
     * <code>required .org.apache.hadoop.ExtendedBlockProto block = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProtoOrBuilder getBlockOrBuilder();

    /**
     * <code>required uint64 newGenStamp = 2;</code>
     * @return Whether the newGenStamp field is set.
     */
    boolean hasNewGenStamp();
    /**
     * <code>required uint64 newGenStamp = 2;</code>
     * @return The newGenStamp.
     */
    long getNewGenStamp();

    /**
     * <code>required uint64 newLength = 3;</code>
     * @return Whether the newLength field is set.
     */
    boolean hasNewLength();
    /**
     * <code>required uint64 newLength = 3;</code>
     * @return The newLength.
     */
    long getNewLength();

    /**
     * <code>required bool closeFile = 4;</code>
     * @return Whether the closeFile field is set.
     */
    boolean hasCloseFile();
    /**
     * <code>required bool closeFile = 4;</code>
     * @return The closeFile.
     */
    boolean getCloseFile();

    /**
     * <code>required bool deleteBlock = 5;</code>
     * @return Whether the deleteBlock field is set.
     */
    boolean hasDeleteBlock();
    /**
     * <code>required bool deleteBlock = 5;</code>
     * @return The deleteBlock.
     */
    boolean getDeleteBlock();

    /**
     * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto>
        getNewTargetsList();
    /**
     * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto getNewTargets(int index);
    /**
     * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
     */
    int getNewTargetsCount();
    /**
     * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProtoOrBuilder>
        getNewTargetsOrBuilderList();
    /**
     * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProtoOrBuilder getNewTargetsOrBuilder(
        int index);

    /**
     * <code>repeated string newTargetStorages = 7;</code>
     * @return A list containing the newTargetStorages.
     */
    java.util.List<java.lang.String>
        getNewTargetStoragesList();
    /**
     * <code>repeated string newTargetStorages = 7;</code>
     * @return The count of newTargetStorages.
     */
    int getNewTargetStoragesCount();
    /**
     * <code>repeated string newTargetStorages = 7;</code>
     * @param index The index of the element to return.
     * @return The newTargetStorages at the given index.
     */
    java.lang.String getNewTargetStorages(int index);
    /**
     * <code>repeated string newTargetStorages = 7;</code>
     * @param index The index of the value to return.
     * @return The bytes of the newTargetStorages at the given index.
     */
    com.google.protobuf.ByteString
        getNewTargetStoragesBytes(int index);
  }
  /**
   * <pre>
   **
   * Commit block synchronization request during lease recovery
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.CommitBlockSynchronizationRequestProto}
   */
  public static final class CommitBlockSynchronizationRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.CommitBlockSynchronizationRequestProto)
      CommitBlockSynchronizationRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use CommitBlockSynchronizationRequestProto.newBuilder() to construct.
    private CommitBlockSynchronizationRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private CommitBlockSynchronizationRequestProto() {
      newTargets_ = java.util.Collections.emptyList();
      newTargetStorages_ = com.google.protobuf.LazyStringArrayList.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new CommitBlockSynchronizationRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private CommitBlockSynchronizationRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = block_.toBuilder();
              }
              block_ = input.readMessage(org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(block_);
                block_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              newGenStamp_ = input.readUInt64();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              newLength_ = input.readUInt64();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              closeFile_ = input.readBool();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000010;
              deleteBlock_ = input.readBool();
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000020) != 0)) {
                newTargets_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto>();
                mutable_bitField0_ |= 0x00000020;
              }
              newTargets_.add(
                  input.readMessage(org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.PARSER, extensionRegistry));
              break;
            }
            case 58: {
              com.google.protobuf.ByteString bs = input.readBytes();
              if (!((mutable_bitField0_ & 0x00000040) != 0)) {
                newTargetStorages_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000040;
              }
              newTargetStorages_.add(bs);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000020) != 0)) {
          newTargets_ = java.util.Collections.unmodifiableList(newTargets_);
        }
        if (((mutable_bitField0_ & 0x00000040) != 0)) {
          newTargetStorages_ = newTargetStorages_.getUnmodifiableView();
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int BLOCK_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto block_;
    /**
     * <code>required .org.apache.hadoop.ExtendedBlockProto block = 1;</code>
     * @return Whether the block field is set.
     */
    @java.lang.Override
    public boolean hasBlock() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required .org.apache.hadoop.ExtendedBlockProto block = 1;</code>
     * @return The block.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto getBlock() {
      return block_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto.getDefaultInstance() : block_;
    }
    /**
     * <code>required .org.apache.hadoop.ExtendedBlockProto block = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProtoOrBuilder getBlockOrBuilder() {
      return block_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto.getDefaultInstance() : block_;
    }

    public static final int NEWGENSTAMP_FIELD_NUMBER = 2;
    private long newGenStamp_;
    /**
     * <code>required uint64 newGenStamp = 2;</code>
     * @return Whether the newGenStamp field is set.
     */
    @java.lang.Override
    public boolean hasNewGenStamp() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>required uint64 newGenStamp = 2;</code>
     * @return The newGenStamp.
     */
    @java.lang.Override
    public long getNewGenStamp() {
      return newGenStamp_;
    }

    public static final int NEWLENGTH_FIELD_NUMBER = 3;
    private long newLength_;
    /**
     * <code>required uint64 newLength = 3;</code>
     * @return Whether the newLength field is set.
     */
    @java.lang.Override
    public boolean hasNewLength() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>required uint64 newLength = 3;</code>
     * @return The newLength.
     */
    @java.lang.Override
    public long getNewLength() {
      return newLength_;
    }

    public static final int CLOSEFILE_FIELD_NUMBER = 4;
    private boolean closeFile_;
    /**
     * <code>required bool closeFile = 4;</code>
     * @return Whether the closeFile field is set.
     */
    @java.lang.Override
    public boolean hasCloseFile() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>required bool closeFile = 4;</code>
     * @return The closeFile.
     */
    @java.lang.Override
    public boolean getCloseFile() {
      return closeFile_;
    }

    public static final int DELETEBLOCK_FIELD_NUMBER = 5;
    private boolean deleteBlock_;
    /**
     * <code>required bool deleteBlock = 5;</code>
     * @return Whether the deleteBlock field is set.
     */
    @java.lang.Override
    public boolean hasDeleteBlock() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>required bool deleteBlock = 5;</code>
     * @return The deleteBlock.
     */
    @java.lang.Override
    public boolean getDeleteBlock() {
      return deleteBlock_;
    }

    public static final int NEWTARGETS_FIELD_NUMBER = 6;
    private java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto> newTargets_;
    /**
     * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto> getNewTargetsList() {
      return newTargets_;
    }
    /**
     * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProtoOrBuilder>
        getNewTargetsOrBuilderList() {
      return newTargets_;
    }
    /**
     * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
     */
    @java.lang.Override
    public int getNewTargetsCount() {
      return newTargets_.size();
    }
    /**
     * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto getNewTargets(int index) {
      return newTargets_.get(index);
    }
    /**
     * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProtoOrBuilder getNewTargetsOrBuilder(
        int index) {
      return newTargets_.get(index);
    }

    public static final int NEWTARGETSTORAGES_FIELD_NUMBER = 7;
    private com.google.protobuf.LazyStringList newTargetStorages_;
    /**
     * <code>repeated string newTargetStorages = 7;</code>
     * @return A list containing the newTargetStorages.
     */
    public com.google.protobuf.ProtocolStringList
        getNewTargetStoragesList() {
      return newTargetStorages_;
    }
    /**
     * <code>repeated string newTargetStorages = 7;</code>
     * @return The count of newTargetStorages.
     */
    public int getNewTargetStoragesCount() {
      return newTargetStorages_.size();
    }
    /**
     * <code>repeated string newTargetStorages = 7;</code>
     * @param index The index of the element to return.
     * @return The newTargetStorages at the given index.
     */
    public java.lang.String getNewTargetStorages(int index) {
      return newTargetStorages_.get(index);
    }
    /**
     * <code>repeated string newTargetStorages = 7;</code>
     * @param index The index of the value to return.
     * @return The bytes of the newTargetStorages at the given index.
     */
    public com.google.protobuf.ByteString
        getNewTargetStoragesBytes(int index) {
      return newTargetStorages_.getByteString(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasBlock()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasNewGenStamp()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasNewLength()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasCloseFile()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasDeleteBlock()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getBlock().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      for (int i = 0; i < getNewTargetsCount(); i++) {
        if (!getNewTargets(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getBlock());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeUInt64(2, newGenStamp_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeUInt64(3, newLength_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeBool(4, closeFile_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        output.writeBool(5, deleteBlock_);
      }
      for (int i = 0; i < newTargets_.size(); i++) {
        output.writeMessage(6, newTargets_.get(i));
      }
      for (int i = 0; i < newTargetStorages_.size(); i++) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 7, newTargetStorages_.getRaw(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getBlock());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, newGenStamp_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, newLength_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(4, closeFile_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(5, deleteBlock_);
      }
      for (int i = 0; i < newTargets_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, newTargets_.get(i));
      }
      {
        int dataSize = 0;
        for (int i = 0; i < newTargetStorages_.size(); i++) {
          dataSize += computeStringSizeNoTag(newTargetStorages_.getRaw(i));
        }
        size += dataSize;
        size += 1 * getNewTargetStoragesList().size();
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto) obj;

      if (hasBlock() != other.hasBlock()) return false;
      if (hasBlock()) {
        if (!getBlock()
            .equals(other.getBlock())) return false;
      }
      if (hasNewGenStamp() != other.hasNewGenStamp()) return false;
      if (hasNewGenStamp()) {
        if (getNewGenStamp()
            != other.getNewGenStamp()) return false;
      }
      if (hasNewLength() != other.hasNewLength()) return false;
      if (hasNewLength()) {
        if (getNewLength()
            != other.getNewLength()) return false;
      }
      if (hasCloseFile() != other.hasCloseFile()) return false;
      if (hasCloseFile()) {
        if (getCloseFile()
            != other.getCloseFile()) return false;
      }
      if (hasDeleteBlock() != other.hasDeleteBlock()) return false;
      if (hasDeleteBlock()) {
        if (getDeleteBlock()
            != other.getDeleteBlock()) return false;
      }
      if (!getNewTargetsList()
          .equals(other.getNewTargetsList())) return false;
      if (!getNewTargetStoragesList()
          .equals(other.getNewTargetStoragesList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasBlock()) {
        hash = (37 * hash) + BLOCK_FIELD_NUMBER;
        hash = (53 * hash) + getBlock().hashCode();
      }
      if (hasNewGenStamp()) {
        hash = (37 * hash) + NEWGENSTAMP_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getNewGenStamp());
      }
      if (hasNewLength()) {
        hash = (37 * hash) + NEWLENGTH_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getNewLength());
      }
      if (hasCloseFile()) {
        hash = (37 * hash) + CLOSEFILE_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getCloseFile());
      }
      if (hasDeleteBlock()) {
        hash = (37 * hash) + DELETEBLOCK_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getDeleteBlock());
      }
      if (getNewTargetsCount() > 0) {
        hash = (37 * hash) + NEWTARGETS_FIELD_NUMBER;
        hash = (53 * hash) + getNewTargetsList().hashCode();
      }
      if (getNewTargetStoragesCount() > 0) {
        hash = (37 * hash) + NEWTARGETSTORAGES_FIELD_NUMBER;
        hash = (53 * hash) + getNewTargetStoragesList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * Commit block synchronization request during lease recovery
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.CommitBlockSynchronizationRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.CommitBlockSynchronizationRequestProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getBlockFieldBuilder();
          getNewTargetsFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (blockBuilder_ == null) {
          block_ = null;
        } else {
          blockBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        newGenStamp_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        newLength_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        closeFile_ = false;
        bitField0_ = (bitField0_ & ~0x00000008);
        deleteBlock_ = false;
        bitField0_ = (bitField0_ & ~0x00000010);
        if (newTargetsBuilder_ == null) {
          newTargets_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
        } else {
          newTargetsBuilder_.clear();
        }
        newTargetStorages_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000040);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (blockBuilder_ == null) {
            result.block_ = block_;
          } else {
            result.block_ = blockBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.newGenStamp_ = newGenStamp_;
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.newLength_ = newLength_;
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          result.closeFile_ = closeFile_;
          to_bitField0_ |= 0x00000008;
        }
        if (((from_bitField0_ & 0x00000010) != 0)) {
          result.deleteBlock_ = deleteBlock_;
          to_bitField0_ |= 0x00000010;
        }
        if (newTargetsBuilder_ == null) {
          if (((bitField0_ & 0x00000020) != 0)) {
            newTargets_ = java.util.Collections.unmodifiableList(newTargets_);
            bitField0_ = (bitField0_ & ~0x00000020);
          }
          result.newTargets_ = newTargets_;
        } else {
          result.newTargets_ = newTargetsBuilder_.build();
        }
        if (((bitField0_ & 0x00000040) != 0)) {
          newTargetStorages_ = newTargetStorages_.getUnmodifiableView();
          bitField0_ = (bitField0_ & ~0x00000040);
        }
        result.newTargetStorages_ = newTargetStorages_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto.getDefaultInstance()) return this;
        if (other.hasBlock()) {
          mergeBlock(other.getBlock());
        }
        if (other.hasNewGenStamp()) {
          setNewGenStamp(other.getNewGenStamp());
        }
        if (other.hasNewLength()) {
          setNewLength(other.getNewLength());
        }
        if (other.hasCloseFile()) {
          setCloseFile(other.getCloseFile());
        }
        if (other.hasDeleteBlock()) {
          setDeleteBlock(other.getDeleteBlock());
        }
        if (newTargetsBuilder_ == null) {
          if (!other.newTargets_.isEmpty()) {
            if (newTargets_.isEmpty()) {
              newTargets_ = other.newTargets_;
              bitField0_ = (bitField0_ & ~0x00000020);
            } else {
              ensureNewTargetsIsMutable();
              newTargets_.addAll(other.newTargets_);
            }
            onChanged();
          }
        } else {
          if (!other.newTargets_.isEmpty()) {
            if (newTargetsBuilder_.isEmpty()) {
              newTargetsBuilder_.dispose();
              newTargetsBuilder_ = null;
              newTargets_ = other.newTargets_;
              bitField0_ = (bitField0_ & ~0x00000020);
              newTargetsBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getNewTargetsFieldBuilder() : null;
            } else {
              newTargetsBuilder_.addAllMessages(other.newTargets_);
            }
          }
        }
        if (!other.newTargetStorages_.isEmpty()) {
          if (newTargetStorages_.isEmpty()) {
            newTargetStorages_ = other.newTargetStorages_;
            bitField0_ = (bitField0_ & ~0x00000040);
          } else {
            ensureNewTargetStoragesIsMutable();
            newTargetStorages_.addAll(other.newTargetStorages_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasBlock()) {
          return false;
        }
        if (!hasNewGenStamp()) {
          return false;
        }
        if (!hasNewLength()) {
          return false;
        }
        if (!hasCloseFile()) {
          return false;
        }
        if (!hasDeleteBlock()) {
          return false;
        }
        if (!getBlock().isInitialized()) {
          return false;
        }
        for (int i = 0; i < getNewTargetsCount(); i++) {
          if (!getNewTargets(i).isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto block_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProtoOrBuilder> blockBuilder_;
      /**
       * <code>required .org.apache.hadoop.ExtendedBlockProto block = 1;</code>
       * @return Whether the block field is set.
       */
      public boolean hasBlock() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required .org.apache.hadoop.ExtendedBlockProto block = 1;</code>
       * @return The block.
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto getBlock() {
        if (blockBuilder_ == null) {
          return block_ == null ? org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto.getDefaultInstance() : block_;
        } else {
          return blockBuilder_.getMessage();
        }
      }
      /**
       * <code>required .org.apache.hadoop.ExtendedBlockProto block = 1;</code>
       */
      public Builder setBlock(org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto value) {
        if (blockBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          block_ = value;
          onChanged();
        } else {
          blockBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.ExtendedBlockProto block = 1;</code>
       */
      public Builder setBlock(
          org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto.Builder builderForValue) {
        if (blockBuilder_ == null) {
          block_ = builderForValue.build();
          onChanged();
        } else {
          blockBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.ExtendedBlockProto block = 1;</code>
       */
      public Builder mergeBlock(org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto value) {
        if (blockBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              block_ != null &&
              block_ != org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto.getDefaultInstance()) {
            block_ =
              org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto.newBuilder(block_).mergeFrom(value).buildPartial();
          } else {
            block_ = value;
          }
          onChanged();
        } else {
          blockBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.ExtendedBlockProto block = 1;</code>
       */
      public Builder clearBlock() {
        if (blockBuilder_ == null) {
          block_ = null;
          onChanged();
        } else {
          blockBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.ExtendedBlockProto block = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto.Builder getBlockBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getBlockFieldBuilder().getBuilder();
      }
      /**
       * <code>required .org.apache.hadoop.ExtendedBlockProto block = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProtoOrBuilder getBlockOrBuilder() {
        if (blockBuilder_ != null) {
          return blockBuilder_.getMessageOrBuilder();
        } else {
          return block_ == null ?
              org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto.getDefaultInstance() : block_;
        }
      }
      /**
       * <code>required .org.apache.hadoop.ExtendedBlockProto block = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProtoOrBuilder>
          getBlockFieldBuilder() {
        if (blockBuilder_ == null) {
          blockBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.ExtendedBlockProtoOrBuilder>(
                  getBlock(),
                  getParentForChildren(),
                  isClean());
          block_ = null;
        }
        return blockBuilder_;
      }

      private long newGenStamp_ ;
      /**
       * <code>required uint64 newGenStamp = 2;</code>
       * @return Whether the newGenStamp field is set.
       */
      @java.lang.Override
      public boolean hasNewGenStamp() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>required uint64 newGenStamp = 2;</code>
       * @return The newGenStamp.
       */
      @java.lang.Override
      public long getNewGenStamp() {
        return newGenStamp_;
      }
      /**
       * <code>required uint64 newGenStamp = 2;</code>
       * @param value The newGenStamp to set.
       * @return This builder for chaining.
       */
      public Builder setNewGenStamp(long value) {
        bitField0_ |= 0x00000002;
        newGenStamp_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 newGenStamp = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearNewGenStamp() {
        bitField0_ = (bitField0_ & ~0x00000002);
        newGenStamp_ = 0L;
        onChanged();
        return this;
      }

      private long newLength_ ;
      /**
       * <code>required uint64 newLength = 3;</code>
       * @return Whether the newLength field is set.
       */
      @java.lang.Override
      public boolean hasNewLength() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>required uint64 newLength = 3;</code>
       * @return The newLength.
       */
      @java.lang.Override
      public long getNewLength() {
        return newLength_;
      }
      /**
       * <code>required uint64 newLength = 3;</code>
       * @param value The newLength to set.
       * @return This builder for chaining.
       */
      public Builder setNewLength(long value) {
        bitField0_ |= 0x00000004;
        newLength_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 newLength = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearNewLength() {
        bitField0_ = (bitField0_ & ~0x00000004);
        newLength_ = 0L;
        onChanged();
        return this;
      }

      private boolean closeFile_ ;
      /**
       * <code>required bool closeFile = 4;</code>
       * @return Whether the closeFile field is set.
       */
      @java.lang.Override
      public boolean hasCloseFile() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>required bool closeFile = 4;</code>
       * @return The closeFile.
       */
      @java.lang.Override
      public boolean getCloseFile() {
        return closeFile_;
      }
      /**
       * <code>required bool closeFile = 4;</code>
       * @param value The closeFile to set.
       * @return This builder for chaining.
       */
      public Builder setCloseFile(boolean value) {
        bitField0_ |= 0x00000008;
        closeFile_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bool closeFile = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearCloseFile() {
        bitField0_ = (bitField0_ & ~0x00000008);
        closeFile_ = false;
        onChanged();
        return this;
      }

      private boolean deleteBlock_ ;
      /**
       * <code>required bool deleteBlock = 5;</code>
       * @return Whether the deleteBlock field is set.
       */
      @java.lang.Override
      public boolean hasDeleteBlock() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>required bool deleteBlock = 5;</code>
       * @return The deleteBlock.
       */
      @java.lang.Override
      public boolean getDeleteBlock() {
        return deleteBlock_;
      }
      /**
       * <code>required bool deleteBlock = 5;</code>
       * @param value The deleteBlock to set.
       * @return This builder for chaining.
       */
      public Builder setDeleteBlock(boolean value) {
        bitField0_ |= 0x00000010;
        deleteBlock_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bool deleteBlock = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearDeleteBlock() {
        bitField0_ = (bitField0_ & ~0x00000010);
        deleteBlock_ = false;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto> newTargets_ =
        java.util.Collections.emptyList();
      private void ensureNewTargetsIsMutable() {
        if (!((bitField0_ & 0x00000020) != 0)) {
          newTargets_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto>(newTargets_);
          bitField0_ |= 0x00000020;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProtoOrBuilder> newTargetsBuilder_;

      /**
       * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto> getNewTargetsList() {
        if (newTargetsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(newTargets_);
        } else {
          return newTargetsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
       */
      public int getNewTargetsCount() {
        if (newTargetsBuilder_ == null) {
          return newTargets_.size();
        } else {
          return newTargetsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto getNewTargets(int index) {
        if (newTargetsBuilder_ == null) {
          return newTargets_.get(index);
        } else {
          return newTargetsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
       */
      public Builder setNewTargets(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto value) {
        if (newTargetsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNewTargetsIsMutable();
          newTargets_.set(index, value);
          onChanged();
        } else {
          newTargetsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
       */
      public Builder setNewTargets(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.Builder builderForValue) {
        if (newTargetsBuilder_ == null) {
          ensureNewTargetsIsMutable();
          newTargets_.set(index, builderForValue.build());
          onChanged();
        } else {
          newTargetsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
       */
      public Builder addNewTargets(org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto value) {
        if (newTargetsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNewTargetsIsMutable();
          newTargets_.add(value);
          onChanged();
        } else {
          newTargetsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
       */
      public Builder addNewTargets(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto value) {
        if (newTargetsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNewTargetsIsMutable();
          newTargets_.add(index, value);
          onChanged();
        } else {
          newTargetsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
       */
      public Builder addNewTargets(
          org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.Builder builderForValue) {
        if (newTargetsBuilder_ == null) {
          ensureNewTargetsIsMutable();
          newTargets_.add(builderForValue.build());
          onChanged();
        } else {
          newTargetsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
       */
      public Builder addNewTargets(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.Builder builderForValue) {
        if (newTargetsBuilder_ == null) {
          ensureNewTargetsIsMutable();
          newTargets_.add(index, builderForValue.build());
          onChanged();
        } else {
          newTargetsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
       */
      public Builder addAllNewTargets(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto> values) {
        if (newTargetsBuilder_ == null) {
          ensureNewTargetsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, newTargets_);
          onChanged();
        } else {
          newTargetsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
       */
      public Builder clearNewTargets() {
        if (newTargetsBuilder_ == null) {
          newTargets_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
          onChanged();
        } else {
          newTargetsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
       */
      public Builder removeNewTargets(int index) {
        if (newTargetsBuilder_ == null) {
          ensureNewTargetsIsMutable();
          newTargets_.remove(index);
          onChanged();
        } else {
          newTargetsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.Builder getNewTargetsBuilder(
          int index) {
        return getNewTargetsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProtoOrBuilder getNewTargetsOrBuilder(
          int index) {
        if (newTargetsBuilder_ == null) {
          return newTargets_.get(index);  } else {
          return newTargetsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProtoOrBuilder>
           getNewTargetsOrBuilderList() {
        if (newTargetsBuilder_ != null) {
          return newTargetsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(newTargets_);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.Builder addNewTargetsBuilder() {
        return getNewTargetsFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.Builder addNewTargetsBuilder(
          int index) {
        return getNewTargetsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeIDProto newTargets = 6;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.Builder>
           getNewTargetsBuilderList() {
        return getNewTargetsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProtoOrBuilder>
          getNewTargetsFieldBuilder() {
        if (newTargetsBuilder_ == null) {
          newTargetsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeIDProtoOrBuilder>(
                  newTargets_,
                  ((bitField0_ & 0x00000020) != 0),
                  getParentForChildren(),
                  isClean());
          newTargets_ = null;
        }
        return newTargetsBuilder_;
      }

      private com.google.protobuf.LazyStringList newTargetStorages_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureNewTargetStoragesIsMutable() {
        if (!((bitField0_ & 0x00000040) != 0)) {
          newTargetStorages_ = new com.google.protobuf.LazyStringArrayList(newTargetStorages_);
          bitField0_ |= 0x00000040;
         }
      }
      /**
       * <code>repeated string newTargetStorages = 7;</code>
       * @return A list containing the newTargetStorages.
       */
      public com.google.protobuf.ProtocolStringList
          getNewTargetStoragesList() {
        return newTargetStorages_.getUnmodifiableView();
      }
      /**
       * <code>repeated string newTargetStorages = 7;</code>
       * @return The count of newTargetStorages.
       */
      public int getNewTargetStoragesCount() {
        return newTargetStorages_.size();
      }
      /**
       * <code>repeated string newTargetStorages = 7;</code>
       * @param index The index of the element to return.
       * @return The newTargetStorages at the given index.
       */
      public java.lang.String getNewTargetStorages(int index) {
        return newTargetStorages_.get(index);
      }
      /**
       * <code>repeated string newTargetStorages = 7;</code>
       * @param index The index of the value to return.
       * @return The bytes of the newTargetStorages at the given index.
       */
      public com.google.protobuf.ByteString
          getNewTargetStoragesBytes(int index) {
        return newTargetStorages_.getByteString(index);
      }
      /**
       * <code>repeated string newTargetStorages = 7;</code>
       * @param index The index to set the value at.
       * @param value The newTargetStorages to set.
       * @return This builder for chaining.
       */
      public Builder setNewTargetStorages(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureNewTargetStoragesIsMutable();
        newTargetStorages_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string newTargetStorages = 7;</code>
       * @param value The newTargetStorages to add.
       * @return This builder for chaining.
       */
      public Builder addNewTargetStorages(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureNewTargetStoragesIsMutable();
        newTargetStorages_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string newTargetStorages = 7;</code>
       * @param values The newTargetStorages to add.
       * @return This builder for chaining.
       */
      public Builder addAllNewTargetStorages(
          java.lang.Iterable<java.lang.String> values) {
        ensureNewTargetStoragesIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, newTargetStorages_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string newTargetStorages = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearNewTargetStorages() {
        newTargetStorages_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000040);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string newTargetStorages = 7;</code>
       * @param value The bytes of the newTargetStorages to add.
       * @return This builder for chaining.
       */
      public Builder addNewTargetStoragesBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureNewTargetStoragesIsMutable();
        newTargetStorages_.add(value);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.CommitBlockSynchronizationRequestProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.CommitBlockSynchronizationRequestProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<CommitBlockSynchronizationRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<CommitBlockSynchronizationRequestProto>() {
      @java.lang.Override
      public CommitBlockSynchronizationRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new CommitBlockSynchronizationRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<CommitBlockSynchronizationRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<CommitBlockSynchronizationRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface CommitBlockSynchronizationResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.CommitBlockSynchronizationResponseProto)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * <pre>
   **
   * void response
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.CommitBlockSynchronizationResponseProto}
   */
  public static final class CommitBlockSynchronizationResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.CommitBlockSynchronizationResponseProto)
      CommitBlockSynchronizationResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use CommitBlockSynchronizationResponseProto.newBuilder() to construct.
    private CommitBlockSynchronizationResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private CommitBlockSynchronizationResponseProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new CommitBlockSynchronizationResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private CommitBlockSynchronizationResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto) obj;

      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * void response
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.CommitBlockSynchronizationResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.CommitBlockSynchronizationResponseProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.CommitBlockSynchronizationResponseProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.CommitBlockSynchronizationResponseProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<CommitBlockSynchronizationResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<CommitBlockSynchronizationResponseProto>() {
      @java.lang.Override
      public CommitBlockSynchronizationResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new CommitBlockSynchronizationResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<CommitBlockSynchronizationResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<CommitBlockSynchronizationResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ActiveNamenodeListRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.ActiveNamenodeListRequestProto)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * <pre>
   **
   * void request
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.ActiveNamenodeListRequestProto}
   */
  public static final class ActiveNamenodeListRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.ActiveNamenodeListRequestProto)
      ActiveNamenodeListRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ActiveNamenodeListRequestProto.newBuilder() to construct.
    private ActiveNamenodeListRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ActiveNamenodeListRequestProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ActiveNamenodeListRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ActiveNamenodeListRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto) obj;

      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * void request
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.ActiveNamenodeListRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.ActiveNamenodeListRequestProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.ActiveNamenodeListRequestProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.ActiveNamenodeListRequestProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<ActiveNamenodeListRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<ActiveNamenodeListRequestProto>() {
      @java.lang.Override
      public ActiveNamenodeListRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ActiveNamenodeListRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ActiveNamenodeListRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ActiveNamenodeListRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ActiveNamenodeListResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.ActiveNamenodeListResponseProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
     */
    java.util.List<io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto> 
        getNamenodesList();
    /**
     * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
     */
    io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto getNamenodes(int index);
    /**
     * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
     */
    int getNamenodesCount();
    /**
     * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
     */
    java.util.List<? extends io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProtoOrBuilder> 
        getNamenodesOrBuilderList();
    /**
     * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
     */
    io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProtoOrBuilder getNamenodesOrBuilder(
        int index);
  }
  /**
   * <pre>
   **
   * void response
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.ActiveNamenodeListResponseProto}
   */
  public static final class ActiveNamenodeListResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.ActiveNamenodeListResponseProto)
      ActiveNamenodeListResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ActiveNamenodeListResponseProto.newBuilder() to construct.
    private ActiveNamenodeListResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ActiveNamenodeListResponseProto() {
      namenodes_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ActiveNamenodeListResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ActiveNamenodeListResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                namenodes_ = new java.util.ArrayList<io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              namenodes_.add(
                  input.readMessage(io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto.PARSER, extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          namenodes_ = java.util.Collections.unmodifiableList(namenodes_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto.Builder.class);
    }

    public static final int NAMENODES_FIELD_NUMBER = 1;
    private java.util.List<io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto> namenodes_;
    /**
     * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
     */
    @java.lang.Override
    public java.util.List<io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto> getNamenodesList() {
      return namenodes_;
    }
    /**
     * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
     */
    @java.lang.Override
    public java.util.List<? extends io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProtoOrBuilder> 
        getNamenodesOrBuilderList() {
      return namenodes_;
    }
    /**
     * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
     */
    @java.lang.Override
    public int getNamenodesCount() {
      return namenodes_.size();
    }
    /**
     * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
     */
    @java.lang.Override
    public io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto getNamenodes(int index) {
      return namenodes_.get(index);
    }
    /**
     * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
     */
    @java.lang.Override
    public io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProtoOrBuilder getNamenodesOrBuilder(
        int index) {
      return namenodes_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      for (int i = 0; i < getNamenodesCount(); i++) {
        if (!getNamenodes(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < namenodes_.size(); i++) {
        output.writeMessage(1, namenodes_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < namenodes_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, namenodes_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto) obj;

      if (!getNamenodesList()
          .equals(other.getNamenodesList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getNamenodesCount() > 0) {
        hash = (37 * hash) + NAMENODES_FIELD_NUMBER;
        hash = (53 * hash) + getNamenodesList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * void response
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.ActiveNamenodeListResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.ActiveNamenodeListResponseProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getNamenodesFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (namenodesBuilder_ == null) {
          namenodes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          namenodesBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (namenodesBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            namenodes_ = java.util.Collections.unmodifiableList(namenodes_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.namenodes_ = namenodes_;
        } else {
          result.namenodes_ = namenodesBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto.getDefaultInstance()) return this;
        if (namenodesBuilder_ == null) {
          if (!other.namenodes_.isEmpty()) {
            if (namenodes_.isEmpty()) {
              namenodes_ = other.namenodes_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureNamenodesIsMutable();
              namenodes_.addAll(other.namenodes_);
            }
            onChanged();
          }
        } else {
          if (!other.namenodes_.isEmpty()) {
            if (namenodesBuilder_.isEmpty()) {
              namenodesBuilder_.dispose();
              namenodesBuilder_ = null;
              namenodes_ = other.namenodes_;
              bitField0_ = (bitField0_ & ~0x00000001);
              namenodesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getNamenodesFieldBuilder() : null;
            } else {
              namenodesBuilder_.addAllMessages(other.namenodes_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        for (int i = 0; i < getNamenodesCount(); i++) {
          if (!getNamenodes(i).isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto> namenodes_ =
        java.util.Collections.emptyList();
      private void ensureNamenodesIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          namenodes_ = new java.util.ArrayList<io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto>(namenodes_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto, io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto.Builder, io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProtoOrBuilder> namenodesBuilder_;

      /**
       * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
       */
      public java.util.List<io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto> getNamenodesList() {
        if (namenodesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(namenodes_);
        } else {
          return namenodesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
       */
      public int getNamenodesCount() {
        if (namenodesBuilder_ == null) {
          return namenodes_.size();
        } else {
          return namenodesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
       */
      public io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto getNamenodes(int index) {
        if (namenodesBuilder_ == null) {
          return namenodes_.get(index);
        } else {
          return namenodesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
       */
      public Builder setNamenodes(
          int index, io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto value) {
        if (namenodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNamenodesIsMutable();
          namenodes_.set(index, value);
          onChanged();
        } else {
          namenodesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
       */
      public Builder setNamenodes(
          int index, io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto.Builder builderForValue) {
        if (namenodesBuilder_ == null) {
          ensureNamenodesIsMutable();
          namenodes_.set(index, builderForValue.build());
          onChanged();
        } else {
          namenodesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
       */
      public Builder addNamenodes(io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto value) {
        if (namenodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNamenodesIsMutable();
          namenodes_.add(value);
          onChanged();
        } else {
          namenodesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
       */
      public Builder addNamenodes(
          int index, io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto value) {
        if (namenodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNamenodesIsMutable();
          namenodes_.add(index, value);
          onChanged();
        } else {
          namenodesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
       */
      public Builder addNamenodes(
          io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto.Builder builderForValue) {
        if (namenodesBuilder_ == null) {
          ensureNamenodesIsMutable();
          namenodes_.add(builderForValue.build());
          onChanged();
        } else {
          namenodesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
       */
      public Builder addNamenodes(
          int index, io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto.Builder builderForValue) {
        if (namenodesBuilder_ == null) {
          ensureNamenodesIsMutable();
          namenodes_.add(index, builderForValue.build());
          onChanged();
        } else {
          namenodesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
       */
      public Builder addAllNamenodes(
          java.lang.Iterable<? extends io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto> values) {
        if (namenodesBuilder_ == null) {
          ensureNamenodesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, namenodes_);
          onChanged();
        } else {
          namenodesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
       */
      public Builder clearNamenodes() {
        if (namenodesBuilder_ == null) {
          namenodes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          namenodesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
       */
      public Builder removeNamenodes(int index) {
        if (namenodesBuilder_ == null) {
          ensureNamenodesIsMutable();
          namenodes_.remove(index);
          onChanged();
        } else {
          namenodesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
       */
      public io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto.Builder getNamenodesBuilder(
          int index) {
        return getNamenodesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
       */
      public io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProtoOrBuilder getNamenodesOrBuilder(
          int index) {
        if (namenodesBuilder_ == null) {
          return namenodes_.get(index);  } else {
          return namenodesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
       */
      public java.util.List<? extends io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProtoOrBuilder> 
           getNamenodesOrBuilderList() {
        if (namenodesBuilder_ != null) {
          return namenodesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(namenodes_);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
       */
      public io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto.Builder addNamenodesBuilder() {
        return getNamenodesFieldBuilder().addBuilder(
            io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
       */
      public io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto.Builder addNamenodesBuilder(
          int index) {
        return getNamenodesFieldBuilder().addBuilder(
            index, io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.ActiveNodeProto namenodes = 1;</code>
       */
      public java.util.List<io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto.Builder> 
           getNamenodesBuilderList() {
        return getNamenodesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto, io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto.Builder, io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProtoOrBuilder> 
          getNamenodesFieldBuilder() {
        if (namenodesBuilder_ == null) {
          namenodesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto, io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto.Builder, io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProtoOrBuilder>(
                  namenodes_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          namenodes_ = null;
        }
        return namenodesBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.ActiveNamenodeListResponseProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.ActiveNamenodeListResponseProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<ActiveNamenodeListResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<ActiveNamenodeListResponseProto>() {
      @java.lang.Override
      public ActiveNamenodeListResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ActiveNamenodeListResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ActiveNamenodeListResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ActiveNamenodeListResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface NameNodeAddressRequestForBlockReportingProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.NameNodeAddressRequestForBlockReportingProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required uint64 noOfBlks = 1;</code>
     * @return Whether the noOfBlks field is set.
     */
    boolean hasNoOfBlks();
    /**
     * <code>required uint64 noOfBlks = 1;</code>
     * @return The noOfBlks.
     */
    long getNoOfBlks();

    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 2;</code>
     * @return Whether the registration field is set.
     */
    boolean hasRegistration();
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 2;</code>
     * @return The registration.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration();
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 2;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder();
  }
  /**
   * <pre>
   **
   * void request
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.NameNodeAddressRequestForBlockReportingProto}
   */
  public static final class NameNodeAddressRequestForBlockReportingProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.NameNodeAddressRequestForBlockReportingProto)
      NameNodeAddressRequestForBlockReportingProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use NameNodeAddressRequestForBlockReportingProto.newBuilder() to construct.
    private NameNodeAddressRequestForBlockReportingProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private NameNodeAddressRequestForBlockReportingProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new NameNodeAddressRequestForBlockReportingProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private NameNodeAddressRequestForBlockReportingProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              bitField0_ |= 0x00000001;
              noOfBlks_ = input.readUInt64();
              break;
            }
            case 18: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) != 0)) {
                subBuilder = registration_.toBuilder();
              }
              registration_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(registration_);
                registration_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_NameNodeAddressRequestForBlockReportingProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_NameNodeAddressRequestForBlockReportingProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto.Builder.class);
    }

    private int bitField0_;
    public static final int NOOFBLKS_FIELD_NUMBER = 1;
    private long noOfBlks_;
    /**
     * <code>required uint64 noOfBlks = 1;</code>
     * @return Whether the noOfBlks field is set.
     */
    @java.lang.Override
    public boolean hasNoOfBlks() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required uint64 noOfBlks = 1;</code>
     * @return The noOfBlks.
     */
    @java.lang.Override
    public long getNoOfBlks() {
      return noOfBlks_;
    }

    public static final int REGISTRATION_FIELD_NUMBER = 2;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto registration_;
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 2;</code>
     * @return Whether the registration field is set.
     */
    @java.lang.Override
    public boolean hasRegistration() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 2;</code>
     * @return The registration.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration() {
      return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
    }
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 2;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder() {
      return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasNoOfBlks()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasRegistration()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getRegistration().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeUInt64(1, noOfBlks_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeMessage(2, getRegistration());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(1, noOfBlks_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, getRegistration());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto) obj;

      if (hasNoOfBlks() != other.hasNoOfBlks()) return false;
      if (hasNoOfBlks()) {
        if (getNoOfBlks()
            != other.getNoOfBlks()) return false;
      }
      if (hasRegistration() != other.hasRegistration()) return false;
      if (hasRegistration()) {
        if (!getRegistration()
            .equals(other.getRegistration())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasNoOfBlks()) {
        hash = (37 * hash) + NOOFBLKS_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getNoOfBlks());
      }
      if (hasRegistration()) {
        hash = (37 * hash) + REGISTRATION_FIELD_NUMBER;
        hash = (53 * hash) + getRegistration().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * void request
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.NameNodeAddressRequestForBlockReportingProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.NameNodeAddressRequestForBlockReportingProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_NameNodeAddressRequestForBlockReportingProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_NameNodeAddressRequestForBlockReportingProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getRegistrationFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        noOfBlks_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        if (registrationBuilder_ == null) {
          registration_ = null;
        } else {
          registrationBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_NameNodeAddressRequestForBlockReportingProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.noOfBlks_ = noOfBlks_;
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          if (registrationBuilder_ == null) {
            result.registration_ = registration_;
          } else {
            result.registration_ = registrationBuilder_.build();
          }
          to_bitField0_ |= 0x00000002;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto.getDefaultInstance()) return this;
        if (other.hasNoOfBlks()) {
          setNoOfBlks(other.getNoOfBlks());
        }
        if (other.hasRegistration()) {
          mergeRegistration(other.getRegistration());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasNoOfBlks()) {
          return false;
        }
        if (!hasRegistration()) {
          return false;
        }
        if (!getRegistration().isInitialized()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long noOfBlks_ ;
      /**
       * <code>required uint64 noOfBlks = 1;</code>
       * @return Whether the noOfBlks field is set.
       */
      @java.lang.Override
      public boolean hasNoOfBlks() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required uint64 noOfBlks = 1;</code>
       * @return The noOfBlks.
       */
      @java.lang.Override
      public long getNoOfBlks() {
        return noOfBlks_;
      }
      /**
       * <code>required uint64 noOfBlks = 1;</code>
       * @param value The noOfBlks to set.
       * @return This builder for chaining.
       */
      public Builder setNoOfBlks(long value) {
        bitField0_ |= 0x00000001;
        noOfBlks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 noOfBlks = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearNoOfBlks() {
        bitField0_ = (bitField0_ & ~0x00000001);
        noOfBlks_ = 0L;
        onChanged();
        return this;
      }

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto registration_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder> registrationBuilder_;
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 2;</code>
       * @return Whether the registration field is set.
       */
      public boolean hasRegistration() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 2;</code>
       * @return The registration.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration() {
        if (registrationBuilder_ == null) {
          return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
        } else {
          return registrationBuilder_.getMessage();
        }
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 2;</code>
       */
      public Builder setRegistration(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto value) {
        if (registrationBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          registration_ = value;
          onChanged();
        } else {
          registrationBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 2;</code>
       */
      public Builder setRegistration(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder builderForValue) {
        if (registrationBuilder_ == null) {
          registration_ = builderForValue.build();
          onChanged();
        } else {
          registrationBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 2;</code>
       */
      public Builder mergeRegistration(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto value) {
        if (registrationBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0) &&
              registration_ != null &&
              registration_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance()) {
            registration_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.newBuilder(registration_).mergeFrom(value).buildPartial();
          } else {
            registration_ = value;
          }
          onChanged();
        } else {
          registrationBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 2;</code>
       */
      public Builder clearRegistration() {
        if (registrationBuilder_ == null) {
          registration_ = null;
          onChanged();
        } else {
          registrationBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder getRegistrationBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getRegistrationFieldBuilder().getBuilder();
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder() {
        if (registrationBuilder_ != null) {
          return registrationBuilder_.getMessageOrBuilder();
        } else {
          return registration_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
        }
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder>
          getRegistrationFieldBuilder() {
        if (registrationBuilder_ == null) {
          registrationBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder>(
                  getRegistration(),
                  getParentForChildren(),
                  isClean());
          registration_ = null;
        }
        return registrationBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.NameNodeAddressRequestForBlockReportingProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.NameNodeAddressRequestForBlockReportingProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<NameNodeAddressRequestForBlockReportingProto>
        PARSER = new com.google.protobuf.AbstractParser<NameNodeAddressRequestForBlockReportingProto>() {
      @java.lang.Override
      public NameNodeAddressRequestForBlockReportingProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new NameNodeAddressRequestForBlockReportingProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<NameNodeAddressRequestForBlockReportingProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<NameNodeAddressRequestForBlockReportingProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface BlockReportCompletedRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.BlockReportCompletedRequestProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return Whether the registration field is set.
     */
    boolean hasRegistration();
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return The registration.
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration();
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder();

    /**
     * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto>
        getStoragesList();
    /**
     * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto getStorages(int index);
    /**
     * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
     */
    int getStoragesCount();
    /**
     * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder>
        getStoragesOrBuilderList();
    /**
     * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
     */
    org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder getStoragesOrBuilder(
        int index);

    /**
     * <code>required bool success = 3;</code>
     * @return Whether the success field is set.
     */
    boolean hasSuccess();
    /**
     * <code>required bool success = 3;</code>
     * @return The success.
     */
    boolean getSuccess();
  }
  /**
   * <pre>
   **
   * void request
   * </pre>
   *
   * Protobuf type {@code org.apache.hadoop.datanode.BlockReportCompletedRequestProto}
   */
  public static final class BlockReportCompletedRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.BlockReportCompletedRequestProto)
      BlockReportCompletedRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use BlockReportCompletedRequestProto.newBuilder() to construct.
    private BlockReportCompletedRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private BlockReportCompletedRequestProto() {
      storages_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new BlockReportCompletedRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private BlockReportCompletedRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = registration_.toBuilder();
              }
              registration_ = input.readMessage(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(registration_);
                registration_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                storages_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              storages_.add(
                  input.readMessage(org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.PARSER, extensionRegistry));
              break;
            }
            case 24: {
              bitField0_ |= 0x00000002;
              success_ = input.readBool();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          storages_ = java.util.Collections.unmodifiableList(storages_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int REGISTRATION_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto registration_;
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return Whether the registration field is set.
     */
    @java.lang.Override
    public boolean hasRegistration() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     * @return The registration.
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration() {
      return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
    }
    /**
     * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder() {
      return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
    }

    public static final int STORAGES_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto> storages_;
    /**
     * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto> getStoragesList() {
      return storages_;
    }
    /**
     * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder>
        getStoragesOrBuilderList() {
      return storages_;
    }
    /**
     * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
     */
    @java.lang.Override
    public int getStoragesCount() {
      return storages_.size();
    }
    /**
     * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto getStorages(int index) {
      return storages_.get(index);
    }
    /**
     * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder getStoragesOrBuilder(
        int index) {
      return storages_.get(index);
    }

    public static final int SUCCESS_FIELD_NUMBER = 3;
    private boolean success_;
    /**
     * <code>required bool success = 3;</code>
     * @return Whether the success field is set.
     */
    @java.lang.Override
    public boolean hasSuccess() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>required bool success = 3;</code>
     * @return The success.
     */
    @java.lang.Override
    public boolean getSuccess() {
      return success_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasRegistration()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasSuccess()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getRegistration().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      for (int i = 0; i < getStoragesCount(); i++) {
        if (!getStorages(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getRegistration());
      }
      for (int i = 0; i < storages_.size(); i++) {
        output.writeMessage(2, storages_.get(i));
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeBool(3, success_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getRegistration());
      }
      for (int i = 0; i < storages_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, storages_.get(i));
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(3, success_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto) obj;

      if (hasRegistration() != other.hasRegistration()) return false;
      if (hasRegistration()) {
        if (!getRegistration()
            .equals(other.getRegistration())) return false;
      }
      if (!getStoragesList()
          .equals(other.getStoragesList())) return false;
      if (hasSuccess() != other.hasSuccess()) return false;
      if (hasSuccess()) {
        if (getSuccess()
            != other.getSuccess()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasRegistration()) {
        hash = (37 * hash) + REGISTRATION_FIELD_NUMBER;
        hash = (53 * hash) + getRegistration().hashCode();
      }
      if (getStoragesCount() > 0) {
        hash = (37 * hash) + STORAGES_FIELD_NUMBER;
        hash = (53 * hash) + getStoragesList().hashCode();
      }
      if (hasSuccess()) {
        hash = (37 * hash) + SUCCESS_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getSuccess());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * void request
     * </pre>
     *
     * Protobuf type {@code org.apache.hadoop.datanode.BlockReportCompletedRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.BlockReportCompletedRequestProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getRegistrationFieldBuilder();
          getStoragesFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (registrationBuilder_ == null) {
          registration_ = null;
        } else {
          registrationBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (storagesBuilder_ == null) {
          storages_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          storagesBuilder_.clear();
        }
        success_ = false;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (registrationBuilder_ == null) {
            result.registration_ = registration_;
          } else {
            result.registration_ = registrationBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (storagesBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0)) {
            storages_ = java.util.Collections.unmodifiableList(storages_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.storages_ = storages_;
        } else {
          result.storages_ = storagesBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.success_ = success_;
          to_bitField0_ |= 0x00000002;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto.getDefaultInstance()) return this;
        if (other.hasRegistration()) {
          mergeRegistration(other.getRegistration());
        }
        if (storagesBuilder_ == null) {
          if (!other.storages_.isEmpty()) {
            if (storages_.isEmpty()) {
              storages_ = other.storages_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureStoragesIsMutable();
              storages_.addAll(other.storages_);
            }
            onChanged();
          }
        } else {
          if (!other.storages_.isEmpty()) {
            if (storagesBuilder_.isEmpty()) {
              storagesBuilder_.dispose();
              storagesBuilder_ = null;
              storages_ = other.storages_;
              bitField0_ = (bitField0_ & ~0x00000002);
              storagesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getStoragesFieldBuilder() : null;
            } else {
              storagesBuilder_.addAllMessages(other.storages_);
            }
          }
        }
        if (other.hasSuccess()) {
          setSuccess(other.getSuccess());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasRegistration()) {
          return false;
        }
        if (!hasSuccess()) {
          return false;
        }
        if (!getRegistration().isInitialized()) {
          return false;
        }
        for (int i = 0; i < getStoragesCount(); i++) {
          if (!getStorages(i).isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto registration_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder> registrationBuilder_;
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       * @return Whether the registration field is set.
       */
      public boolean hasRegistration() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       * @return The registration.
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto getRegistration() {
        if (registrationBuilder_ == null) {
          return registration_ == null ? org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
        } else {
          return registrationBuilder_.getMessage();
        }
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder setRegistration(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto value) {
        if (registrationBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          registration_ = value;
          onChanged();
        } else {
          registrationBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder setRegistration(
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder builderForValue) {
        if (registrationBuilder_ == null) {
          registration_ = builderForValue.build();
          onChanged();
        } else {
          registrationBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder mergeRegistration(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto value) {
        if (registrationBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              registration_ != null &&
              registration_ != org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance()) {
            registration_ =
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.newBuilder(registration_).mergeFrom(value).buildPartial();
          } else {
            registration_ = value;
          }
          onChanged();
        } else {
          registrationBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public Builder clearRegistration() {
        if (registrationBuilder_ == null) {
          registration_ = null;
          onChanged();
        } else {
          registrationBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder getRegistrationBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getRegistrationFieldBuilder().getBuilder();
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder getRegistrationOrBuilder() {
        if (registrationBuilder_ != null) {
          return registrationBuilder_.getMessageOrBuilder();
        } else {
          return registration_ == null ?
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.getDefaultInstance() : registration_;
        }
      }
      /**
       * <code>required .org.apache.hadoop.datanode.DatanodeRegistrationProto registration = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder>
          getRegistrationFieldBuilder() {
        if (registrationBuilder_ == null) {
          registrationBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProto.Builder, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeRegistrationProtoOrBuilder>(
                  getRegistration(),
                  getParentForChildren(),
                  isClean());
          registration_ = null;
        }
        return registrationBuilder_;
      }

      private java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto> storages_ =
        java.util.Collections.emptyList();
      private void ensureStoragesIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          storages_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto>(storages_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder> storagesBuilder_;

      /**
       * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto> getStoragesList() {
        if (storagesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(storages_);
        } else {
          return storagesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
       */
      public int getStoragesCount() {
        if (storagesBuilder_ == null) {
          return storages_.size();
        } else {
          return storagesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto getStorages(int index) {
        if (storagesBuilder_ == null) {
          return storages_.get(index);
        } else {
          return storagesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
       */
      public Builder setStorages(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto value) {
        if (storagesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStoragesIsMutable();
          storages_.set(index, value);
          onChanged();
        } else {
          storagesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
       */
      public Builder setStorages(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder builderForValue) {
        if (storagesBuilder_ == null) {
          ensureStoragesIsMutable();
          storages_.set(index, builderForValue.build());
          onChanged();
        } else {
          storagesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
       */
      public Builder addStorages(org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto value) {
        if (storagesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStoragesIsMutable();
          storages_.add(value);
          onChanged();
        } else {
          storagesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
       */
      public Builder addStorages(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto value) {
        if (storagesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStoragesIsMutable();
          storages_.add(index, value);
          onChanged();
        } else {
          storagesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
       */
      public Builder addStorages(
          org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder builderForValue) {
        if (storagesBuilder_ == null) {
          ensureStoragesIsMutable();
          storages_.add(builderForValue.build());
          onChanged();
        } else {
          storagesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
       */
      public Builder addStorages(
          int index, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder builderForValue) {
        if (storagesBuilder_ == null) {
          ensureStoragesIsMutable();
          storages_.add(index, builderForValue.build());
          onChanged();
        } else {
          storagesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
       */
      public Builder addAllStorages(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto> values) {
        if (storagesBuilder_ == null) {
          ensureStoragesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, storages_);
          onChanged();
        } else {
          storagesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
       */
      public Builder clearStorages() {
        if (storagesBuilder_ == null) {
          storages_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          storagesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
       */
      public Builder removeStorages(int index) {
        if (storagesBuilder_ == null) {
          ensureStoragesIsMutable();
          storages_.remove(index);
          onChanged();
        } else {
          storagesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder getStoragesBuilder(
          int index) {
        return getStoragesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder getStoragesOrBuilder(
          int index) {
        if (storagesBuilder_ == null) {
          return storages_.get(index);  } else {
          return storagesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder>
           getStoragesOrBuilderList() {
        if (storagesBuilder_ != null) {
          return storagesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(storages_);
        }
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder addStoragesBuilder() {
        return getStoragesFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder addStoragesBuilder(
          int index) {
        return getStoragesFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.getDefaultInstance());
      }
      /**
       * <code>repeated .org.apache.hadoop.DatanodeStorageProto storages = 2;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder>
           getStoragesBuilderList() {
        return getStoragesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder>
          getStoragesFieldBuilder() {
        if (storagesBuilder_ == null) {
          storagesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProto.Builder, org.apache.hadoop.hdfs.protocol.HdfsProtos.DatanodeStorageProtoOrBuilder>(
                  storages_,
                  ((bitField0_ & 0x00000002) != 0),
                  getParentForChildren(),
                  isClean());
          storages_ = null;
        }
        return storagesBuilder_;
      }

      private boolean success_ ;
      /**
       * <code>required bool success = 3;</code>
       * @return Whether the success field is set.
       */
      @java.lang.Override
      public boolean hasSuccess() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>required bool success = 3;</code>
       * @return The success.
       */
      @java.lang.Override
      public boolean getSuccess() {
        return success_;
      }
      /**
       * <code>required bool success = 3;</code>
       * @param value The success to set.
       * @return This builder for chaining.
       */
      public Builder setSuccess(boolean value) {
        bitField0_ |= 0x00000004;
        success_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bool success = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearSuccess() {
        bitField0_ = (bitField0_ & ~0x00000004);
        success_ = false;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.BlockReportCompletedRequestProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.BlockReportCompletedRequestProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<BlockReportCompletedRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<BlockReportCompletedRequestProto>() {
      @java.lang.Override
      public BlockReportCompletedRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new BlockReportCompletedRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<BlockReportCompletedRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<BlockReportCompletedRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface GetSmallFileDataProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.GetSmallFileDataProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required uint32 id = 1;</code>
     * @return Whether the id field is set.
     */
    boolean hasId();
    /**
     * <code>required uint32 id = 1;</code>
     * @return The id.
     */
    int getId();
  }
  /**
   * Protobuf type {@code org.apache.hadoop.datanode.GetSmallFileDataProto}
   */
  public static final class GetSmallFileDataProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.GetSmallFileDataProto)
      GetSmallFileDataProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use GetSmallFileDataProto.newBuilder() to construct.
    private GetSmallFileDataProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private GetSmallFileDataProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new GetSmallFileDataProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private GetSmallFileDataProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              bitField0_ |= 0x00000001;
              id_ = input.readUInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_GetSmallFileDataProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_GetSmallFileDataProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto.Builder.class);
    }

    private int bitField0_;
    public static final int ID_FIELD_NUMBER = 1;
    private int id_;
    /**
     * <code>required uint32 id = 1;</code>
     * @return Whether the id field is set.
     */
    @java.lang.Override
    public boolean hasId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required uint32 id = 1;</code>
     * @return The id.
     */
    @java.lang.Override
    public int getId() {
      return id_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeUInt32(1, id_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(1, id_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto) obj;

      if (hasId() != other.hasId()) return false;
      if (hasId()) {
        if (getId()
            != other.getId()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasId()) {
        hash = (37 * hash) + ID_FIELD_NUMBER;
        hash = (53 * hash) + getId();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.hadoop.datanode.GetSmallFileDataProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.GetSmallFileDataProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_GetSmallFileDataProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_GetSmallFileDataProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        id_ = 0;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_GetSmallFileDataProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.id_ = id_;
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto.getDefaultInstance()) return this;
        if (other.hasId()) {
          setId(other.getId());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasId()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int id_ ;
      /**
       * <code>required uint32 id = 1;</code>
       * @return Whether the id field is set.
       */
      @java.lang.Override
      public boolean hasId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required uint32 id = 1;</code>
       * @return The id.
       */
      @java.lang.Override
      public int getId() {
        return id_;
      }
      /**
       * <code>required uint32 id = 1;</code>
       * @param value The id to set.
       * @return This builder for chaining.
       */
      public Builder setId(int value) {
        bitField0_ |= 0x00000001;
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint32 id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        id_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.GetSmallFileDataProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.GetSmallFileDataProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<GetSmallFileDataProto>
        PARSER = new com.google.protobuf.AbstractParser<GetSmallFileDataProto>() {
      @java.lang.Override
      public GetSmallFileDataProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetSmallFileDataProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<GetSmallFileDataProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<GetSmallFileDataProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SmallFileDataResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.SmallFileDataResponseProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required bytes data = 1;</code>
     * @return Whether the data field is set.
     */
    boolean hasData();
    /**
     * <code>required bytes data = 1;</code>
     * @return The data.
     */
    com.google.protobuf.ByteString getData();
  }
  /**
   * Protobuf type {@code org.apache.hadoop.datanode.SmallFileDataResponseProto}
   */
  public static final class SmallFileDataResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.SmallFileDataResponseProto)
      SmallFileDataResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SmallFileDataResponseProto.newBuilder() to construct.
    private SmallFileDataResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SmallFileDataResponseProto() {
      data_ = com.google.protobuf.ByteString.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SmallFileDataResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SmallFileDataResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              bitField0_ |= 0x00000001;
              data_ = input.readBytes();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_SmallFileDataResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_SmallFileDataResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto.Builder.class);
    }

    private int bitField0_;
    public static final int DATA_FIELD_NUMBER = 1;
    private com.google.protobuf.ByteString data_;
    /**
     * <code>required bytes data = 1;</code>
     * @return Whether the data field is set.
     */
    @java.lang.Override
    public boolean hasData() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required bytes data = 1;</code>
     * @return The data.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString getData() {
      return data_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasData()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeBytes(1, data_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, data_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto) obj;

      if (hasData() != other.hasData()) return false;
      if (hasData()) {
        if (!getData()
            .equals(other.getData())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasData()) {
        hash = (37 * hash) + DATA_FIELD_NUMBER;
        hash = (53 * hash) + getData().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.hadoop.datanode.SmallFileDataResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.SmallFileDataResponseProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_SmallFileDataResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_SmallFileDataResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        data_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_SmallFileDataResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.data_ = data_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto.getDefaultInstance()) return this;
        if (other.hasData()) {
          setData(other.getData());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasData()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.ByteString data_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes data = 1;</code>
       * @return Whether the data field is set.
       */
      @java.lang.Override
      public boolean hasData() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required bytes data = 1;</code>
       * @return The data.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString getData() {
        return data_;
      }
      /**
       * <code>required bytes data = 1;</code>
       * @param value The data to set.
       * @return This builder for chaining.
       */
      public Builder setData(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        data_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes data = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearData() {
        bitField0_ = (bitField0_ & ~0x00000001);
        data_ = getDefaultInstance().getData();
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.SmallFileDataResponseProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.SmallFileDataResponseProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<SmallFileDataResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<SmallFileDataResponseProto>() {
      @java.lang.Override
      public SmallFileDataResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SmallFileDataResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SmallFileDataResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SmallFileDataResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface BlockReportCompletedResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.hadoop.datanode.BlockReportCompletedResponseProto)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code org.apache.hadoop.datanode.BlockReportCompletedResponseProto}
   */
  public static final class BlockReportCompletedResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.hadoop.datanode.BlockReportCompletedResponseProto)
      BlockReportCompletedResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use BlockReportCompletedResponseProto.newBuilder() to construct.
    private BlockReportCompletedResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private BlockReportCompletedResponseProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new BlockReportCompletedResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private BlockReportCompletedResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto other = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto) obj;

      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.hadoop.datanode.BlockReportCompletedResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.hadoop.datanode.BlockReportCompletedResponseProto)
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto.class, org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto build() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto result = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.datanode.BlockReportCompletedResponseProto)
    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.BlockReportCompletedResponseProto)
    private static final org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto();
    }

    public static org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<BlockReportCompletedResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<BlockReportCompletedResponseProto>() {
      @java.lang.Override
      public BlockReportCompletedResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new BlockReportCompletedResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<BlockReportCompletedResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<BlockReportCompletedResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  /**
   * <pre>
   **
   * Protocol used from datanode to the namenode
   * See the request and response for details of rpc call.
   * </pre>
   *
   * Protobuf service {@code org.apache.hadoop.datanode.DatanodeProtocolService}
   */
  public static abstract class DatanodeProtocolService
      implements com.google.protobuf.Service {
    protected DatanodeProtocolService() {}

    public interface Interface {
      /**
       * <pre>
       **
       * Register a datanode at a namenode
       * </pre>
       *
       * <code>rpc registerDatanode(.org.apache.hadoop.datanode.RegisterDatanodeRequestProto) returns (.org.apache.hadoop.datanode.RegisterDatanodeResponseProto);</code>
       */
      public abstract void registerDatanode(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto> done);

      /**
       * <pre>
       **
       * Send heartbeat from datanode to namenode
       * </pre>
       *
       * <code>rpc sendHeartbeat(.org.apache.hadoop.datanode.HeartbeatRequestProto) returns (.org.apache.hadoop.datanode.HeartbeatResponseProto);</code>
       */
      public abstract void sendHeartbeat(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto> done);

      /**
       * <pre>
       **
       * Report blocks at a given datanode to the namenode
       * </pre>
       *
       * <code>rpc blockReport(.org.apache.hadoop.datanode.BlockReportRequestProto) returns (.org.apache.hadoop.datanode.BlockReportResponseProto);</code>
       */
      public abstract void blockReport(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto> done);

      /**
       * <pre>
       **
       * Report hashes at a given datanode to the namenode
       * </pre>
       *
       * <code>rpc reportHashes(.org.apache.hadoop.datanode.BlockReportRequestProto) returns (.org.apache.hadoop.datanode.BlockReportResponseProto);</code>
       */
      public abstract void reportHashes(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto> done);

      /**
       * <pre>
       **
       * Report cached blocks at a datanode to the namenode
       * </pre>
       *
       * <code>rpc cacheReport(.org.apache.hadoop.datanode.CacheReportRequestProto) returns (.org.apache.hadoop.datanode.CacheReportResponseProto);</code>
       */
      public abstract void cacheReport(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto> done);

      /**
       * <pre>
       **
       * Incremental block report from the DN. This contains info about recently
       * received and deleted blocks, as well as when blocks start being
       * received.
       * </pre>
       *
       * <code>rpc blockReceivedAndDeleted(.org.apache.hadoop.datanode.BlockReceivedAndDeletedRequestProto) returns (.org.apache.hadoop.datanode.BlockReceivedAndDeletedResponseProto);</code>
       */
      public abstract void blockReceivedAndDeleted(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto> done);

      /**
       * <pre>
       **
       * Report from a datanode of an error to the active namenode.
       * Used for debugging.
       * </pre>
       *
       * <code>rpc errorReport(.org.apache.hadoop.datanode.ErrorReportRequestProto) returns (.org.apache.hadoop.datanode.ErrorReportResponseProto);</code>
       */
      public abstract void errorReport(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto> done);

      /**
       * <pre>
       **
       * Request the version
       * </pre>
       *
       * <code>rpc versionRequest(.org.apache.hadoop.VersionRequestProto) returns (.org.apache.hadoop.VersionResponseProto);</code>
       */
      public abstract void versionRequest(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionResponseProto> done);

      /**
       * <pre>
       **
       * Report corrupt blocks at the specified location
       * </pre>
       *
       * <code>rpc reportBadBlocks(.org.apache.hadoop.datanode.ReportBadBlocksRequestProto) returns (.org.apache.hadoop.datanode.ReportBadBlocksResponseProto);</code>
       */
      public abstract void reportBadBlocks(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto> done);

      /**
       * <pre>
       **
       * Commit block synchronization during lease recovery.
       * </pre>
       *
       * <code>rpc commitBlockSynchronization(.org.apache.hadoop.datanode.CommitBlockSynchronizationRequestProto) returns (.org.apache.hadoop.datanode.CommitBlockSynchronizationResponseProto);</code>
       */
      public abstract void commitBlockSynchronization(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto> done);

      /**
       * <pre>
       **
       * data node sends a request to name node to get list of all active
       * name nodes.
       * </pre>
       *
       * <code>rpc getActiveNamenodes(.org.apache.hadoop.datanode.ActiveNamenodeListRequestProto) returns (.org.apache.hadoop.datanode.ActiveNamenodeListResponseProto);</code>
       */
      public abstract void getActiveNamenodes(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto> done);

      /**
       * <pre>
       **
       * Ask the leader which namenode the datanode should report to.
       * </pre>
       *
       * <code>rpc getNextNamenodeToSendBlockReport(.org.apache.hadoop.datanode.NameNodeAddressRequestForBlockReportingProto) returns (.org.apache.hadoop.ActiveNodeProto);</code>
       */
      public abstract void getNextNamenodeToSendBlockReport(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto request,
          com.google.protobuf.RpcCallback<io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto> done);

      /**
       * <pre>
       **
       * Request leader NN to mark the block report completed.
       * </pre>
       *
       * <code>rpc blockReportCompleted(.org.apache.hadoop.datanode.BlockReportCompletedRequestProto) returns (.org.apache.hadoop.datanode.BlockReportCompletedResponseProto);</code>
       */
      public abstract void blockReportCompleted(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto> done);

      /**
       * <pre>
       **
       * Ask a namenode to read the small file data
       * </pre>
       *
       * <code>rpc getSmallFileData(.org.apache.hadoop.datanode.GetSmallFileDataProto) returns (.org.apache.hadoop.datanode.SmallFileDataResponseProto);</code>
       */
      public abstract void getSmallFileData(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto> done);

    }

    public static com.google.protobuf.Service newReflectiveService(
        final Interface impl) {
      return new DatanodeProtocolService() {
        @java.lang.Override
        public  void registerDatanode(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto> done) {
          impl.registerDatanode(controller, request, done);
        }

        @java.lang.Override
        public  void sendHeartbeat(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto> done) {
          impl.sendHeartbeat(controller, request, done);
        }

        @java.lang.Override
        public  void blockReport(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto> done) {
          impl.blockReport(controller, request, done);
        }

        @java.lang.Override
        public  void reportHashes(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto> done) {
          impl.reportHashes(controller, request, done);
        }

        @java.lang.Override
        public  void cacheReport(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto> done) {
          impl.cacheReport(controller, request, done);
        }

        @java.lang.Override
        public  void blockReceivedAndDeleted(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto> done) {
          impl.blockReceivedAndDeleted(controller, request, done);
        }

        @java.lang.Override
        public  void errorReport(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto> done) {
          impl.errorReport(controller, request, done);
        }

        @java.lang.Override
        public  void versionRequest(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionResponseProto> done) {
          impl.versionRequest(controller, request, done);
        }

        @java.lang.Override
        public  void reportBadBlocks(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto> done) {
          impl.reportBadBlocks(controller, request, done);
        }

        @java.lang.Override
        public  void commitBlockSynchronization(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto> done) {
          impl.commitBlockSynchronization(controller, request, done);
        }

        @java.lang.Override
        public  void getActiveNamenodes(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto> done) {
          impl.getActiveNamenodes(controller, request, done);
        }

        @java.lang.Override
        public  void getNextNamenodeToSendBlockReport(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto request,
            com.google.protobuf.RpcCallback<io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto> done) {
          impl.getNextNamenodeToSendBlockReport(controller, request, done);
        }

        @java.lang.Override
        public  void blockReportCompleted(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto> done) {
          impl.blockReportCompleted(controller, request, done);
        }

        @java.lang.Override
        public  void getSmallFileData(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto> done) {
          impl.getSmallFileData(controller, request, done);
        }

      };
    }

    public static com.google.protobuf.BlockingService
        newReflectiveBlockingService(final BlockingInterface impl) {
      return new com.google.protobuf.BlockingService() {
        public final com.google.protobuf.Descriptors.ServiceDescriptor
            getDescriptorForType() {
          return getDescriptor();
        }

        public final com.google.protobuf.Message callBlockingMethod(
            com.google.protobuf.Descriptors.MethodDescriptor method,
            com.google.protobuf.RpcController controller,
            com.google.protobuf.Message request)
            throws com.google.protobuf.ServiceException {
          if (method.getService() != getDescriptor()) {
            throw new java.lang.IllegalArgumentException(
              "Service.callBlockingMethod() given method descriptor for " +
              "wrong service type.");
          }
          switch(method.getIndex()) {
            case 0:
              return impl.registerDatanode(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto)request);
            case 1:
              return impl.sendHeartbeat(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto)request);
            case 2:
              return impl.blockReport(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto)request);
            case 3:
              return impl.reportHashes(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto)request);
            case 4:
              return impl.cacheReport(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto)request);
            case 5:
              return impl.blockReceivedAndDeleted(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto)request);
            case 6:
              return impl.errorReport(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto)request);
            case 7:
              return impl.versionRequest(controller, (org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionRequestProto)request);
            case 8:
              return impl.reportBadBlocks(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto)request);
            case 9:
              return impl.commitBlockSynchronization(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto)request);
            case 10:
              return impl.getActiveNamenodes(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto)request);
            case 11:
              return impl.getNextNamenodeToSendBlockReport(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto)request);
            case 12:
              return impl.blockReportCompleted(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto)request);
            case 13:
              return impl.getSmallFileData(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto)request);
            default:
              throw new java.lang.AssertionError("Can't get here.");
          }
        }

        public final com.google.protobuf.Message
            getRequestPrototype(
            com.google.protobuf.Descriptors.MethodDescriptor method) {
          if (method.getService() != getDescriptor()) {
            throw new java.lang.IllegalArgumentException(
              "Service.getRequestPrototype() given method " +
              "descriptor for wrong service type.");
          }
          switch(method.getIndex()) {
            case 0:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto.getDefaultInstance();
            case 1:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto.getDefaultInstance();
            case 2:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto.getDefaultInstance();
            case 3:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto.getDefaultInstance();
            case 4:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto.getDefaultInstance();
            case 5:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto.getDefaultInstance();
            case 6:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto.getDefaultInstance();
            case 7:
              return org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionRequestProto.getDefaultInstance();
            case 8:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto.getDefaultInstance();
            case 9:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto.getDefaultInstance();
            case 10:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto.getDefaultInstance();
            case 11:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto.getDefaultInstance();
            case 12:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto.getDefaultInstance();
            case 13:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto.getDefaultInstance();
            default:
              throw new java.lang.AssertionError("Can't get here.");
          }
        }

        public final com.google.protobuf.Message
            getResponsePrototype(
            com.google.protobuf.Descriptors.MethodDescriptor method) {
          if (method.getService() != getDescriptor()) {
            throw new java.lang.IllegalArgumentException(
              "Service.getResponsePrototype() given method " +
              "descriptor for wrong service type.");
          }
          switch(method.getIndex()) {
            case 0:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto.getDefaultInstance();
            case 1:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto.getDefaultInstance();
            case 2:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto.getDefaultInstance();
            case 3:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto.getDefaultInstance();
            case 4:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto.getDefaultInstance();
            case 5:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto.getDefaultInstance();
            case 6:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto.getDefaultInstance();
            case 7:
              return org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionResponseProto.getDefaultInstance();
            case 8:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto.getDefaultInstance();
            case 9:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto.getDefaultInstance();
            case 10:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto.getDefaultInstance();
            case 11:
              return io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto.getDefaultInstance();
            case 12:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto.getDefaultInstance();
            case 13:
              return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto.getDefaultInstance();
            default:
              throw new java.lang.AssertionError("Can't get here.");
          }
        }

      };
    }

    /**
     * <pre>
     **
     * Register a datanode at a namenode
     * </pre>
     *
     * <code>rpc registerDatanode(.org.apache.hadoop.datanode.RegisterDatanodeRequestProto) returns (.org.apache.hadoop.datanode.RegisterDatanodeResponseProto);</code>
     */
    public abstract void registerDatanode(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto> done);

    /**
     * <pre>
     **
     * Send heartbeat from datanode to namenode
     * </pre>
     *
     * <code>rpc sendHeartbeat(.org.apache.hadoop.datanode.HeartbeatRequestProto) returns (.org.apache.hadoop.datanode.HeartbeatResponseProto);</code>
     */
    public abstract void sendHeartbeat(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto> done);

    /**
     * <pre>
     **
     * Report blocks at a given datanode to the namenode
     * </pre>
     *
     * <code>rpc blockReport(.org.apache.hadoop.datanode.BlockReportRequestProto) returns (.org.apache.hadoop.datanode.BlockReportResponseProto);</code>
     */
    public abstract void blockReport(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto> done);

    /**
     * <pre>
     **
     * Report hashes at a given datanode to the namenode
     * </pre>
     *
     * <code>rpc reportHashes(.org.apache.hadoop.datanode.BlockReportRequestProto) returns (.org.apache.hadoop.datanode.BlockReportResponseProto);</code>
     */
    public abstract void reportHashes(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto> done);

    /**
     * <pre>
     **
     * Report cached blocks at a datanode to the namenode
     * </pre>
     *
     * <code>rpc cacheReport(.org.apache.hadoop.datanode.CacheReportRequestProto) returns (.org.apache.hadoop.datanode.CacheReportResponseProto);</code>
     */
    public abstract void cacheReport(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto> done);

    /**
     * <pre>
     **
     * Incremental block report from the DN. This contains info about recently
     * received and deleted blocks, as well as when blocks start being
     * received.
     * </pre>
     *
     * <code>rpc blockReceivedAndDeleted(.org.apache.hadoop.datanode.BlockReceivedAndDeletedRequestProto) returns (.org.apache.hadoop.datanode.BlockReceivedAndDeletedResponseProto);</code>
     */
    public abstract void blockReceivedAndDeleted(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto> done);

    /**
     * <pre>
     **
     * Report from a datanode of an error to the active namenode.
     * Used for debugging.
     * </pre>
     *
     * <code>rpc errorReport(.org.apache.hadoop.datanode.ErrorReportRequestProto) returns (.org.apache.hadoop.datanode.ErrorReportResponseProto);</code>
     */
    public abstract void errorReport(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto> done);

    /**
     * <pre>
     **
     * Request the version
     * </pre>
     *
     * <code>rpc versionRequest(.org.apache.hadoop.VersionRequestProto) returns (.org.apache.hadoop.VersionResponseProto);</code>
     */
    public abstract void versionRequest(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionResponseProto> done);

    /**
     * <pre>
     **
     * Report corrupt blocks at the specified location
     * </pre>
     *
     * <code>rpc reportBadBlocks(.org.apache.hadoop.datanode.ReportBadBlocksRequestProto) returns (.org.apache.hadoop.datanode.ReportBadBlocksResponseProto);</code>
     */
    public abstract void reportBadBlocks(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto> done);

    /**
     * <pre>
     **
     * Commit block synchronization during lease recovery.
     * </pre>
     *
     * <code>rpc commitBlockSynchronization(.org.apache.hadoop.datanode.CommitBlockSynchronizationRequestProto) returns (.org.apache.hadoop.datanode.CommitBlockSynchronizationResponseProto);</code>
     */
    public abstract void commitBlockSynchronization(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto> done);

    /**
     * <pre>
     **
     * data node sends a request to name node to get list of all active
     * name nodes.
     * </pre>
     *
     * <code>rpc getActiveNamenodes(.org.apache.hadoop.datanode.ActiveNamenodeListRequestProto) returns (.org.apache.hadoop.datanode.ActiveNamenodeListResponseProto);</code>
     */
    public abstract void getActiveNamenodes(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto> done);

    /**
     * <pre>
     **
     * Ask the leader which namenode the datanode should report to.
     * </pre>
     *
     * <code>rpc getNextNamenodeToSendBlockReport(.org.apache.hadoop.datanode.NameNodeAddressRequestForBlockReportingProto) returns (.org.apache.hadoop.ActiveNodeProto);</code>
     */
    public abstract void getNextNamenodeToSendBlockReport(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto request,
        com.google.protobuf.RpcCallback<io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto> done);

    /**
     * <pre>
     **
     * Request leader NN to mark the block report completed.
     * </pre>
     *
     * <code>rpc blockReportCompleted(.org.apache.hadoop.datanode.BlockReportCompletedRequestProto) returns (.org.apache.hadoop.datanode.BlockReportCompletedResponseProto);</code>
     */
    public abstract void blockReportCompleted(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto> done);

    /**
     * <pre>
     **
     * Ask a namenode to read the small file data
     * </pre>
     *
     * <code>rpc getSmallFileData(.org.apache.hadoop.datanode.GetSmallFileDataProto) returns (.org.apache.hadoop.datanode.SmallFileDataResponseProto);</code>
     */
    public abstract void getSmallFileData(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto> done);

    public static final
        com.google.protobuf.Descriptors.ServiceDescriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.getDescriptor().getServices().get(0);
    }
    public final com.google.protobuf.Descriptors.ServiceDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }

    public final void callMethod(
        com.google.protobuf.Descriptors.MethodDescriptor method,
        com.google.protobuf.RpcController controller,
        com.google.protobuf.Message request,
        com.google.protobuf.RpcCallback<
          com.google.protobuf.Message> done) {
      if (method.getService() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "Service.callMethod() given method descriptor for wrong " +
          "service type.");
      }
      switch(method.getIndex()) {
        case 0:
          this.registerDatanode(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto>specializeCallback(
              done));
          return;
        case 1:
          this.sendHeartbeat(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto>specializeCallback(
              done));
          return;
        case 2:
          this.blockReport(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto>specializeCallback(
              done));
          return;
        case 3:
          this.reportHashes(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto>specializeCallback(
              done));
          return;
        case 4:
          this.cacheReport(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto>specializeCallback(
              done));
          return;
        case 5:
          this.blockReceivedAndDeleted(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto>specializeCallback(
              done));
          return;
        case 6:
          this.errorReport(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto>specializeCallback(
              done));
          return;
        case 7:
          this.versionRequest(controller, (org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionResponseProto>specializeCallback(
              done));
          return;
        case 8:
          this.reportBadBlocks(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto>specializeCallback(
              done));
          return;
        case 9:
          this.commitBlockSynchronization(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto>specializeCallback(
              done));
          return;
        case 10:
          this.getActiveNamenodes(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto>specializeCallback(
              done));
          return;
        case 11:
          this.getNextNamenodeToSendBlockReport(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto)request,
            com.google.protobuf.RpcUtil.<io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto>specializeCallback(
              done));
          return;
        case 12:
          this.blockReportCompleted(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto>specializeCallback(
              done));
          return;
        case 13:
          this.getSmallFileData(controller, (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto>specializeCallback(
              done));
          return;
        default:
          throw new java.lang.AssertionError("Can't get here.");
      }
    }

    public final com.google.protobuf.Message
        getRequestPrototype(
        com.google.protobuf.Descriptors.MethodDescriptor method) {
      if (method.getService() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "Service.getRequestPrototype() given method " +
          "descriptor for wrong service type.");
      }
      switch(method.getIndex()) {
        case 0:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto.getDefaultInstance();
        case 1:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto.getDefaultInstance();
        case 2:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto.getDefaultInstance();
        case 3:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto.getDefaultInstance();
        case 4:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto.getDefaultInstance();
        case 5:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto.getDefaultInstance();
        case 6:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto.getDefaultInstance();
        case 7:
          return org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionRequestProto.getDefaultInstance();
        case 8:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto.getDefaultInstance();
        case 9:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto.getDefaultInstance();
        case 10:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto.getDefaultInstance();
        case 11:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto.getDefaultInstance();
        case 12:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto.getDefaultInstance();
        case 13:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto.getDefaultInstance();
        default:
          throw new java.lang.AssertionError("Can't get here.");
      }
    }

    public final com.google.protobuf.Message
        getResponsePrototype(
        com.google.protobuf.Descriptors.MethodDescriptor method) {
      if (method.getService() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "Service.getResponsePrototype() given method " +
          "descriptor for wrong service type.");
      }
      switch(method.getIndex()) {
        case 0:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto.getDefaultInstance();
        case 1:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto.getDefaultInstance();
        case 2:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto.getDefaultInstance();
        case 3:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto.getDefaultInstance();
        case 4:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto.getDefaultInstance();
        case 5:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto.getDefaultInstance();
        case 6:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto.getDefaultInstance();
        case 7:
          return org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionResponseProto.getDefaultInstance();
        case 8:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto.getDefaultInstance();
        case 9:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto.getDefaultInstance();
        case 10:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto.getDefaultInstance();
        case 11:
          return io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto.getDefaultInstance();
        case 12:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto.getDefaultInstance();
        case 13:
          return org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto.getDefaultInstance();
        default:
          throw new java.lang.AssertionError("Can't get here.");
      }
    }

    public static Stub newStub(
        com.google.protobuf.RpcChannel channel) {
      return new Stub(channel);
    }

    public static final class Stub extends org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.DatanodeProtocolService implements Interface {
      private Stub(com.google.protobuf.RpcChannel channel) {
        this.channel = channel;
      }

      private final com.google.protobuf.RpcChannel channel;

      public com.google.protobuf.RpcChannel getChannel() {
        return channel;
      }

      public  void registerDatanode(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(0),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto.class,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto.getDefaultInstance()));
      }

      public  void sendHeartbeat(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(1),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto.class,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto.getDefaultInstance()));
      }

      public  void blockReport(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(2),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto.class,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto.getDefaultInstance()));
      }

      public  void reportHashes(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(3),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto.class,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto.getDefaultInstance()));
      }

      public  void cacheReport(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(4),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto.class,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto.getDefaultInstance()));
      }

      public  void blockReceivedAndDeleted(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(5),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto.class,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto.getDefaultInstance()));
      }

      public  void errorReport(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(6),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto.class,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto.getDefaultInstance()));
      }

      public  void versionRequest(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(7),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionResponseProto.class,
            org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionResponseProto.getDefaultInstance()));
      }

      public  void reportBadBlocks(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(8),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto.class,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto.getDefaultInstance()));
      }

      public  void commitBlockSynchronization(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(9),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto.class,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto.getDefaultInstance()));
      }

      public  void getActiveNamenodes(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(10),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto.class,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto.getDefaultInstance()));
      }

      public  void getNextNamenodeToSendBlockReport(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto request,
          com.google.protobuf.RpcCallback<io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(11),
          controller,
          request,
          io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto.class,
            io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto.getDefaultInstance()));
      }

      public  void blockReportCompleted(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(12),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto.class,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto.getDefaultInstance()));
      }

      public  void getSmallFileData(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(13),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto.class,
            org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto.getDefaultInstance()));
      }
    }

    public static BlockingInterface newBlockingStub(
        com.google.protobuf.BlockingRpcChannel channel) {
      return new BlockingStub(channel);
    }

    public interface BlockingInterface {
      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto registerDatanode(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto sendHeartbeat(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto blockReport(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto reportHashes(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto cacheReport(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto blockReceivedAndDeleted(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto errorReport(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionResponseProto versionRequest(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto reportBadBlocks(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto commitBlockSynchronization(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto getActiveNamenodes(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto request)
          throws com.google.protobuf.ServiceException;

      public io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto getNextNamenodeToSendBlockReport(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto blockReportCompleted(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto getSmallFileData(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto request)
          throws com.google.protobuf.ServiceException;
    }

    private static final class BlockingStub implements BlockingInterface {
      private BlockingStub(com.google.protobuf.BlockingRpcChannel channel) {
        this.channel = channel;
      }

      private final com.google.protobuf.BlockingRpcChannel channel;

      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto registerDatanode(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(0),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.RegisterDatanodeResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto sendHeartbeat(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(1),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.HeartbeatResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto blockReport(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(2),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto reportHashes(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(3),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto cacheReport(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(4),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CacheReportResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto blockReceivedAndDeleted(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(5),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReceivedAndDeletedResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto errorReport(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(6),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ErrorReportResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionResponseProto versionRequest(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(7),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.HdfsProtos.VersionResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto reportBadBlocks(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(8),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ReportBadBlocksResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto commitBlockSynchronization(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(9),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.CommitBlockSynchronizationResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto getActiveNamenodes(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(10),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.ActiveNamenodeListResponseProto.getDefaultInstance());
      }


      public io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto getNextNamenodeToSendBlockReport(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.NameNodeAddressRequestForBlockReportingProto request)
          throws com.google.protobuf.ServiceException {
        return (io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(11),
          controller,
          request,
          io.hops.leader_election.proto.ActiveNodeProtos.ActiveNodeProto.getDefaultInstance());
      }


      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto blockReportCompleted(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(12),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.BlockReportCompletedResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto getSmallFileData(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.GetSmallFileDataProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(13),
          controller,
          request,
          org.apache.hadoop.hdfs.protocol.DatanodeProtocolProtos.SmallFileDataResponseProto.getDefaultInstance());
      }

    }

    // @@protoc_insertion_point(class_scope:org.apache.hadoop.datanode.DatanodeProtocolService)
  }

  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeRegistrationProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeRegistrationProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeCommandProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeCommandProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BalancerBandwidthCommandProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BalancerBandwidthCommandProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockCommandProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockCommandProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockIdCommandProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockIdCommandProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockRecoveryCommandProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockRecoveryCommandProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_FinalizeCommandProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_FinalizeCommandProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageMismatchingHashes_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageMismatchingHashes_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_AllStorageMismatchingHashes_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_AllStorageMismatchingHashes_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HashMismatchCommandProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HashMismatchCommandProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_KeyUpdateCommandProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_KeyUpdateCommandProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterCommandProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterCommandProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_VolumeFailureSummaryProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_VolumeFailureSummaryProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportContextProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportContextProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageBlockReportProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageBlockReportProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportBucketProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportBucketProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReceivedDeletedBlockInfoProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReceivedDeletedBlockInfoProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageReceivedDeletedBlocksProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageReceivedDeletedBlocksProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_NameNodeAddressRequestForBlockReportingProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_NameNodeAddressRequestForBlockReportingProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_GetSmallFileDataProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_GetSmallFileDataProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_SmallFileDataResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_SmallFileDataResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedResponseProto_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static  com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\026DatanodeProtocol.proto\0220com.gmail.benr" +
      "carver.serverlessnamenode.datanode\032\nhdfs" +
      ".proto\032\020ActiveNode.proto\"\241\002\n\031DatanodeReg" +
      "istrationProto\022L\n\ndatanodeID\030\001 \002(\01328.com" +
      ".gmail.benrcarver.serverlessnamenode.Dat" +
      "anodeIDProto\022N\n\013storageInfo\030\002 \002(\01329.com." +
      "gmail.benrcarver.serverlessnamenode.Stor" +
      "ageInfoProto\022M\n\004keys\030\003 \002(\0132?.com.gmail.b" +
      "enrcarver.serverlessnamenode.ExportedBlo" +
      "ckKeysProto\022\027\n\017softwareVersion\030\004 \002(\t\"\335\010\n" +
      "\024DatanodeCommandProto\022\\\n\007cmdType\030\001 \002(\0162K" +
      ".org.apache.hadoop" +
      ".datanode.DatanodeCommandProto.Type\022d\n\013b" +
      "alancerCmd\030\002 \001(\0132O.com.gmail.benrcarver." +
      "serverlessnamenode.datanode.BalancerBand" +
      "widthCommandProto\022S\n\006blkCmd\030\003 \001(\0132C.com." +
      "gmail.benrcarver.serverlessnamenode.data" +
      "node.BlockCommandProto\022`\n\013recoveryCmd\030\004 " +
      "\001(\0132K.com.gmail.benrcarver.serverlessnam" +
      "enode.datanode.BlockRecoveryCommandProto" +
      "\022[\n\013finalizeCmd\030\005 \001(\0132F.com.gmail.benrca" +
      "rver.serverlessnamenode.datanode.Finaliz" +
      "eCommandProto\022]\n\014keyUpdateCmd\030\006 \001(\0132G.co" +
      "m.gmail.benrcarver.serverlessnamenode.da" +
      "tanode.KeyUpdateCommandProto\022[\n\013register" +
      "Cmd\030\007 \001(\0132F.com.gmail.benrcarver.serverl" +
      "essnamenode.datanode.RegisterCommandProt" +
      "o\022W\n\010blkIdCmd\030\010 \001(\0132E.com.gmail.benrcarv" +
      "er.serverlessnamenode.datanode.BlockIdCo" +
      "mmandProto\022e\n\021mismatchHashesCmd\030\013 \001(\0132J." +
      "org.apache.hadoop." +
      "datanode.HashMismatchCommandProto\"\360\001\n\004Ty" +
      "pe\022\034\n\030BalancerBandwidthCommand\020\000\022\020\n\014Bloc" +
      "kCommand\020\001\022\030\n\024BlockRecoveryCommand\020\002\022\023\n\017" +
      "FinalizeCommand\020\003\022\024\n\020KeyUpdateCommand\020\004\022" +
      "\023\n\017RegisterCommand\020\005\022\030\n\024UnusedUpgradeCom" +
      "mand\020\006\022\027\n\023NullDatanodeCommand\020\007\022\022\n\016Block" +
      "IdCommand\020\010\022\027\n\023HashMismatchCommand\020\013\"2\n\035" +
      "BalancerBandwidthCommandProto\022\021\n\tbandwid" +
      "th\030\001 \002(\004\"\375\003\n\021BlockCommandProto\022Z\n\006action" +
      "\030\001 \002(\0162J.com.gmail.benrcarver.serverless" +
      "namenode.datanode.BlockCommandProto.Acti" +
      "on\022\023\n\013blockPoolId\030\002 \002(\t\022C\n\006blocks\030\003 \003(\0132" +
      "3.com.gmail.benrcarver.serverlessnamenod" +
      "e.BlockProto\022L\n\007targets\030\004 \003(\0132;.com.gmai" +
      "l.benrcarver.serverlessnamenode.Datanode" +
      "InfosProto\022V\n\022targetStorageUuids\030\005 \003(\0132:" +
      ".org.apache.hadoop" +
      ".StorageUuidsProto\022V\n\022targetStorageTypes" +
      "\030\006 \003(\0132:.com.gmail.benrcarver.serverless" +
      "namenode.StorageTypesProto\"4\n\006Action\022\014\n\010" +
      "TRANSFER\020\001\022\016\n\nINVALIDATE\020\002\022\014\n\010SHUTDOWN\020\003" +
      "\"\300\001\n\023BlockIdCommandProto\022\\\n\006action\030\001 \002(\016" +
      "2L.com.gmail.benrcarver.serverlessnameno" +
      "de.datanode.BlockIdCommandProto.Action\022\023" +
      "\n\013blockPoolId\030\002 \002(\t\022\024\n\010blockIds\030\003 \003(\004B\002\020" +
      "\001\" \n\006Action\022\t\n\005CACHE\020\001\022\013\n\007UNCACHE\020\002\"j\n\031B" +
      "lockRecoveryCommandProto\022M\n\006blocks\030\001 \003(\013" +
      "2=.com.gmail.benrcarver.serverlessnameno" +
      "de.RecoveringBlockProto\"+\n\024FinalizeComma" +
      "ndProto\022\023\n\013blockPoolId\030\001 \002(\t\"@\n\030StorageM" +
      "ismatchingHashes\022\021\n\tstorageID\030\001 \002(\t\022\021\n\tb" +
      "ucketIDs\030\002 \003(\r\"{\n\033AllStorageMismatchingH" +
      "ashes\022\\\n\010storages\030\001 \003(\0132J.com.gmail.benr" +
      "carver.serverlessnamenode.datanode.Stora" +
      "geMismatchingHashes\"{\n\030HashMismatchComma" +
      "ndProto\022_\n\010storages\030\001 \002(\0132M.com.gmail.be" +
      "nrcarver.serverlessnamenode.datanode.All" +
      "StorageMismatchingHashes\"f\n\025KeyUpdateCom" +
      "mandProto\022M\n\004keys\030\001 \002(\0132?.com.gmail.benr" +
      "carver.serverlessnamenode.ExportedBlockK" +
      "eysProto\"\026\n\024RegisterCommandProto\"\201\001\n\034Reg" +
      "isterDatanodeRequestProto\022a\n\014registratio" +
      "n\030\001 \002(\0132K.com.gmail.benrcarver.serverles" +
      "snamenode.datanode.DatanodeRegistrationP" +
      "roto\"\202\001\n\035RegisterDatanodeResponseProto\022a" +
      "\n\014registration\030\001 \002(\0132K.com.gmail.benrcar" +
      "ver.serverlessnamenode.datanode.Datanode" +
      "RegistrationProto\"~\n\031VolumeFailureSummar" +
      "yProto\022\036\n\026failedStorageLocations\030\001 \003(\t\022\035" +
      "\n\025lastVolumeFailureDate\030\002 \002(\004\022\"\n\032estimat" +
      "edCapacityLostTotal\030\003 \002(\004\"\262\003\n\025HeartbeatR" +
      "equestProto\022a\n\014registration\030\001 \002(\0132K.com." +
      "gmail.benrcarver.serverlessnamenode.data" +
      "node.DatanodeRegistrationProto\022L\n\007report" +
      "s\030\002 \003(\0132;.com.gmail.benrcarver.serverles" +
      "snamenode.StorageReportProto\022\032\n\017xmitsInP" +
      "rogress\030\003 \001(\r:\0010\022\027\n\014xceiverCount\030\004 \001(\r:\001" +
      "0\022\030\n\rfailedVolumes\030\005 \001(\r:\0010\022\030\n\rcacheCapa" +
      "city\030\006 \001(\004:\0010\022\024\n\tcacheUsed\030\007 \001(\004:\0010\022i\n\024v" +
      "olumeFailureSummary\030\010 \001(\0132K.com.gmail.be" +
      "nrcarver.serverlessnamenode.datanode.Vol" +
      "umeFailureSummaryProto\"\320\001\n\026HeartbeatResp" +
      "onseProto\022T\n\004cmds\030\001 \003(\0132F.com.gmail.benr" +
      "carver.serverlessnamenode.datanode.Datan" +
      "odeCommandProto\022`\n\024rollingUpgradeStatus\030" +
      "\002 \001(\0132B.com.gmail.benrcarver.serverlessn" +
      "amenode.RollingUpgradeStatusProto\"\311\002\n\027Bl" +
      "ockReportRequestProto\022a\n\014registration\030\001 " +
      "\002(\0132K.com.gmail.benrcarver.serverlessnam" +
      "enode.datanode.DatanodeRegistrationProto" +
      "\022\023\n\013blockPoolId\030\002 \002(\t\022Z\n\007reports\030\003 \003(\0132I" +
      ".org.apache.hadoop" +
      ".datanode.StorageBlockReportProto\022Z\n\007con" +
      "text\030\004 \001(\0132I.com.gmail.benrcarver.server" +
      "lessnamenode.datanode.BlockReportContext" +
      "Proto\"H\n\027BlockReportContextProto\022\021\n\ttota" +
      "lRpcs\030\001 \002(\005\022\016\n\006curRpc\030\002 \002(\005\022\n\n\002id\030\003 \002(\003\"" +
      "\275\001\n\027StorageBlockReportProto\022N\n\007storage\030\001" +
      " \002(\0132=.com.gmail.benrcarver.serverlessna" +
      "menode.DatanodeStorageProto\022R\n\006report\030\002 " +
      "\002(\0132B.com.gmail.benrcarver.serverlessnam" +
      "enode.datanode.BlockReportProto\"~\n\026Block" +
      "ReportBucketProto\022\022\n\006blocks\030\001 \003(\004B\002\020\001\022\014\n" +
      "\004hash\030\002 \002(\014\022\026\n\016numberOfBlocks\030\003 \001(\004\022\025\n\rb" +
      "locksBuffers\030\004 \003(\014\022\023\n\004skip\030\005 \002(\010:\005false\"" +
      "m\n\020BlockReportProto\022Y\n\007buckets\030\001 \003(\0132H.c" +
      "om.gmail.benrcarver.serverlessnamenode.d" +
      "atanode.BlockReportBucketProto\"o\n\030BlockR" +
      "eportResponseProto\022S\n\003cmd\030\001 \001(\0132F.com.gm" +
      "ail.benrcarver.serverlessnamenode.datano" +
      "de.DatanodeCommandProto\"\325\001\n\027CacheReportR" +
      "equestProto\022a\n\014registration\030\001 \002(\0132K.com." +
      "gmail.benrcarver.serverlessnamenode.data" +
      "node.DatanodeRegistrationProto\022\023\n\013blockP" +
      "oolId\030\002 \002(\t\022\022\n\006blocks\030\003 \003(\004B\002\020\001\022\030\n\rcache" +
      "Capacity\030\004 \001(\004:\0010\022\024\n\tcacheUsed\030\005 \001(\004:\0010\"" +
      "o\n\030CacheReportResponseProto\022S\n\003cmd\030\001 \001(\013" +
      "2F.com.gmail.benrcarver.serverlessnameno" +
      "de.datanode.DatanodeCommandProto\"\330\002\n\035Rec" +
      "eivedDeletedBlockInfoProto\022B\n\005block\030\001 \002(" +
      "\01323.com.gmail.benrcarver.serverlessnamen" +
      "ode.BlockProto\022k\n\006status\030\003 \002(\0162[.com.gma" +
      "il.benrcarver.serverlessnamenode.datanod" +
      "e.ReceivedDeletedBlockInfoProto.BlockSta" +
      "tus\022\022\n\ndeleteHint\030\002 \001(\t\"r\n\013BlockStatus\022\014" +
      "\n\010CREATING\020\001\022\r\n\tAPPENDING\020\002\022\025\n\021RECOVERIN" +
      "G_APPEND\020\003\022\014\n\010RECEIVED\020\004\022\024\n\020UPDATE_RECOV" +
      "ERED\020\005\022\013\n\007DELETED\020\006\"\355\001\n!StorageReceivedD" +
      "eletedBlocksProto\022\027\n\013storageUuid\030\001 \002(\tB\002" +
      "\030\001\022_\n\006blocks\030\002 \003(\0132O.com.gmail.benrcarve" +
      "r.serverlessnamenode.datanode.ReceivedDe" +
      "letedBlockInfoProto\022N\n\007storage\030\003 \001(\0132=.c" +
      "om.gmail.benrcarver.serverlessnamenode.D" +
      "atanodeStorageProto\"\202\002\n#BlockReceivedAnd" +
      "DeletedRequestProto\022a\n\014registration\030\001 \002(" +
      "\0132K.com.gmail.benrcarver.serverlessnamen" +
      "ode.datanode.DatanodeRegistrationProto\022\023" +
      "\n\013blockPoolId\030\002 \002(\t\022c\n\006blocks\030\003 \003(\0132S.co" +
      "m.gmail.benrcarver.serverlessnamenode.da" +
      "tanode.StorageReceivedDeletedBlocksProto" +
      "\"&\n$BlockReceivedAndDeletedResponseProto" +
      "\"\356\001\n\027ErrorReportRequestProto\022a\n\014registar" +
      "tion\030\001 \002(\0132K.com.gmail.benrcarver.server" +
      "lessnamenode.datanode.DatanodeRegistrati" +
      "onProto\022\021\n\terrorCode\030\002 \002(\r\022\013\n\003msg\030\003 \002(\t\"" +
      "P\n\tErrorCode\022\n\n\006NOTIFY\020\000\022\016\n\nDISK_ERROR\020\001" +
      "\022\021\n\rINVALID_BLOCK\020\002\022\024\n\020FATAL_DISK_ERROR\020" +
      "\003\"\032\n\030ErrorReportResponseProto\"i\n\033ReportB" +
      "adBlocksRequestProto\022J\n\006blocks\030\001 \003(\0132:.c" +
      "om.gmail.benrcarver.serverlessnamenode.L" +
      "ocatedBlockProto\"\036\n\034ReportBadBlocksRespo" +
      "nseProto\"\255\002\n&CommitBlockSynchronizationR" +
      "equestProto\022J\n\005block\030\001 \002(\0132;.com.gmail.b" +
      "enrcarver.serverlessnamenode.ExtendedBlo" +
      "ckProto\022\023\n\013newGenStamp\030\002 \002(\004\022\021\n\tnewLengt" +
      "h\030\003 \002(\004\022\021\n\tcloseFile\030\004 \002(\010\022\023\n\013deleteBloc" +
      "k\030\005 \002(\010\022L\n\nnewTargets\030\006 \003(\01328.com.gmail." +
      "benrcarver.serverlessnamenode.DatanodeID" +
      "Proto\022\031\n\021newTargetStorages\030\007 \003(\t\")\n\'Comm" +
      "itBlockSynchronizationResponseProto\" \n\036A" +
      "ctiveNamenodeListRequestProto\"n\n\037ActiveN" +
      "amenodeListResponseProto\022K\n\tnamenodes\030\001 " +
      "\003(\01328.com.gmail.benrcarver.serverlessnam" +
      "enode.ActiveNodeProto\"\243\001\n,NameNodeAddres" +
      "sRequestForBlockReportingProto\022\020\n\010noOfBl" +
      "ks\030\001 \002(\004\022a\n\014registration\030\002 \002(\0132K.com.gma" +
      "il.benrcarver.serverlessnamenode.datanod" +
      "e.DatanodeRegistrationProto\"\347\001\n BlockRep" +
      "ortCompletedRequestProto\022a\n\014registration" +
      "\030\001 \002(\0132K.com.gmail.benrcarver.serverless" +
      "namenode.datanode.DatanodeRegistrationPr" +
      "oto\022O\n\010storages\030\002 \003(\0132=.com.gmail.benrca" +
      "rver.serverlessnamenode.DatanodeStorageP" +
      "roto\022\017\n\007success\030\003 \002(\010\"#\n\025GetSmallFileDat" +
      "aProto\022\n\n\002id\030\001 \002(\r\"*\n\032SmallFileDataRespo" +
      "nseProto\022\014\n\004data\030\001 \002(\014\"#\n!BlockReportCom" +
      "pletedResponseProto2\334\023\n\027DatanodeProtocol" +
      "Service\022\263\001\n\020registerDatanode\022N.com.gmail" +
      ".benrcarver.serverlessnamenode.datanode." +
      "RegisterDatanodeRequestProto\032O.com.gmail" +
      ".benrcarver.serverlessnamenode.datanode." +
      "RegisterDatanodeResponseProto\022\242\001\n\rsendHe" +
      "artbeat\022G.com.gmail.benrcarver.serverles" +
      "snamenode.datanode.HeartbeatRequestProto" +
      "\032H.com.gmail.benrcarver.serverlessnameno" +
      "de.datanode.HeartbeatResponseProto\022\244\001\n\013b" +
      "lockReport\022I.com.gmail.benrcarver.server" +
      "lessnamenode.datanode.BlockReportRequest" +
      "Proto\032J.com.gmail.benrcarver.serverlessn" +
      "amenode.datanode.BlockReportResponseProt" +
      "o\022\245\001\n\014reportHashes\022I.com.gmail.benrcarve" +
      "r.serverlessnamenode.datanode.BlockRepor" +
      "tRequestProto\032J.com.gmail.benrcarver.ser" +
      "verlessnamenode.datanode.BlockReportResp" +
      "onseProto\022\244\001\n\013cacheReport\022I.com.gmail.be" +
      "nrcarver.serverlessnamenode.datanode.Cac" +
      "heReportRequestProto\032J.com.gmail.benrcar" +
      "ver.serverlessnamenode.datanode.CacheRep" +
      "ortResponseProto\022\310\001\n\027blockReceivedAndDel" +
      "eted\022U.com.gmail.benrcarver.serverlessna" +
      "menode.datanode.BlockReceivedAndDeletedR" +
      "equestProto\032V.com.gmail.benrcarver.serve" +
      "rlessnamenode.datanode.BlockReceivedAndD" +
      "eletedResponseProto\022\244\001\n\013errorReport\022I.co" +
      "m.gmail.benrcarver.serverlessnamenode.da" +
      "tanode.ErrorReportRequestProto\032J.com.gma" +
      "il.benrcarver.serverlessnamenode.datanod" +
      "e.ErrorReportResponseProto\022\215\001\n\016versionRe" +
      "quest\022<.com.gmail.benrcarver.serverlessn" +
      "amenode.VersionRequestProto\032=.com.gmail." +
      "benrcarver.serverlessnamenode.VersionRes" +
      "ponseProto\022\260\001\n\017reportBadBlocks\022M.com.gma" +
      "il.benrcarver.serverlessnamenode.datanod" +
      "e.ReportBadBlocksRequestProto\032N.com.gmai" +
      "l.benrcarver.serverlessnamenode.datanode" +
      ".ReportBadBlocksResponseProto\022\321\001\n\032commit" +
      "BlockSynchronization\022X.com.gmail.benrcar" +
      "ver.serverlessnamenode.datanode.CommitBl" +
      "ockSynchronizationRequestProto\032Y.com.gma" +
      "il.benrcarver.serverlessnamenode.datanod" +
      "e.CommitBlockSynchronizationResponseProt" +
      "o\022\271\001\n\022getActiveNamenodes\022P.com.gmail.ben" +
      "rcarver.serverlessnamenode.datanode.Acti" +
      "veNamenodeListRequestProto\032Q.com.gmail.b" +
      "enrcarver.serverlessnamenode.datanode.Ac" +
      "tiveNamenodeListResponseProto\022\274\001\n getNex" +
      "tNamenodeToSendBlockReport\022^.com.gmail.b" +
      "enrcarver.serverlessnamenode.datanode.Na" +
      "meNodeAddressRequestForBlockReportingPro" +
      "to\0328.com.gmail.benrcarver.serverlessname" +
      "node.ActiveNodeProto\022\277\001\n\024blockReportComp" +
      "leted\022R.com.gmail.benrcarver.serverlessn" +
      "amenode.datanode.BlockReportCompletedReq" +
      "uestProto\032S.com.gmail.benrcarver.serverl" +
      "essnamenode.datanode.BlockReportComplete" +
      "dResponseProto\022\251\001\n\020getSmallFileData\022G.co" +
      "m.gmail.benrcarver.serverlessnamenode.da" +
      "tanode.GetSmallFileDataProto\032L.com.gmail" +
      ".benrcarver.serverlessnamenode.datanode." +
      "SmallFileDataResponseProtoBP\n0com.gmail." +
      "benrcarver.serverlessnamenode.protocolB\026" +
      "DatanodeProtocolProtos\210\001\001\240\001\001"
    };
    descriptor = com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
          org.apache.hadoop.hdfs.protocol.HdfsProtos.getDescriptor(),
          io.hops.leader_election.proto.ActiveNodeProtos.getDescriptor(),
        });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeRegistrationProto_descriptor =
      getDescriptor().getMessageTypes().get(0);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeRegistrationProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeRegistrationProto_descriptor,
        new java.lang.String[] { "DatanodeID", "StorageInfo", "Keys", "SoftwareVersion", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeCommandProto_descriptor =
      getDescriptor().getMessageTypes().get(1);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeCommandProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_DatanodeCommandProto_descriptor,
        new java.lang.String[] { "CmdType", "BalancerCmd", "BlkCmd", "RecoveryCmd", "FinalizeCmd", "KeyUpdateCmd", "RegisterCmd", "BlkIdCmd", "MismatchHashesCmd", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BalancerBandwidthCommandProto_descriptor =
      getDescriptor().getMessageTypes().get(2);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BalancerBandwidthCommandProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BalancerBandwidthCommandProto_descriptor,
        new java.lang.String[] { "Bandwidth", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockCommandProto_descriptor =
      getDescriptor().getMessageTypes().get(3);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockCommandProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockCommandProto_descriptor,
        new java.lang.String[] { "Action", "BlockPoolId", "Blocks", "Targets", "TargetStorageUuids", "TargetStorageTypes", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockIdCommandProto_descriptor =
      getDescriptor().getMessageTypes().get(4);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockIdCommandProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockIdCommandProto_descriptor,
        new java.lang.String[] { "Action", "BlockPoolId", "BlockIds", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockRecoveryCommandProto_descriptor =
      getDescriptor().getMessageTypes().get(5);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockRecoveryCommandProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockRecoveryCommandProto_descriptor,
        new java.lang.String[] { "Blocks", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_FinalizeCommandProto_descriptor =
      getDescriptor().getMessageTypes().get(6);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_FinalizeCommandProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_FinalizeCommandProto_descriptor,
        new java.lang.String[] { "BlockPoolId", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageMismatchingHashes_descriptor =
      getDescriptor().getMessageTypes().get(7);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageMismatchingHashes_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageMismatchingHashes_descriptor,
        new java.lang.String[] { "StorageID", "BucketIDs", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_AllStorageMismatchingHashes_descriptor =
      getDescriptor().getMessageTypes().get(8);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_AllStorageMismatchingHashes_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_AllStorageMismatchingHashes_descriptor,
        new java.lang.String[] { "Storages", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HashMismatchCommandProto_descriptor =
      getDescriptor().getMessageTypes().get(9);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HashMismatchCommandProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HashMismatchCommandProto_descriptor,
        new java.lang.String[] { "Storages", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_KeyUpdateCommandProto_descriptor =
      getDescriptor().getMessageTypes().get(10);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_KeyUpdateCommandProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_KeyUpdateCommandProto_descriptor,
        new java.lang.String[] { "Keys", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterCommandProto_descriptor =
      getDescriptor().getMessageTypes().get(11);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterCommandProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterCommandProto_descriptor,
        new java.lang.String[] { });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(12);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeRequestProto_descriptor,
        new java.lang.String[] { "Registration", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(13);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_RegisterDatanodeResponseProto_descriptor,
        new java.lang.String[] { "Registration", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_VolumeFailureSummaryProto_descriptor =
      getDescriptor().getMessageTypes().get(14);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_VolumeFailureSummaryProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_VolumeFailureSummaryProto_descriptor,
        new java.lang.String[] { "FailedStorageLocations", "LastVolumeFailureDate", "EstimatedCapacityLostTotal", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(15);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatRequestProto_descriptor,
        new java.lang.String[] { "Registration", "Reports", "XmitsInProgress", "XceiverCount", "FailedVolumes", "CacheCapacity", "CacheUsed", "VolumeFailureSummary", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(16);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_HeartbeatResponseProto_descriptor,
        new java.lang.String[] { "Cmds", "RollingUpgradeStatus", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(17);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportRequestProto_descriptor,
        new java.lang.String[] { "Registration", "BlockPoolId", "Reports", "Context", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportContextProto_descriptor =
      getDescriptor().getMessageTypes().get(18);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportContextProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportContextProto_descriptor,
        new java.lang.String[] { "TotalRpcs", "CurRpc", "Id", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageBlockReportProto_descriptor =
      getDescriptor().getMessageTypes().get(19);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageBlockReportProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageBlockReportProto_descriptor,
        new java.lang.String[] { "Storage", "Report", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportBucketProto_descriptor =
      getDescriptor().getMessageTypes().get(20);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportBucketProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportBucketProto_descriptor,
        new java.lang.String[] { "Blocks", "Hash", "NumberOfBlocks", "BlocksBuffers", "Skip", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportProto_descriptor =
      getDescriptor().getMessageTypes().get(21);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportProto_descriptor,
        new java.lang.String[] { "Buckets", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(22);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportResponseProto_descriptor,
        new java.lang.String[] { "Cmd", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(23);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportRequestProto_descriptor,
        new java.lang.String[] { "Registration", "BlockPoolId", "Blocks", "CacheCapacity", "CacheUsed", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(24);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CacheReportResponseProto_descriptor,
        new java.lang.String[] { "Cmd", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReceivedDeletedBlockInfoProto_descriptor =
      getDescriptor().getMessageTypes().get(25);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReceivedDeletedBlockInfoProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReceivedDeletedBlockInfoProto_descriptor,
        new java.lang.String[] { "Block", "Status", "DeleteHint", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageReceivedDeletedBlocksProto_descriptor =
      getDescriptor().getMessageTypes().get(26);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageReceivedDeletedBlocksProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_StorageReceivedDeletedBlocksProto_descriptor,
        new java.lang.String[] { "StorageUuid", "Blocks", "Storage", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(27);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedRequestProto_descriptor,
        new java.lang.String[] { "Registration", "BlockPoolId", "Blocks", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(28);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReceivedAndDeletedResponseProto_descriptor,
        new java.lang.String[] { });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(29);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportRequestProto_descriptor,
        new java.lang.String[] { "Registartion", "ErrorCode", "Msg", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(30);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ErrorReportResponseProto_descriptor,
        new java.lang.String[] { });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(31);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksRequestProto_descriptor,
        new java.lang.String[] { "Blocks", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(32);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ReportBadBlocksResponseProto_descriptor,
        new java.lang.String[] { });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(33);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationRequestProto_descriptor,
        new java.lang.String[] { "Block", "NewGenStamp", "NewLength", "CloseFile", "DeleteBlock", "NewTargets", "NewTargetStorages", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(34);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_CommitBlockSynchronizationResponseProto_descriptor,
        new java.lang.String[] { });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(35);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListRequestProto_descriptor,
        new java.lang.String[] { });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(36);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_ActiveNamenodeListResponseProto_descriptor,
        new java.lang.String[] { "Namenodes", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_NameNodeAddressRequestForBlockReportingProto_descriptor =
      getDescriptor().getMessageTypes().get(37);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_NameNodeAddressRequestForBlockReportingProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_NameNodeAddressRequestForBlockReportingProto_descriptor,
        new java.lang.String[] { "NoOfBlks", "Registration", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(38);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedRequestProto_descriptor,
        new java.lang.String[] { "Registration", "Storages", "Success", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_GetSmallFileDataProto_descriptor =
      getDescriptor().getMessageTypes().get(39);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_GetSmallFileDataProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_GetSmallFileDataProto_descriptor,
        new java.lang.String[] { "Id", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_SmallFileDataResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(40);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_SmallFileDataResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_SmallFileDataResponseProto_descriptor,
        new java.lang.String[] { "Data", });
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(41);
    internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_com_gmail_benrcarver_serverlessnamenode_datanode_BlockReportCompletedResponseProto_descriptor,
        new java.lang.String[] { });
    org.apache.hadoop.hdfs.protocol.HdfsProtos.getDescriptor();
    io.hops.leader_election.proto.ActiveNodeProtos.getDescriptor();
  }

  // @@protoc_insertion_point(outer_class_scope)
}
